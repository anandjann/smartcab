<!DOCTYPE html>
<html>
<head><meta charset="utf-8" />
<title>smartcab</title><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.1.10/require.min.js"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>

<style type="text/css">
    /*!
*
* Twitter Bootstrap
*
*/
/*!
 * Bootstrap v3.3.7 (http://getbootstrap.com)
 * Copyright 2011-2016 Twitter, Inc.
 * Licensed under MIT (https://github.com/twbs/bootstrap/blob/master/LICENSE)
 */
/*! normalize.css v3.0.3 | MIT License | github.com/necolas/normalize.css */
html {
  font-family: sans-serif;
  -ms-text-size-adjust: 100%;
  -webkit-text-size-adjust: 100%;
}
body {
  margin: 0;
}
article,
aside,
details,
figcaption,
figure,
footer,
header,
hgroup,
main,
menu,
nav,
section,
summary {
  display: block;
}
audio,
canvas,
progress,
video {
  display: inline-block;
  vertical-align: baseline;
}
audio:not([controls]) {
  display: none;
  height: 0;
}
[hidden],
template {
  display: none;
}
a {
  background-color: transparent;
}
a:active,
a:hover {
  outline: 0;
}
abbr[title] {
  border-bottom: 1px dotted;
}
b,
strong {
  font-weight: bold;
}
dfn {
  font-style: italic;
}
h1 {
  font-size: 2em;
  margin: 0.67em 0;
}
mark {
  background: #ff0;
  color: #000;
}
small {
  font-size: 80%;
}
sub,
sup {
  font-size: 75%;
  line-height: 0;
  position: relative;
  vertical-align: baseline;
}
sup {
  top: -0.5em;
}
sub {
  bottom: -0.25em;
}
img {
  border: 0;
}
svg:not(:root) {
  overflow: hidden;
}
figure {
  margin: 1em 40px;
}
hr {
  box-sizing: content-box;
  height: 0;
}
pre {
  overflow: auto;
}
code,
kbd,
pre,
samp {
  font-family: monospace, monospace;
  font-size: 1em;
}
button,
input,
optgroup,
select,
textarea {
  color: inherit;
  font: inherit;
  margin: 0;
}
button {
  overflow: visible;
}
button,
select {
  text-transform: none;
}
button,
html input[type="button"],
input[type="reset"],
input[type="submit"] {
  -webkit-appearance: button;
  cursor: pointer;
}
button[disabled],
html input[disabled] {
  cursor: default;
}
button::-moz-focus-inner,
input::-moz-focus-inner {
  border: 0;
  padding: 0;
}
input {
  line-height: normal;
}
input[type="checkbox"],
input[type="radio"] {
  box-sizing: border-box;
  padding: 0;
}
input[type="number"]::-webkit-inner-spin-button,
input[type="number"]::-webkit-outer-spin-button {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: textfield;
  box-sizing: content-box;
}
input[type="search"]::-webkit-search-cancel-button,
input[type="search"]::-webkit-search-decoration {
  -webkit-appearance: none;
}
fieldset {
  border: 1px solid #c0c0c0;
  margin: 0 2px;
  padding: 0.35em 0.625em 0.75em;
}
legend {
  border: 0;
  padding: 0;
}
textarea {
  overflow: auto;
}
optgroup {
  font-weight: bold;
}
table {
  border-collapse: collapse;
  border-spacing: 0;
}
td,
th {
  padding: 0;
}
/*! Source: https://github.com/h5bp/html5-boilerplate/blob/master/src/css/main.css */
@media print {
  *,
  *:before,
  *:after {
    background: transparent !important;
    color: #000 !important;
    box-shadow: none !important;
    text-shadow: none !important;
  }
  a,
  a:visited {
    text-decoration: underline;
  }
  a[href]:after {
    content: " (" attr(href) ")";
  }
  abbr[title]:after {
    content: " (" attr(title) ")";
  }
  a[href^="#"]:after,
  a[href^="javascript:"]:after {
    content: "";
  }
  pre,
  blockquote {
    border: 1px solid #999;
    page-break-inside: avoid;
  }
  thead {
    display: table-header-group;
  }
  tr,
  img {
    page-break-inside: avoid;
  }
  img {
    max-width: 100% !important;
  }
  p,
  h2,
  h3 {
    orphans: 3;
    widows: 3;
  }
  h2,
  h3 {
    page-break-after: avoid;
  }
  .navbar {
    display: none;
  }
  .btn > .caret,
  .dropup > .btn > .caret {
    border-top-color: #000 !important;
  }
  .label {
    border: 1px solid #000;
  }
  .table {
    border-collapse: collapse !important;
  }
  .table td,
  .table th {
    background-color: #fff !important;
  }
  .table-bordered th,
  .table-bordered td {
    border: 1px solid #ddd !important;
  }
}
@font-face {
  font-family: 'Glyphicons Halflings';
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot');
  src: url('../components/bootstrap/fonts/glyphicons-halflings-regular.eot?#iefix') format('embedded-opentype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff2') format('woff2'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.woff') format('woff'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.ttf') format('truetype'), url('../components/bootstrap/fonts/glyphicons-halflings-regular.svg#glyphicons_halflingsregular') format('svg');
}
.glyphicon {
  position: relative;
  top: 1px;
  display: inline-block;
  font-family: 'Glyphicons Halflings';
  font-style: normal;
  font-weight: normal;
  line-height: 1;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
.glyphicon-asterisk:before {
  content: "\002a";
}
.glyphicon-plus:before {
  content: "\002b";
}
.glyphicon-euro:before,
.glyphicon-eur:before {
  content: "\20ac";
}
.glyphicon-minus:before {
  content: "\2212";
}
.glyphicon-cloud:before {
  content: "\2601";
}
.glyphicon-envelope:before {
  content: "\2709";
}
.glyphicon-pencil:before {
  content: "\270f";
}
.glyphicon-glass:before {
  content: "\e001";
}
.glyphicon-music:before {
  content: "\e002";
}
.glyphicon-search:before {
  content: "\e003";
}
.glyphicon-heart:before {
  content: "\e005";
}
.glyphicon-star:before {
  content: "\e006";
}
.glyphicon-star-empty:before {
  content: "\e007";
}
.glyphicon-user:before {
  content: "\e008";
}
.glyphicon-film:before {
  content: "\e009";
}
.glyphicon-th-large:before {
  content: "\e010";
}
.glyphicon-th:before {
  content: "\e011";
}
.glyphicon-th-list:before {
  content: "\e012";
}
.glyphicon-ok:before {
  content: "\e013";
}
.glyphicon-remove:before {
  content: "\e014";
}
.glyphicon-zoom-in:before {
  content: "\e015";
}
.glyphicon-zoom-out:before {
  content: "\e016";
}
.glyphicon-off:before {
  content: "\e017";
}
.glyphicon-signal:before {
  content: "\e018";
}
.glyphicon-cog:before {
  content: "\e019";
}
.glyphicon-trash:before {
  content: "\e020";
}
.glyphicon-home:before {
  content: "\e021";
}
.glyphicon-file:before {
  content: "\e022";
}
.glyphicon-time:before {
  content: "\e023";
}
.glyphicon-road:before {
  content: "\e024";
}
.glyphicon-download-alt:before {
  content: "\e025";
}
.glyphicon-download:before {
  content: "\e026";
}
.glyphicon-upload:before {
  content: "\e027";
}
.glyphicon-inbox:before {
  content: "\e028";
}
.glyphicon-play-circle:before {
  content: "\e029";
}
.glyphicon-repeat:before {
  content: "\e030";
}
.glyphicon-refresh:before {
  content: "\e031";
}
.glyphicon-list-alt:before {
  content: "\e032";
}
.glyphicon-lock:before {
  content: "\e033";
}
.glyphicon-flag:before {
  content: "\e034";
}
.glyphicon-headphones:before {
  content: "\e035";
}
.glyphicon-volume-off:before {
  content: "\e036";
}
.glyphicon-volume-down:before {
  content: "\e037";
}
.glyphicon-volume-up:before {
  content: "\e038";
}
.glyphicon-qrcode:before {
  content: "\e039";
}
.glyphicon-barcode:before {
  content: "\e040";
}
.glyphicon-tag:before {
  content: "\e041";
}
.glyphicon-tags:before {
  content: "\e042";
}
.glyphicon-book:before {
  content: "\e043";
}
.glyphicon-bookmark:before {
  content: "\e044";
}
.glyphicon-print:before {
  content: "\e045";
}
.glyphicon-camera:before {
  content: "\e046";
}
.glyphicon-font:before {
  content: "\e047";
}
.glyphicon-bold:before {
  content: "\e048";
}
.glyphicon-italic:before {
  content: "\e049";
}
.glyphicon-text-height:before {
  content: "\e050";
}
.glyphicon-text-width:before {
  content: "\e051";
}
.glyphicon-align-left:before {
  content: "\e052";
}
.glyphicon-align-center:before {
  content: "\e053";
}
.glyphicon-align-right:before {
  content: "\e054";
}
.glyphicon-align-justify:before {
  content: "\e055";
}
.glyphicon-list:before {
  content: "\e056";
}
.glyphicon-indent-left:before {
  content: "\e057";
}
.glyphicon-indent-right:before {
  content: "\e058";
}
.glyphicon-facetime-video:before {
  content: "\e059";
}
.glyphicon-picture:before {
  content: "\e060";
}
.glyphicon-map-marker:before {
  content: "\e062";
}
.glyphicon-adjust:before {
  content: "\e063";
}
.glyphicon-tint:before {
  content: "\e064";
}
.glyphicon-edit:before {
  content: "\e065";
}
.glyphicon-share:before {
  content: "\e066";
}
.glyphicon-check:before {
  content: "\e067";
}
.glyphicon-move:before {
  content: "\e068";
}
.glyphicon-step-backward:before {
  content: "\e069";
}
.glyphicon-fast-backward:before {
  content: "\e070";
}
.glyphicon-backward:before {
  content: "\e071";
}
.glyphicon-play:before {
  content: "\e072";
}
.glyphicon-pause:before {
  content: "\e073";
}
.glyphicon-stop:before {
  content: "\e074";
}
.glyphicon-forward:before {
  content: "\e075";
}
.glyphicon-fast-forward:before {
  content: "\e076";
}
.glyphicon-step-forward:before {
  content: "\e077";
}
.glyphicon-eject:before {
  content: "\e078";
}
.glyphicon-chevron-left:before {
  content: "\e079";
}
.glyphicon-chevron-right:before {
  content: "\e080";
}
.glyphicon-plus-sign:before {
  content: "\e081";
}
.glyphicon-minus-sign:before {
  content: "\e082";
}
.glyphicon-remove-sign:before {
  content: "\e083";
}
.glyphicon-ok-sign:before {
  content: "\e084";
}
.glyphicon-question-sign:before {
  content: "\e085";
}
.glyphicon-info-sign:before {
  content: "\e086";
}
.glyphicon-screenshot:before {
  content: "\e087";
}
.glyphicon-remove-circle:before {
  content: "\e088";
}
.glyphicon-ok-circle:before {
  content: "\e089";
}
.glyphicon-ban-circle:before {
  content: "\e090";
}
.glyphicon-arrow-left:before {
  content: "\e091";
}
.glyphicon-arrow-right:before {
  content: "\e092";
}
.glyphicon-arrow-up:before {
  content: "\e093";
}
.glyphicon-arrow-down:before {
  content: "\e094";
}
.glyphicon-share-alt:before {
  content: "\e095";
}
.glyphicon-resize-full:before {
  content: "\e096";
}
.glyphicon-resize-small:before {
  content: "\e097";
}
.glyphicon-exclamation-sign:before {
  content: "\e101";
}
.glyphicon-gift:before {
  content: "\e102";
}
.glyphicon-leaf:before {
  content: "\e103";
}
.glyphicon-fire:before {
  content: "\e104";
}
.glyphicon-eye-open:before {
  content: "\e105";
}
.glyphicon-eye-close:before {
  content: "\e106";
}
.glyphicon-warning-sign:before {
  content: "\e107";
}
.glyphicon-plane:before {
  content: "\e108";
}
.glyphicon-calendar:before {
  content: "\e109";
}
.glyphicon-random:before {
  content: "\e110";
}
.glyphicon-comment:before {
  content: "\e111";
}
.glyphicon-magnet:before {
  content: "\e112";
}
.glyphicon-chevron-up:before {
  content: "\e113";
}
.glyphicon-chevron-down:before {
  content: "\e114";
}
.glyphicon-retweet:before {
  content: "\e115";
}
.glyphicon-shopping-cart:before {
  content: "\e116";
}
.glyphicon-folder-close:before {
  content: "\e117";
}
.glyphicon-folder-open:before {
  content: "\e118";
}
.glyphicon-resize-vertical:before {
  content: "\e119";
}
.glyphicon-resize-horizontal:before {
  content: "\e120";
}
.glyphicon-hdd:before {
  content: "\e121";
}
.glyphicon-bullhorn:before {
  content: "\e122";
}
.glyphicon-bell:before {
  content: "\e123";
}
.glyphicon-certificate:before {
  content: "\e124";
}
.glyphicon-thumbs-up:before {
  content: "\e125";
}
.glyphicon-thumbs-down:before {
  content: "\e126";
}
.glyphicon-hand-right:before {
  content: "\e127";
}
.glyphicon-hand-left:before {
  content: "\e128";
}
.glyphicon-hand-up:before {
  content: "\e129";
}
.glyphicon-hand-down:before {
  content: "\e130";
}
.glyphicon-circle-arrow-right:before {
  content: "\e131";
}
.glyphicon-circle-arrow-left:before {
  content: "\e132";
}
.glyphicon-circle-arrow-up:before {
  content: "\e133";
}
.glyphicon-circle-arrow-down:before {
  content: "\e134";
}
.glyphicon-globe:before {
  content: "\e135";
}
.glyphicon-wrench:before {
  content: "\e136";
}
.glyphicon-tasks:before {
  content: "\e137";
}
.glyphicon-filter:before {
  content: "\e138";
}
.glyphicon-briefcase:before {
  content: "\e139";
}
.glyphicon-fullscreen:before {
  content: "\e140";
}
.glyphicon-dashboard:before {
  content: "\e141";
}
.glyphicon-paperclip:before {
  content: "\e142";
}
.glyphicon-heart-empty:before {
  content: "\e143";
}
.glyphicon-link:before {
  content: "\e144";
}
.glyphicon-phone:before {
  content: "\e145";
}
.glyphicon-pushpin:before {
  content: "\e146";
}
.glyphicon-usd:before {
  content: "\e148";
}
.glyphicon-gbp:before {
  content: "\e149";
}
.glyphicon-sort:before {
  content: "\e150";
}
.glyphicon-sort-by-alphabet:before {
  content: "\e151";
}
.glyphicon-sort-by-alphabet-alt:before {
  content: "\e152";
}
.glyphicon-sort-by-order:before {
  content: "\e153";
}
.glyphicon-sort-by-order-alt:before {
  content: "\e154";
}
.glyphicon-sort-by-attributes:before {
  content: "\e155";
}
.glyphicon-sort-by-attributes-alt:before {
  content: "\e156";
}
.glyphicon-unchecked:before {
  content: "\e157";
}
.glyphicon-expand:before {
  content: "\e158";
}
.glyphicon-collapse-down:before {
  content: "\e159";
}
.glyphicon-collapse-up:before {
  content: "\e160";
}
.glyphicon-log-in:before {
  content: "\e161";
}
.glyphicon-flash:before {
  content: "\e162";
}
.glyphicon-log-out:before {
  content: "\e163";
}
.glyphicon-new-window:before {
  content: "\e164";
}
.glyphicon-record:before {
  content: "\e165";
}
.glyphicon-save:before {
  content: "\e166";
}
.glyphicon-open:before {
  content: "\e167";
}
.glyphicon-saved:before {
  content: "\e168";
}
.glyphicon-import:before {
  content: "\e169";
}
.glyphicon-export:before {
  content: "\e170";
}
.glyphicon-send:before {
  content: "\e171";
}
.glyphicon-floppy-disk:before {
  content: "\e172";
}
.glyphicon-floppy-saved:before {
  content: "\e173";
}
.glyphicon-floppy-remove:before {
  content: "\e174";
}
.glyphicon-floppy-save:before {
  content: "\e175";
}
.glyphicon-floppy-open:before {
  content: "\e176";
}
.glyphicon-credit-card:before {
  content: "\e177";
}
.glyphicon-transfer:before {
  content: "\e178";
}
.glyphicon-cutlery:before {
  content: "\e179";
}
.glyphicon-header:before {
  content: "\e180";
}
.glyphicon-compressed:before {
  content: "\e181";
}
.glyphicon-earphone:before {
  content: "\e182";
}
.glyphicon-phone-alt:before {
  content: "\e183";
}
.glyphicon-tower:before {
  content: "\e184";
}
.glyphicon-stats:before {
  content: "\e185";
}
.glyphicon-sd-video:before {
  content: "\e186";
}
.glyphicon-hd-video:before {
  content: "\e187";
}
.glyphicon-subtitles:before {
  content: "\e188";
}
.glyphicon-sound-stereo:before {
  content: "\e189";
}
.glyphicon-sound-dolby:before {
  content: "\e190";
}
.glyphicon-sound-5-1:before {
  content: "\e191";
}
.glyphicon-sound-6-1:before {
  content: "\e192";
}
.glyphicon-sound-7-1:before {
  content: "\e193";
}
.glyphicon-copyright-mark:before {
  content: "\e194";
}
.glyphicon-registration-mark:before {
  content: "\e195";
}
.glyphicon-cloud-download:before {
  content: "\e197";
}
.glyphicon-cloud-upload:before {
  content: "\e198";
}
.glyphicon-tree-conifer:before {
  content: "\e199";
}
.glyphicon-tree-deciduous:before {
  content: "\e200";
}
.glyphicon-cd:before {
  content: "\e201";
}
.glyphicon-save-file:before {
  content: "\e202";
}
.glyphicon-open-file:before {
  content: "\e203";
}
.glyphicon-level-up:before {
  content: "\e204";
}
.glyphicon-copy:before {
  content: "\e205";
}
.glyphicon-paste:before {
  content: "\e206";
}
.glyphicon-alert:before {
  content: "\e209";
}
.glyphicon-equalizer:before {
  content: "\e210";
}
.glyphicon-king:before {
  content: "\e211";
}
.glyphicon-queen:before {
  content: "\e212";
}
.glyphicon-pawn:before {
  content: "\e213";
}
.glyphicon-bishop:before {
  content: "\e214";
}
.glyphicon-knight:before {
  content: "\e215";
}
.glyphicon-baby-formula:before {
  content: "\e216";
}
.glyphicon-tent:before {
  content: "\26fa";
}
.glyphicon-blackboard:before {
  content: "\e218";
}
.glyphicon-bed:before {
  content: "\e219";
}
.glyphicon-apple:before {
  content: "\f8ff";
}
.glyphicon-erase:before {
  content: "\e221";
}
.glyphicon-hourglass:before {
  content: "\231b";
}
.glyphicon-lamp:before {
  content: "\e223";
}
.glyphicon-duplicate:before {
  content: "\e224";
}
.glyphicon-piggy-bank:before {
  content: "\e225";
}
.glyphicon-scissors:before {
  content: "\e226";
}
.glyphicon-bitcoin:before {
  content: "\e227";
}
.glyphicon-btc:before {
  content: "\e227";
}
.glyphicon-xbt:before {
  content: "\e227";
}
.glyphicon-yen:before {
  content: "\00a5";
}
.glyphicon-jpy:before {
  content: "\00a5";
}
.glyphicon-ruble:before {
  content: "\20bd";
}
.glyphicon-rub:before {
  content: "\20bd";
}
.glyphicon-scale:before {
  content: "\e230";
}
.glyphicon-ice-lolly:before {
  content: "\e231";
}
.glyphicon-ice-lolly-tasted:before {
  content: "\e232";
}
.glyphicon-education:before {
  content: "\e233";
}
.glyphicon-option-horizontal:before {
  content: "\e234";
}
.glyphicon-option-vertical:before {
  content: "\e235";
}
.glyphicon-menu-hamburger:before {
  content: "\e236";
}
.glyphicon-modal-window:before {
  content: "\e237";
}
.glyphicon-oil:before {
  content: "\e238";
}
.glyphicon-grain:before {
  content: "\e239";
}
.glyphicon-sunglasses:before {
  content: "\e240";
}
.glyphicon-text-size:before {
  content: "\e241";
}
.glyphicon-text-color:before {
  content: "\e242";
}
.glyphicon-text-background:before {
  content: "\e243";
}
.glyphicon-object-align-top:before {
  content: "\e244";
}
.glyphicon-object-align-bottom:before {
  content: "\e245";
}
.glyphicon-object-align-horizontal:before {
  content: "\e246";
}
.glyphicon-object-align-left:before {
  content: "\e247";
}
.glyphicon-object-align-vertical:before {
  content: "\e248";
}
.glyphicon-object-align-right:before {
  content: "\e249";
}
.glyphicon-triangle-right:before {
  content: "\e250";
}
.glyphicon-triangle-left:before {
  content: "\e251";
}
.glyphicon-triangle-bottom:before {
  content: "\e252";
}
.glyphicon-triangle-top:before {
  content: "\e253";
}
.glyphicon-console:before {
  content: "\e254";
}
.glyphicon-superscript:before {
  content: "\e255";
}
.glyphicon-subscript:before {
  content: "\e256";
}
.glyphicon-menu-left:before {
  content: "\e257";
}
.glyphicon-menu-right:before {
  content: "\e258";
}
.glyphicon-menu-down:before {
  content: "\e259";
}
.glyphicon-menu-up:before {
  content: "\e260";
}
* {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
*:before,
*:after {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
html {
  font-size: 10px;
  -webkit-tap-highlight-color: rgba(0, 0, 0, 0);
}
body {
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-size: 13px;
  line-height: 1.42857143;
  color: #000;
  background-color: #fff;
}
input,
button,
select,
textarea {
  font-family: inherit;
  font-size: inherit;
  line-height: inherit;
}
a {
  color: #337ab7;
  text-decoration: none;
}
a:hover,
a:focus {
  color: #23527c;
  text-decoration: underline;
}
a:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
figure {
  margin: 0;
}
img {
  vertical-align: middle;
}
.img-responsive,
.thumbnail > img,
.thumbnail a > img,
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  display: block;
  max-width: 100%;
  height: auto;
}
.img-rounded {
  border-radius: 3px;
}
.img-thumbnail {
  padding: 4px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: all 0.2s ease-in-out;
  -o-transition: all 0.2s ease-in-out;
  transition: all 0.2s ease-in-out;
  display: inline-block;
  max-width: 100%;
  height: auto;
}
.img-circle {
  border-radius: 50%;
}
hr {
  margin-top: 18px;
  margin-bottom: 18px;
  border: 0;
  border-top: 1px solid #eeeeee;
}
.sr-only {
  position: absolute;
  width: 1px;
  height: 1px;
  margin: -1px;
  padding: 0;
  overflow: hidden;
  clip: rect(0, 0, 0, 0);
  border: 0;
}
.sr-only-focusable:active,
.sr-only-focusable:focus {
  position: static;
  width: auto;
  height: auto;
  margin: 0;
  overflow: visible;
  clip: auto;
}
[role="button"] {
  cursor: pointer;
}
h1,
h2,
h3,
h4,
h5,
h6,
.h1,
.h2,
.h3,
.h4,
.h5,
.h6 {
  font-family: inherit;
  font-weight: 500;
  line-height: 1.1;
  color: inherit;
}
h1 small,
h2 small,
h3 small,
h4 small,
h5 small,
h6 small,
.h1 small,
.h2 small,
.h3 small,
.h4 small,
.h5 small,
.h6 small,
h1 .small,
h2 .small,
h3 .small,
h4 .small,
h5 .small,
h6 .small,
.h1 .small,
.h2 .small,
.h3 .small,
.h4 .small,
.h5 .small,
.h6 .small {
  font-weight: normal;
  line-height: 1;
  color: #777777;
}
h1,
.h1,
h2,
.h2,
h3,
.h3 {
  margin-top: 18px;
  margin-bottom: 9px;
}
h1 small,
.h1 small,
h2 small,
.h2 small,
h3 small,
.h3 small,
h1 .small,
.h1 .small,
h2 .small,
.h2 .small,
h3 .small,
.h3 .small {
  font-size: 65%;
}
h4,
.h4,
h5,
.h5,
h6,
.h6 {
  margin-top: 9px;
  margin-bottom: 9px;
}
h4 small,
.h4 small,
h5 small,
.h5 small,
h6 small,
.h6 small,
h4 .small,
.h4 .small,
h5 .small,
.h5 .small,
h6 .small,
.h6 .small {
  font-size: 75%;
}
h1,
.h1 {
  font-size: 33px;
}
h2,
.h2 {
  font-size: 27px;
}
h3,
.h3 {
  font-size: 23px;
}
h4,
.h4 {
  font-size: 17px;
}
h5,
.h5 {
  font-size: 13px;
}
h6,
.h6 {
  font-size: 12px;
}
p {
  margin: 0 0 9px;
}
.lead {
  margin-bottom: 18px;
  font-size: 14px;
  font-weight: 300;
  line-height: 1.4;
}
@media (min-width: 768px) {
  .lead {
    font-size: 19.5px;
  }
}
small,
.small {
  font-size: 92%;
}
mark,
.mark {
  background-color: #fcf8e3;
  padding: .2em;
}
.text-left {
  text-align: left;
}
.text-right {
  text-align: right;
}
.text-center {
  text-align: center;
}
.text-justify {
  text-align: justify;
}
.text-nowrap {
  white-space: nowrap;
}
.text-lowercase {
  text-transform: lowercase;
}
.text-uppercase {
  text-transform: uppercase;
}
.text-capitalize {
  text-transform: capitalize;
}
.text-muted {
  color: #777777;
}
.text-primary {
  color: #337ab7;
}
a.text-primary:hover,
a.text-primary:focus {
  color: #286090;
}
.text-success {
  color: #3c763d;
}
a.text-success:hover,
a.text-success:focus {
  color: #2b542c;
}
.text-info {
  color: #31708f;
}
a.text-info:hover,
a.text-info:focus {
  color: #245269;
}
.text-warning {
  color: #8a6d3b;
}
a.text-warning:hover,
a.text-warning:focus {
  color: #66512c;
}
.text-danger {
  color: #a94442;
}
a.text-danger:hover,
a.text-danger:focus {
  color: #843534;
}
.bg-primary {
  color: #fff;
  background-color: #337ab7;
}
a.bg-primary:hover,
a.bg-primary:focus {
  background-color: #286090;
}
.bg-success {
  background-color: #dff0d8;
}
a.bg-success:hover,
a.bg-success:focus {
  background-color: #c1e2b3;
}
.bg-info {
  background-color: #d9edf7;
}
a.bg-info:hover,
a.bg-info:focus {
  background-color: #afd9ee;
}
.bg-warning {
  background-color: #fcf8e3;
}
a.bg-warning:hover,
a.bg-warning:focus {
  background-color: #f7ecb5;
}
.bg-danger {
  background-color: #f2dede;
}
a.bg-danger:hover,
a.bg-danger:focus {
  background-color: #e4b9b9;
}
.page-header {
  padding-bottom: 8px;
  margin: 36px 0 18px;
  border-bottom: 1px solid #eeeeee;
}
ul,
ol {
  margin-top: 0;
  margin-bottom: 9px;
}
ul ul,
ol ul,
ul ol,
ol ol {
  margin-bottom: 0;
}
.list-unstyled {
  padding-left: 0;
  list-style: none;
}
.list-inline {
  padding-left: 0;
  list-style: none;
  margin-left: -5px;
}
.list-inline > li {
  display: inline-block;
  padding-left: 5px;
  padding-right: 5px;
}
dl {
  margin-top: 0;
  margin-bottom: 18px;
}
dt,
dd {
  line-height: 1.42857143;
}
dt {
  font-weight: bold;
}
dd {
  margin-left: 0;
}
@media (min-width: 541px) {
  .dl-horizontal dt {
    float: left;
    width: 160px;
    clear: left;
    text-align: right;
    overflow: hidden;
    text-overflow: ellipsis;
    white-space: nowrap;
  }
  .dl-horizontal dd {
    margin-left: 180px;
  }
}
abbr[title],
abbr[data-original-title] {
  cursor: help;
  border-bottom: 1px dotted #777777;
}
.initialism {
  font-size: 90%;
  text-transform: uppercase;
}
blockquote {
  padding: 9px 18px;
  margin: 0 0 18px;
  font-size: inherit;
  border-left: 5px solid #eeeeee;
}
blockquote p:last-child,
blockquote ul:last-child,
blockquote ol:last-child {
  margin-bottom: 0;
}
blockquote footer,
blockquote small,
blockquote .small {
  display: block;
  font-size: 80%;
  line-height: 1.42857143;
  color: #777777;
}
blockquote footer:before,
blockquote small:before,
blockquote .small:before {
  content: '\2014 \00A0';
}
.blockquote-reverse,
blockquote.pull-right {
  padding-right: 15px;
  padding-left: 0;
  border-right: 5px solid #eeeeee;
  border-left: 0;
  text-align: right;
}
.blockquote-reverse footer:before,
blockquote.pull-right footer:before,
.blockquote-reverse small:before,
blockquote.pull-right small:before,
.blockquote-reverse .small:before,
blockquote.pull-right .small:before {
  content: '';
}
.blockquote-reverse footer:after,
blockquote.pull-right footer:after,
.blockquote-reverse small:after,
blockquote.pull-right small:after,
.blockquote-reverse .small:after,
blockquote.pull-right .small:after {
  content: '\00A0 \2014';
}
address {
  margin-bottom: 18px;
  font-style: normal;
  line-height: 1.42857143;
}
code,
kbd,
pre,
samp {
  font-family: monospace;
}
code {
  padding: 2px 4px;
  font-size: 90%;
  color: #c7254e;
  background-color: #f9f2f4;
  border-radius: 2px;
}
kbd {
  padding: 2px 4px;
  font-size: 90%;
  color: #888;
  background-color: transparent;
  border-radius: 1px;
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.25);
}
kbd kbd {
  padding: 0;
  font-size: 100%;
  font-weight: bold;
  box-shadow: none;
}
pre {
  display: block;
  padding: 8.5px;
  margin: 0 0 9px;
  font-size: 12px;
  line-height: 1.42857143;
  word-break: break-all;
  word-wrap: break-word;
  color: #333333;
  background-color: #f5f5f5;
  border: 1px solid #ccc;
  border-radius: 2px;
}
pre code {
  padding: 0;
  font-size: inherit;
  color: inherit;
  white-space: pre-wrap;
  background-color: transparent;
  border-radius: 0;
}
.pre-scrollable {
  max-height: 340px;
  overflow-y: scroll;
}
.container {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
@media (min-width: 768px) {
  .container {
    width: 768px;
  }
}
@media (min-width: 992px) {
  .container {
    width: 940px;
  }
}
@media (min-width: 1200px) {
  .container {
    width: 1140px;
  }
}
.container-fluid {
  margin-right: auto;
  margin-left: auto;
  padding-left: 0px;
  padding-right: 0px;
}
.row {
  margin-left: 0px;
  margin-right: 0px;
}
.col-xs-1, .col-sm-1, .col-md-1, .col-lg-1, .col-xs-2, .col-sm-2, .col-md-2, .col-lg-2, .col-xs-3, .col-sm-3, .col-md-3, .col-lg-3, .col-xs-4, .col-sm-4, .col-md-4, .col-lg-4, .col-xs-5, .col-sm-5, .col-md-5, .col-lg-5, .col-xs-6, .col-sm-6, .col-md-6, .col-lg-6, .col-xs-7, .col-sm-7, .col-md-7, .col-lg-7, .col-xs-8, .col-sm-8, .col-md-8, .col-lg-8, .col-xs-9, .col-sm-9, .col-md-9, .col-lg-9, .col-xs-10, .col-sm-10, .col-md-10, .col-lg-10, .col-xs-11, .col-sm-11, .col-md-11, .col-lg-11, .col-xs-12, .col-sm-12, .col-md-12, .col-lg-12 {
  position: relative;
  min-height: 1px;
  padding-left: 0px;
  padding-right: 0px;
}
.col-xs-1, .col-xs-2, .col-xs-3, .col-xs-4, .col-xs-5, .col-xs-6, .col-xs-7, .col-xs-8, .col-xs-9, .col-xs-10, .col-xs-11, .col-xs-12 {
  float: left;
}
.col-xs-12 {
  width: 100%;
}
.col-xs-11 {
  width: 91.66666667%;
}
.col-xs-10 {
  width: 83.33333333%;
}
.col-xs-9 {
  width: 75%;
}
.col-xs-8 {
  width: 66.66666667%;
}
.col-xs-7 {
  width: 58.33333333%;
}
.col-xs-6 {
  width: 50%;
}
.col-xs-5 {
  width: 41.66666667%;
}
.col-xs-4 {
  width: 33.33333333%;
}
.col-xs-3 {
  width: 25%;
}
.col-xs-2 {
  width: 16.66666667%;
}
.col-xs-1 {
  width: 8.33333333%;
}
.col-xs-pull-12 {
  right: 100%;
}
.col-xs-pull-11 {
  right: 91.66666667%;
}
.col-xs-pull-10 {
  right: 83.33333333%;
}
.col-xs-pull-9 {
  right: 75%;
}
.col-xs-pull-8 {
  right: 66.66666667%;
}
.col-xs-pull-7 {
  right: 58.33333333%;
}
.col-xs-pull-6 {
  right: 50%;
}
.col-xs-pull-5 {
  right: 41.66666667%;
}
.col-xs-pull-4 {
  right: 33.33333333%;
}
.col-xs-pull-3 {
  right: 25%;
}
.col-xs-pull-2 {
  right: 16.66666667%;
}
.col-xs-pull-1 {
  right: 8.33333333%;
}
.col-xs-pull-0 {
  right: auto;
}
.col-xs-push-12 {
  left: 100%;
}
.col-xs-push-11 {
  left: 91.66666667%;
}
.col-xs-push-10 {
  left: 83.33333333%;
}
.col-xs-push-9 {
  left: 75%;
}
.col-xs-push-8 {
  left: 66.66666667%;
}
.col-xs-push-7 {
  left: 58.33333333%;
}
.col-xs-push-6 {
  left: 50%;
}
.col-xs-push-5 {
  left: 41.66666667%;
}
.col-xs-push-4 {
  left: 33.33333333%;
}
.col-xs-push-3 {
  left: 25%;
}
.col-xs-push-2 {
  left: 16.66666667%;
}
.col-xs-push-1 {
  left: 8.33333333%;
}
.col-xs-push-0 {
  left: auto;
}
.col-xs-offset-12 {
  margin-left: 100%;
}
.col-xs-offset-11 {
  margin-left: 91.66666667%;
}
.col-xs-offset-10 {
  margin-left: 83.33333333%;
}
.col-xs-offset-9 {
  margin-left: 75%;
}
.col-xs-offset-8 {
  margin-left: 66.66666667%;
}
.col-xs-offset-7 {
  margin-left: 58.33333333%;
}
.col-xs-offset-6 {
  margin-left: 50%;
}
.col-xs-offset-5 {
  margin-left: 41.66666667%;
}
.col-xs-offset-4 {
  margin-left: 33.33333333%;
}
.col-xs-offset-3 {
  margin-left: 25%;
}
.col-xs-offset-2 {
  margin-left: 16.66666667%;
}
.col-xs-offset-1 {
  margin-left: 8.33333333%;
}
.col-xs-offset-0 {
  margin-left: 0%;
}
@media (min-width: 768px) {
  .col-sm-1, .col-sm-2, .col-sm-3, .col-sm-4, .col-sm-5, .col-sm-6, .col-sm-7, .col-sm-8, .col-sm-9, .col-sm-10, .col-sm-11, .col-sm-12 {
    float: left;
  }
  .col-sm-12 {
    width: 100%;
  }
  .col-sm-11 {
    width: 91.66666667%;
  }
  .col-sm-10 {
    width: 83.33333333%;
  }
  .col-sm-9 {
    width: 75%;
  }
  .col-sm-8 {
    width: 66.66666667%;
  }
  .col-sm-7 {
    width: 58.33333333%;
  }
  .col-sm-6 {
    width: 50%;
  }
  .col-sm-5 {
    width: 41.66666667%;
  }
  .col-sm-4 {
    width: 33.33333333%;
  }
  .col-sm-3 {
    width: 25%;
  }
  .col-sm-2 {
    width: 16.66666667%;
  }
  .col-sm-1 {
    width: 8.33333333%;
  }
  .col-sm-pull-12 {
    right: 100%;
  }
  .col-sm-pull-11 {
    right: 91.66666667%;
  }
  .col-sm-pull-10 {
    right: 83.33333333%;
  }
  .col-sm-pull-9 {
    right: 75%;
  }
  .col-sm-pull-8 {
    right: 66.66666667%;
  }
  .col-sm-pull-7 {
    right: 58.33333333%;
  }
  .col-sm-pull-6 {
    right: 50%;
  }
  .col-sm-pull-5 {
    right: 41.66666667%;
  }
  .col-sm-pull-4 {
    right: 33.33333333%;
  }
  .col-sm-pull-3 {
    right: 25%;
  }
  .col-sm-pull-2 {
    right: 16.66666667%;
  }
  .col-sm-pull-1 {
    right: 8.33333333%;
  }
  .col-sm-pull-0 {
    right: auto;
  }
  .col-sm-push-12 {
    left: 100%;
  }
  .col-sm-push-11 {
    left: 91.66666667%;
  }
  .col-sm-push-10 {
    left: 83.33333333%;
  }
  .col-sm-push-9 {
    left: 75%;
  }
  .col-sm-push-8 {
    left: 66.66666667%;
  }
  .col-sm-push-7 {
    left: 58.33333333%;
  }
  .col-sm-push-6 {
    left: 50%;
  }
  .col-sm-push-5 {
    left: 41.66666667%;
  }
  .col-sm-push-4 {
    left: 33.33333333%;
  }
  .col-sm-push-3 {
    left: 25%;
  }
  .col-sm-push-2 {
    left: 16.66666667%;
  }
  .col-sm-push-1 {
    left: 8.33333333%;
  }
  .col-sm-push-0 {
    left: auto;
  }
  .col-sm-offset-12 {
    margin-left: 100%;
  }
  .col-sm-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-sm-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-sm-offset-9 {
    margin-left: 75%;
  }
  .col-sm-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-sm-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-sm-offset-6 {
    margin-left: 50%;
  }
  .col-sm-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-sm-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-sm-offset-3 {
    margin-left: 25%;
  }
  .col-sm-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-sm-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-sm-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 992px) {
  .col-md-1, .col-md-2, .col-md-3, .col-md-4, .col-md-5, .col-md-6, .col-md-7, .col-md-8, .col-md-9, .col-md-10, .col-md-11, .col-md-12 {
    float: left;
  }
  .col-md-12 {
    width: 100%;
  }
  .col-md-11 {
    width: 91.66666667%;
  }
  .col-md-10 {
    width: 83.33333333%;
  }
  .col-md-9 {
    width: 75%;
  }
  .col-md-8 {
    width: 66.66666667%;
  }
  .col-md-7 {
    width: 58.33333333%;
  }
  .col-md-6 {
    width: 50%;
  }
  .col-md-5 {
    width: 41.66666667%;
  }
  .col-md-4 {
    width: 33.33333333%;
  }
  .col-md-3 {
    width: 25%;
  }
  .col-md-2 {
    width: 16.66666667%;
  }
  .col-md-1 {
    width: 8.33333333%;
  }
  .col-md-pull-12 {
    right: 100%;
  }
  .col-md-pull-11 {
    right: 91.66666667%;
  }
  .col-md-pull-10 {
    right: 83.33333333%;
  }
  .col-md-pull-9 {
    right: 75%;
  }
  .col-md-pull-8 {
    right: 66.66666667%;
  }
  .col-md-pull-7 {
    right: 58.33333333%;
  }
  .col-md-pull-6 {
    right: 50%;
  }
  .col-md-pull-5 {
    right: 41.66666667%;
  }
  .col-md-pull-4 {
    right: 33.33333333%;
  }
  .col-md-pull-3 {
    right: 25%;
  }
  .col-md-pull-2 {
    right: 16.66666667%;
  }
  .col-md-pull-1 {
    right: 8.33333333%;
  }
  .col-md-pull-0 {
    right: auto;
  }
  .col-md-push-12 {
    left: 100%;
  }
  .col-md-push-11 {
    left: 91.66666667%;
  }
  .col-md-push-10 {
    left: 83.33333333%;
  }
  .col-md-push-9 {
    left: 75%;
  }
  .col-md-push-8 {
    left: 66.66666667%;
  }
  .col-md-push-7 {
    left: 58.33333333%;
  }
  .col-md-push-6 {
    left: 50%;
  }
  .col-md-push-5 {
    left: 41.66666667%;
  }
  .col-md-push-4 {
    left: 33.33333333%;
  }
  .col-md-push-3 {
    left: 25%;
  }
  .col-md-push-2 {
    left: 16.66666667%;
  }
  .col-md-push-1 {
    left: 8.33333333%;
  }
  .col-md-push-0 {
    left: auto;
  }
  .col-md-offset-12 {
    margin-left: 100%;
  }
  .col-md-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-md-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-md-offset-9 {
    margin-left: 75%;
  }
  .col-md-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-md-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-md-offset-6 {
    margin-left: 50%;
  }
  .col-md-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-md-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-md-offset-3 {
    margin-left: 25%;
  }
  .col-md-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-md-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-md-offset-0 {
    margin-left: 0%;
  }
}
@media (min-width: 1200px) {
  .col-lg-1, .col-lg-2, .col-lg-3, .col-lg-4, .col-lg-5, .col-lg-6, .col-lg-7, .col-lg-8, .col-lg-9, .col-lg-10, .col-lg-11, .col-lg-12 {
    float: left;
  }
  .col-lg-12 {
    width: 100%;
  }
  .col-lg-11 {
    width: 91.66666667%;
  }
  .col-lg-10 {
    width: 83.33333333%;
  }
  .col-lg-9 {
    width: 75%;
  }
  .col-lg-8 {
    width: 66.66666667%;
  }
  .col-lg-7 {
    width: 58.33333333%;
  }
  .col-lg-6 {
    width: 50%;
  }
  .col-lg-5 {
    width: 41.66666667%;
  }
  .col-lg-4 {
    width: 33.33333333%;
  }
  .col-lg-3 {
    width: 25%;
  }
  .col-lg-2 {
    width: 16.66666667%;
  }
  .col-lg-1 {
    width: 8.33333333%;
  }
  .col-lg-pull-12 {
    right: 100%;
  }
  .col-lg-pull-11 {
    right: 91.66666667%;
  }
  .col-lg-pull-10 {
    right: 83.33333333%;
  }
  .col-lg-pull-9 {
    right: 75%;
  }
  .col-lg-pull-8 {
    right: 66.66666667%;
  }
  .col-lg-pull-7 {
    right: 58.33333333%;
  }
  .col-lg-pull-6 {
    right: 50%;
  }
  .col-lg-pull-5 {
    right: 41.66666667%;
  }
  .col-lg-pull-4 {
    right: 33.33333333%;
  }
  .col-lg-pull-3 {
    right: 25%;
  }
  .col-lg-pull-2 {
    right: 16.66666667%;
  }
  .col-lg-pull-1 {
    right: 8.33333333%;
  }
  .col-lg-pull-0 {
    right: auto;
  }
  .col-lg-push-12 {
    left: 100%;
  }
  .col-lg-push-11 {
    left: 91.66666667%;
  }
  .col-lg-push-10 {
    left: 83.33333333%;
  }
  .col-lg-push-9 {
    left: 75%;
  }
  .col-lg-push-8 {
    left: 66.66666667%;
  }
  .col-lg-push-7 {
    left: 58.33333333%;
  }
  .col-lg-push-6 {
    left: 50%;
  }
  .col-lg-push-5 {
    left: 41.66666667%;
  }
  .col-lg-push-4 {
    left: 33.33333333%;
  }
  .col-lg-push-3 {
    left: 25%;
  }
  .col-lg-push-2 {
    left: 16.66666667%;
  }
  .col-lg-push-1 {
    left: 8.33333333%;
  }
  .col-lg-push-0 {
    left: auto;
  }
  .col-lg-offset-12 {
    margin-left: 100%;
  }
  .col-lg-offset-11 {
    margin-left: 91.66666667%;
  }
  .col-lg-offset-10 {
    margin-left: 83.33333333%;
  }
  .col-lg-offset-9 {
    margin-left: 75%;
  }
  .col-lg-offset-8 {
    margin-left: 66.66666667%;
  }
  .col-lg-offset-7 {
    margin-left: 58.33333333%;
  }
  .col-lg-offset-6 {
    margin-left: 50%;
  }
  .col-lg-offset-5 {
    margin-left: 41.66666667%;
  }
  .col-lg-offset-4 {
    margin-left: 33.33333333%;
  }
  .col-lg-offset-3 {
    margin-left: 25%;
  }
  .col-lg-offset-2 {
    margin-left: 16.66666667%;
  }
  .col-lg-offset-1 {
    margin-left: 8.33333333%;
  }
  .col-lg-offset-0 {
    margin-left: 0%;
  }
}
table {
  background-color: transparent;
}
caption {
  padding-top: 8px;
  padding-bottom: 8px;
  color: #777777;
  text-align: left;
}
th {
  text-align: left;
}
.table {
  width: 100%;
  max-width: 100%;
  margin-bottom: 18px;
}
.table > thead > tr > th,
.table > tbody > tr > th,
.table > tfoot > tr > th,
.table > thead > tr > td,
.table > tbody > tr > td,
.table > tfoot > tr > td {
  padding: 8px;
  line-height: 1.42857143;
  vertical-align: top;
  border-top: 1px solid #ddd;
}
.table > thead > tr > th {
  vertical-align: bottom;
  border-bottom: 2px solid #ddd;
}
.table > caption + thead > tr:first-child > th,
.table > colgroup + thead > tr:first-child > th,
.table > thead:first-child > tr:first-child > th,
.table > caption + thead > tr:first-child > td,
.table > colgroup + thead > tr:first-child > td,
.table > thead:first-child > tr:first-child > td {
  border-top: 0;
}
.table > tbody + tbody {
  border-top: 2px solid #ddd;
}
.table .table {
  background-color: #fff;
}
.table-condensed > thead > tr > th,
.table-condensed > tbody > tr > th,
.table-condensed > tfoot > tr > th,
.table-condensed > thead > tr > td,
.table-condensed > tbody > tr > td,
.table-condensed > tfoot > tr > td {
  padding: 5px;
}
.table-bordered {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > tbody > tr > th,
.table-bordered > tfoot > tr > th,
.table-bordered > thead > tr > td,
.table-bordered > tbody > tr > td,
.table-bordered > tfoot > tr > td {
  border: 1px solid #ddd;
}
.table-bordered > thead > tr > th,
.table-bordered > thead > tr > td {
  border-bottom-width: 2px;
}
.table-striped > tbody > tr:nth-of-type(odd) {
  background-color: #f9f9f9;
}
.table-hover > tbody > tr:hover {
  background-color: #f5f5f5;
}
table col[class*="col-"] {
  position: static;
  float: none;
  display: table-column;
}
table td[class*="col-"],
table th[class*="col-"] {
  position: static;
  float: none;
  display: table-cell;
}
.table > thead > tr > td.active,
.table > tbody > tr > td.active,
.table > tfoot > tr > td.active,
.table > thead > tr > th.active,
.table > tbody > tr > th.active,
.table > tfoot > tr > th.active,
.table > thead > tr.active > td,
.table > tbody > tr.active > td,
.table > tfoot > tr.active > td,
.table > thead > tr.active > th,
.table > tbody > tr.active > th,
.table > tfoot > tr.active > th {
  background-color: #f5f5f5;
}
.table-hover > tbody > tr > td.active:hover,
.table-hover > tbody > tr > th.active:hover,
.table-hover > tbody > tr.active:hover > td,
.table-hover > tbody > tr:hover > .active,
.table-hover > tbody > tr.active:hover > th {
  background-color: #e8e8e8;
}
.table > thead > tr > td.success,
.table > tbody > tr > td.success,
.table > tfoot > tr > td.success,
.table > thead > tr > th.success,
.table > tbody > tr > th.success,
.table > tfoot > tr > th.success,
.table > thead > tr.success > td,
.table > tbody > tr.success > td,
.table > tfoot > tr.success > td,
.table > thead > tr.success > th,
.table > tbody > tr.success > th,
.table > tfoot > tr.success > th {
  background-color: #dff0d8;
}
.table-hover > tbody > tr > td.success:hover,
.table-hover > tbody > tr > th.success:hover,
.table-hover > tbody > tr.success:hover > td,
.table-hover > tbody > tr:hover > .success,
.table-hover > tbody > tr.success:hover > th {
  background-color: #d0e9c6;
}
.table > thead > tr > td.info,
.table > tbody > tr > td.info,
.table > tfoot > tr > td.info,
.table > thead > tr > th.info,
.table > tbody > tr > th.info,
.table > tfoot > tr > th.info,
.table > thead > tr.info > td,
.table > tbody > tr.info > td,
.table > tfoot > tr.info > td,
.table > thead > tr.info > th,
.table > tbody > tr.info > th,
.table > tfoot > tr.info > th {
  background-color: #d9edf7;
}
.table-hover > tbody > tr > td.info:hover,
.table-hover > tbody > tr > th.info:hover,
.table-hover > tbody > tr.info:hover > td,
.table-hover > tbody > tr:hover > .info,
.table-hover > tbody > tr.info:hover > th {
  background-color: #c4e3f3;
}
.table > thead > tr > td.warning,
.table > tbody > tr > td.warning,
.table > tfoot > tr > td.warning,
.table > thead > tr > th.warning,
.table > tbody > tr > th.warning,
.table > tfoot > tr > th.warning,
.table > thead > tr.warning > td,
.table > tbody > tr.warning > td,
.table > tfoot > tr.warning > td,
.table > thead > tr.warning > th,
.table > tbody > tr.warning > th,
.table > tfoot > tr.warning > th {
  background-color: #fcf8e3;
}
.table-hover > tbody > tr > td.warning:hover,
.table-hover > tbody > tr > th.warning:hover,
.table-hover > tbody > tr.warning:hover > td,
.table-hover > tbody > tr:hover > .warning,
.table-hover > tbody > tr.warning:hover > th {
  background-color: #faf2cc;
}
.table > thead > tr > td.danger,
.table > tbody > tr > td.danger,
.table > tfoot > tr > td.danger,
.table > thead > tr > th.danger,
.table > tbody > tr > th.danger,
.table > tfoot > tr > th.danger,
.table > thead > tr.danger > td,
.table > tbody > tr.danger > td,
.table > tfoot > tr.danger > td,
.table > thead > tr.danger > th,
.table > tbody > tr.danger > th,
.table > tfoot > tr.danger > th {
  background-color: #f2dede;
}
.table-hover > tbody > tr > td.danger:hover,
.table-hover > tbody > tr > th.danger:hover,
.table-hover > tbody > tr.danger:hover > td,
.table-hover > tbody > tr:hover > .danger,
.table-hover > tbody > tr.danger:hover > th {
  background-color: #ebcccc;
}
.table-responsive {
  overflow-x: auto;
  min-height: 0.01%;
}
@media screen and (max-width: 767px) {
  .table-responsive {
    width: 100%;
    margin-bottom: 13.5px;
    overflow-y: hidden;
    -ms-overflow-style: -ms-autohiding-scrollbar;
    border: 1px solid #ddd;
  }
  .table-responsive > .table {
    margin-bottom: 0;
  }
  .table-responsive > .table > thead > tr > th,
  .table-responsive > .table > tbody > tr > th,
  .table-responsive > .table > tfoot > tr > th,
  .table-responsive > .table > thead > tr > td,
  .table-responsive > .table > tbody > tr > td,
  .table-responsive > .table > tfoot > tr > td {
    white-space: nowrap;
  }
  .table-responsive > .table-bordered {
    border: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:first-child,
  .table-responsive > .table-bordered > tbody > tr > th:first-child,
  .table-responsive > .table-bordered > tfoot > tr > th:first-child,
  .table-responsive > .table-bordered > thead > tr > td:first-child,
  .table-responsive > .table-bordered > tbody > tr > td:first-child,
  .table-responsive > .table-bordered > tfoot > tr > td:first-child {
    border-left: 0;
  }
  .table-responsive > .table-bordered > thead > tr > th:last-child,
  .table-responsive > .table-bordered > tbody > tr > th:last-child,
  .table-responsive > .table-bordered > tfoot > tr > th:last-child,
  .table-responsive > .table-bordered > thead > tr > td:last-child,
  .table-responsive > .table-bordered > tbody > tr > td:last-child,
  .table-responsive > .table-bordered > tfoot > tr > td:last-child {
    border-right: 0;
  }
  .table-responsive > .table-bordered > tbody > tr:last-child > th,
  .table-responsive > .table-bordered > tfoot > tr:last-child > th,
  .table-responsive > .table-bordered > tbody > tr:last-child > td,
  .table-responsive > .table-bordered > tfoot > tr:last-child > td {
    border-bottom: 0;
  }
}
fieldset {
  padding: 0;
  margin: 0;
  border: 0;
  min-width: 0;
}
legend {
  display: block;
  width: 100%;
  padding: 0;
  margin-bottom: 18px;
  font-size: 19.5px;
  line-height: inherit;
  color: #333333;
  border: 0;
  border-bottom: 1px solid #e5e5e5;
}
label {
  display: inline-block;
  max-width: 100%;
  margin-bottom: 5px;
  font-weight: bold;
}
input[type="search"] {
  -webkit-box-sizing: border-box;
  -moz-box-sizing: border-box;
  box-sizing: border-box;
}
input[type="radio"],
input[type="checkbox"] {
  margin: 4px 0 0;
  margin-top: 1px \9;
  line-height: normal;
}
input[type="file"] {
  display: block;
}
input[type="range"] {
  display: block;
  width: 100%;
}
select[multiple],
select[size] {
  height: auto;
}
input[type="file"]:focus,
input[type="radio"]:focus,
input[type="checkbox"]:focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
output {
  display: block;
  padding-top: 7px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
}
.form-control {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
}
.form-control:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.form-control::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.form-control:-ms-input-placeholder {
  color: #999;
}
.form-control::-webkit-input-placeholder {
  color: #999;
}
.form-control::-ms-expand {
  border: 0;
  background-color: transparent;
}
.form-control[disabled],
.form-control[readonly],
fieldset[disabled] .form-control {
  background-color: #eeeeee;
  opacity: 1;
}
.form-control[disabled],
fieldset[disabled] .form-control {
  cursor: not-allowed;
}
textarea.form-control {
  height: auto;
}
input[type="search"] {
  -webkit-appearance: none;
}
@media screen and (-webkit-min-device-pixel-ratio: 0) {
  input[type="date"].form-control,
  input[type="time"].form-control,
  input[type="datetime-local"].form-control,
  input[type="month"].form-control {
    line-height: 32px;
  }
  input[type="date"].input-sm,
  input[type="time"].input-sm,
  input[type="datetime-local"].input-sm,
  input[type="month"].input-sm,
  .input-group-sm input[type="date"],
  .input-group-sm input[type="time"],
  .input-group-sm input[type="datetime-local"],
  .input-group-sm input[type="month"] {
    line-height: 30px;
  }
  input[type="date"].input-lg,
  input[type="time"].input-lg,
  input[type="datetime-local"].input-lg,
  input[type="month"].input-lg,
  .input-group-lg input[type="date"],
  .input-group-lg input[type="time"],
  .input-group-lg input[type="datetime-local"],
  .input-group-lg input[type="month"] {
    line-height: 45px;
  }
}
.form-group {
  margin-bottom: 15px;
}
.radio,
.checkbox {
  position: relative;
  display: block;
  margin-top: 10px;
  margin-bottom: 10px;
}
.radio label,
.checkbox label {
  min-height: 18px;
  padding-left: 20px;
  margin-bottom: 0;
  font-weight: normal;
  cursor: pointer;
}
.radio input[type="radio"],
.radio-inline input[type="radio"],
.checkbox input[type="checkbox"],
.checkbox-inline input[type="checkbox"] {
  position: absolute;
  margin-left: -20px;
  margin-top: 4px \9;
}
.radio + .radio,
.checkbox + .checkbox {
  margin-top: -5px;
}
.radio-inline,
.checkbox-inline {
  position: relative;
  display: inline-block;
  padding-left: 20px;
  margin-bottom: 0;
  vertical-align: middle;
  font-weight: normal;
  cursor: pointer;
}
.radio-inline + .radio-inline,
.checkbox-inline + .checkbox-inline {
  margin-top: 0;
  margin-left: 10px;
}
input[type="radio"][disabled],
input[type="checkbox"][disabled],
input[type="radio"].disabled,
input[type="checkbox"].disabled,
fieldset[disabled] input[type="radio"],
fieldset[disabled] input[type="checkbox"] {
  cursor: not-allowed;
}
.radio-inline.disabled,
.checkbox-inline.disabled,
fieldset[disabled] .radio-inline,
fieldset[disabled] .checkbox-inline {
  cursor: not-allowed;
}
.radio.disabled label,
.checkbox.disabled label,
fieldset[disabled] .radio label,
fieldset[disabled] .checkbox label {
  cursor: not-allowed;
}
.form-control-static {
  padding-top: 7px;
  padding-bottom: 7px;
  margin-bottom: 0;
  min-height: 31px;
}
.form-control-static.input-lg,
.form-control-static.input-sm {
  padding-left: 0;
  padding-right: 0;
}
.input-sm {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-sm {
  height: 30px;
  line-height: 30px;
}
textarea.input-sm,
select[multiple].input-sm {
  height: auto;
}
.form-group-sm .form-control {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.form-group-sm select.form-control {
  height: 30px;
  line-height: 30px;
}
.form-group-sm textarea.form-control,
.form-group-sm select[multiple].form-control {
  height: auto;
}
.form-group-sm .form-control-static {
  height: 30px;
  min-height: 30px;
  padding: 6px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.input-lg {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-lg {
  height: 45px;
  line-height: 45px;
}
textarea.input-lg,
select[multiple].input-lg {
  height: auto;
}
.form-group-lg .form-control {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.form-group-lg select.form-control {
  height: 45px;
  line-height: 45px;
}
.form-group-lg textarea.form-control,
.form-group-lg select[multiple].form-control {
  height: auto;
}
.form-group-lg .form-control-static {
  height: 45px;
  min-height: 35px;
  padding: 11px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.has-feedback {
  position: relative;
}
.has-feedback .form-control {
  padding-right: 40px;
}
.form-control-feedback {
  position: absolute;
  top: 0;
  right: 0;
  z-index: 2;
  display: block;
  width: 32px;
  height: 32px;
  line-height: 32px;
  text-align: center;
  pointer-events: none;
}
.input-lg + .form-control-feedback,
.input-group-lg + .form-control-feedback,
.form-group-lg .form-control + .form-control-feedback {
  width: 45px;
  height: 45px;
  line-height: 45px;
}
.input-sm + .form-control-feedback,
.input-group-sm + .form-control-feedback,
.form-group-sm .form-control + .form-control-feedback {
  width: 30px;
  height: 30px;
  line-height: 30px;
}
.has-success .help-block,
.has-success .control-label,
.has-success .radio,
.has-success .checkbox,
.has-success .radio-inline,
.has-success .checkbox-inline,
.has-success.radio label,
.has-success.checkbox label,
.has-success.radio-inline label,
.has-success.checkbox-inline label {
  color: #3c763d;
}
.has-success .form-control {
  border-color: #3c763d;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-success .form-control:focus {
  border-color: #2b542c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #67b168;
}
.has-success .input-group-addon {
  color: #3c763d;
  border-color: #3c763d;
  background-color: #dff0d8;
}
.has-success .form-control-feedback {
  color: #3c763d;
}
.has-warning .help-block,
.has-warning .control-label,
.has-warning .radio,
.has-warning .checkbox,
.has-warning .radio-inline,
.has-warning .checkbox-inline,
.has-warning.radio label,
.has-warning.checkbox label,
.has-warning.radio-inline label,
.has-warning.checkbox-inline label {
  color: #8a6d3b;
}
.has-warning .form-control {
  border-color: #8a6d3b;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-warning .form-control:focus {
  border-color: #66512c;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #c0a16b;
}
.has-warning .input-group-addon {
  color: #8a6d3b;
  border-color: #8a6d3b;
  background-color: #fcf8e3;
}
.has-warning .form-control-feedback {
  color: #8a6d3b;
}
.has-error .help-block,
.has-error .control-label,
.has-error .radio,
.has-error .checkbox,
.has-error .radio-inline,
.has-error .checkbox-inline,
.has-error.radio label,
.has-error.checkbox label,
.has-error.radio-inline label,
.has-error.checkbox-inline label {
  color: #a94442;
}
.has-error .form-control {
  border-color: #a94442;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
}
.has-error .form-control:focus {
  border-color: #843534;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075), 0 0 6px #ce8483;
}
.has-error .input-group-addon {
  color: #a94442;
  border-color: #a94442;
  background-color: #f2dede;
}
.has-error .form-control-feedback {
  color: #a94442;
}
.has-feedback label ~ .form-control-feedback {
  top: 23px;
}
.has-feedback label.sr-only ~ .form-control-feedback {
  top: 0;
}
.help-block {
  display: block;
  margin-top: 5px;
  margin-bottom: 10px;
  color: #404040;
}
@media (min-width: 768px) {
  .form-inline .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .form-inline .form-control-static {
    display: inline-block;
  }
  .form-inline .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .form-inline .input-group .input-group-addon,
  .form-inline .input-group .input-group-btn,
  .form-inline .input-group .form-control {
    width: auto;
  }
  .form-inline .input-group > .form-control {
    width: 100%;
  }
  .form-inline .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio,
  .form-inline .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .form-inline .radio label,
  .form-inline .checkbox label {
    padding-left: 0;
  }
  .form-inline .radio input[type="radio"],
  .form-inline .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .form-inline .has-feedback .form-control-feedback {
    top: 0;
  }
}
.form-horizontal .radio,
.form-horizontal .checkbox,
.form-horizontal .radio-inline,
.form-horizontal .checkbox-inline {
  margin-top: 0;
  margin-bottom: 0;
  padding-top: 7px;
}
.form-horizontal .radio,
.form-horizontal .checkbox {
  min-height: 25px;
}
.form-horizontal .form-group {
  margin-left: 0px;
  margin-right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .control-label {
    text-align: right;
    margin-bottom: 0;
    padding-top: 7px;
  }
}
.form-horizontal .has-feedback .form-control-feedback {
  right: 0px;
}
@media (min-width: 768px) {
  .form-horizontal .form-group-lg .control-label {
    padding-top: 11px;
    font-size: 17px;
  }
}
@media (min-width: 768px) {
  .form-horizontal .form-group-sm .control-label {
    padding-top: 6px;
    font-size: 12px;
  }
}
.btn {
  display: inline-block;
  margin-bottom: 0;
  font-weight: normal;
  text-align: center;
  vertical-align: middle;
  touch-action: manipulation;
  cursor: pointer;
  background-image: none;
  border: 1px solid transparent;
  white-space: nowrap;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  border-radius: 2px;
  -webkit-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
}
.btn:focus,
.btn:active:focus,
.btn.active:focus,
.btn.focus,
.btn:active.focus,
.btn.active.focus {
  outline: 5px auto -webkit-focus-ring-color;
  outline-offset: -2px;
}
.btn:hover,
.btn:focus,
.btn.focus {
  color: #333;
  text-decoration: none;
}
.btn:active,
.btn.active {
  outline: 0;
  background-image: none;
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn.disabled,
.btn[disabled],
fieldset[disabled] .btn {
  cursor: not-allowed;
  opacity: 0.65;
  filter: alpha(opacity=65);
  -webkit-box-shadow: none;
  box-shadow: none;
}
a.btn.disabled,
fieldset[disabled] a.btn {
  pointer-events: none;
}
.btn-default {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.btn-default:focus,
.btn-default.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.btn-default:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.btn-default:active:hover,
.btn-default.active:hover,
.open > .dropdown-toggle.btn-default:hover,
.btn-default:active:focus,
.btn-default.active:focus,
.open > .dropdown-toggle.btn-default:focus,
.btn-default:active.focus,
.btn-default.active.focus,
.open > .dropdown-toggle.btn-default.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.btn-default:active,
.btn-default.active,
.open > .dropdown-toggle.btn-default {
  background-image: none;
}
.btn-default.disabled:hover,
.btn-default[disabled]:hover,
fieldset[disabled] .btn-default:hover,
.btn-default.disabled:focus,
.btn-default[disabled]:focus,
fieldset[disabled] .btn-default:focus,
.btn-default.disabled.focus,
.btn-default[disabled].focus,
fieldset[disabled] .btn-default.focus {
  background-color: #fff;
  border-color: #ccc;
}
.btn-default .badge {
  color: #fff;
  background-color: #333;
}
.btn-primary {
  color: #fff;
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary:focus,
.btn-primary.focus {
  color: #fff;
  background-color: #286090;
  border-color: #122b40;
}
.btn-primary:hover {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  color: #fff;
  background-color: #286090;
  border-color: #204d74;
}
.btn-primary:active:hover,
.btn-primary.active:hover,
.open > .dropdown-toggle.btn-primary:hover,
.btn-primary:active:focus,
.btn-primary.active:focus,
.open > .dropdown-toggle.btn-primary:focus,
.btn-primary:active.focus,
.btn-primary.active.focus,
.open > .dropdown-toggle.btn-primary.focus {
  color: #fff;
  background-color: #204d74;
  border-color: #122b40;
}
.btn-primary:active,
.btn-primary.active,
.open > .dropdown-toggle.btn-primary {
  background-image: none;
}
.btn-primary.disabled:hover,
.btn-primary[disabled]:hover,
fieldset[disabled] .btn-primary:hover,
.btn-primary.disabled:focus,
.btn-primary[disabled]:focus,
fieldset[disabled] .btn-primary:focus,
.btn-primary.disabled.focus,
.btn-primary[disabled].focus,
fieldset[disabled] .btn-primary.focus {
  background-color: #337ab7;
  border-color: #2e6da4;
}
.btn-primary .badge {
  color: #337ab7;
  background-color: #fff;
}
.btn-success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success:focus,
.btn-success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.btn-success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.btn-success:active:hover,
.btn-success.active:hover,
.open > .dropdown-toggle.btn-success:hover,
.btn-success:active:focus,
.btn-success.active:focus,
.open > .dropdown-toggle.btn-success:focus,
.btn-success:active.focus,
.btn-success.active.focus,
.open > .dropdown-toggle.btn-success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.btn-success:active,
.btn-success.active,
.open > .dropdown-toggle.btn-success {
  background-image: none;
}
.btn-success.disabled:hover,
.btn-success[disabled]:hover,
fieldset[disabled] .btn-success:hover,
.btn-success.disabled:focus,
.btn-success[disabled]:focus,
fieldset[disabled] .btn-success:focus,
.btn-success.disabled.focus,
.btn-success[disabled].focus,
fieldset[disabled] .btn-success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.btn-success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.btn-info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info:focus,
.btn-info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.btn-info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.btn-info:active:hover,
.btn-info.active:hover,
.open > .dropdown-toggle.btn-info:hover,
.btn-info:active:focus,
.btn-info.active:focus,
.open > .dropdown-toggle.btn-info:focus,
.btn-info:active.focus,
.btn-info.active.focus,
.open > .dropdown-toggle.btn-info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.btn-info:active,
.btn-info.active,
.open > .dropdown-toggle.btn-info {
  background-image: none;
}
.btn-info.disabled:hover,
.btn-info[disabled]:hover,
fieldset[disabled] .btn-info:hover,
.btn-info.disabled:focus,
.btn-info[disabled]:focus,
fieldset[disabled] .btn-info:focus,
.btn-info.disabled.focus,
.btn-info[disabled].focus,
fieldset[disabled] .btn-info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.btn-info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.btn-warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning:focus,
.btn-warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.btn-warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.btn-warning:active:hover,
.btn-warning.active:hover,
.open > .dropdown-toggle.btn-warning:hover,
.btn-warning:active:focus,
.btn-warning.active:focus,
.open > .dropdown-toggle.btn-warning:focus,
.btn-warning:active.focus,
.btn-warning.active.focus,
.open > .dropdown-toggle.btn-warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.btn-warning:active,
.btn-warning.active,
.open > .dropdown-toggle.btn-warning {
  background-image: none;
}
.btn-warning.disabled:hover,
.btn-warning[disabled]:hover,
fieldset[disabled] .btn-warning:hover,
.btn-warning.disabled:focus,
.btn-warning[disabled]:focus,
fieldset[disabled] .btn-warning:focus,
.btn-warning.disabled.focus,
.btn-warning[disabled].focus,
fieldset[disabled] .btn-warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.btn-warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.btn-danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger:focus,
.btn-danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.btn-danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.btn-danger:active:hover,
.btn-danger.active:hover,
.open > .dropdown-toggle.btn-danger:hover,
.btn-danger:active:focus,
.btn-danger.active:focus,
.open > .dropdown-toggle.btn-danger:focus,
.btn-danger:active.focus,
.btn-danger.active.focus,
.open > .dropdown-toggle.btn-danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.btn-danger:active,
.btn-danger.active,
.open > .dropdown-toggle.btn-danger {
  background-image: none;
}
.btn-danger.disabled:hover,
.btn-danger[disabled]:hover,
fieldset[disabled] .btn-danger:hover,
.btn-danger.disabled:focus,
.btn-danger[disabled]:focus,
fieldset[disabled] .btn-danger:focus,
.btn-danger.disabled.focus,
.btn-danger[disabled].focus,
fieldset[disabled] .btn-danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.btn-danger .badge {
  color: #d9534f;
  background-color: #fff;
}
.btn-link {
  color: #337ab7;
  font-weight: normal;
  border-radius: 0;
}
.btn-link,
.btn-link:active,
.btn-link.active,
.btn-link[disabled],
fieldset[disabled] .btn-link {
  background-color: transparent;
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn-link,
.btn-link:hover,
.btn-link:focus,
.btn-link:active {
  border-color: transparent;
}
.btn-link:hover,
.btn-link:focus {
  color: #23527c;
  text-decoration: underline;
  background-color: transparent;
}
.btn-link[disabled]:hover,
fieldset[disabled] .btn-link:hover,
.btn-link[disabled]:focus,
fieldset[disabled] .btn-link:focus {
  color: #777777;
  text-decoration: none;
}
.btn-lg,
.btn-group-lg > .btn {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
.btn-sm,
.btn-group-sm > .btn {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-xs,
.btn-group-xs > .btn {
  padding: 1px 5px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
.btn-block {
  display: block;
  width: 100%;
}
.btn-block + .btn-block {
  margin-top: 5px;
}
input[type="submit"].btn-block,
input[type="reset"].btn-block,
input[type="button"].btn-block {
  width: 100%;
}
.fade {
  opacity: 0;
  -webkit-transition: opacity 0.15s linear;
  -o-transition: opacity 0.15s linear;
  transition: opacity 0.15s linear;
}
.fade.in {
  opacity: 1;
}
.collapse {
  display: none;
}
.collapse.in {
  display: block;
}
tr.collapse.in {
  display: table-row;
}
tbody.collapse.in {
  display: table-row-group;
}
.collapsing {
  position: relative;
  height: 0;
  overflow: hidden;
  -webkit-transition-property: height, visibility;
  transition-property: height, visibility;
  -webkit-transition-duration: 0.35s;
  transition-duration: 0.35s;
  -webkit-transition-timing-function: ease;
  transition-timing-function: ease;
}
.caret {
  display: inline-block;
  width: 0;
  height: 0;
  margin-left: 2px;
  vertical-align: middle;
  border-top: 4px dashed;
  border-top: 4px solid \9;
  border-right: 4px solid transparent;
  border-left: 4px solid transparent;
}
.dropup,
.dropdown {
  position: relative;
}
.dropdown-toggle:focus {
  outline: 0;
}
.dropdown-menu {
  position: absolute;
  top: 100%;
  left: 0;
  z-index: 1000;
  display: none;
  float: left;
  min-width: 160px;
  padding: 5px 0;
  margin: 2px 0 0;
  list-style: none;
  font-size: 13px;
  text-align: left;
  background-color: #fff;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.15);
  border-radius: 2px;
  -webkit-box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  box-shadow: 0 6px 12px rgba(0, 0, 0, 0.175);
  background-clip: padding-box;
}
.dropdown-menu.pull-right {
  right: 0;
  left: auto;
}
.dropdown-menu .divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.dropdown-menu > li > a {
  display: block;
  padding: 3px 20px;
  clear: both;
  font-weight: normal;
  line-height: 1.42857143;
  color: #333333;
  white-space: nowrap;
}
.dropdown-menu > li > a:hover,
.dropdown-menu > li > a:focus {
  text-decoration: none;
  color: #262626;
  background-color: #f5f5f5;
}
.dropdown-menu > .active > a,
.dropdown-menu > .active > a:hover,
.dropdown-menu > .active > a:focus {
  color: #fff;
  text-decoration: none;
  outline: 0;
  background-color: #337ab7;
}
.dropdown-menu > .disabled > a,
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  color: #777777;
}
.dropdown-menu > .disabled > a:hover,
.dropdown-menu > .disabled > a:focus {
  text-decoration: none;
  background-color: transparent;
  background-image: none;
  filter: progid:DXImageTransform.Microsoft.gradient(enabled = false);
  cursor: not-allowed;
}
.open > .dropdown-menu {
  display: block;
}
.open > a {
  outline: 0;
}
.dropdown-menu-right {
  left: auto;
  right: 0;
}
.dropdown-menu-left {
  left: 0;
  right: auto;
}
.dropdown-header {
  display: block;
  padding: 3px 20px;
  font-size: 12px;
  line-height: 1.42857143;
  color: #777777;
  white-space: nowrap;
}
.dropdown-backdrop {
  position: fixed;
  left: 0;
  right: 0;
  bottom: 0;
  top: 0;
  z-index: 990;
}
.pull-right > .dropdown-menu {
  right: 0;
  left: auto;
}
.dropup .caret,
.navbar-fixed-bottom .dropdown .caret {
  border-top: 0;
  border-bottom: 4px dashed;
  border-bottom: 4px solid \9;
  content: "";
}
.dropup .dropdown-menu,
.navbar-fixed-bottom .dropdown .dropdown-menu {
  top: auto;
  bottom: 100%;
  margin-bottom: 2px;
}
@media (min-width: 541px) {
  .navbar-right .dropdown-menu {
    left: auto;
    right: 0;
  }
  .navbar-right .dropdown-menu-left {
    left: 0;
    right: auto;
  }
}
.btn-group,
.btn-group-vertical {
  position: relative;
  display: inline-block;
  vertical-align: middle;
}
.btn-group > .btn,
.btn-group-vertical > .btn {
  position: relative;
  float: left;
}
.btn-group > .btn:hover,
.btn-group-vertical > .btn:hover,
.btn-group > .btn:focus,
.btn-group-vertical > .btn:focus,
.btn-group > .btn:active,
.btn-group-vertical > .btn:active,
.btn-group > .btn.active,
.btn-group-vertical > .btn.active {
  z-index: 2;
}
.btn-group .btn + .btn,
.btn-group .btn + .btn-group,
.btn-group .btn-group + .btn,
.btn-group .btn-group + .btn-group {
  margin-left: -1px;
}
.btn-toolbar {
  margin-left: -5px;
}
.btn-toolbar .btn,
.btn-toolbar .btn-group,
.btn-toolbar .input-group {
  float: left;
}
.btn-toolbar > .btn,
.btn-toolbar > .btn-group,
.btn-toolbar > .input-group {
  margin-left: 5px;
}
.btn-group > .btn:not(:first-child):not(:last-child):not(.dropdown-toggle) {
  border-radius: 0;
}
.btn-group > .btn:first-child {
  margin-left: 0;
}
.btn-group > .btn:first-child:not(:last-child):not(.dropdown-toggle) {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn:last-child:not(:first-child),
.btn-group > .dropdown-toggle:not(:first-child) {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group > .btn-group {
  float: left;
}
.btn-group > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.btn-group > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.btn-group .dropdown-toggle:active,
.btn-group.open .dropdown-toggle {
  outline: 0;
}
.btn-group > .btn + .dropdown-toggle {
  padding-left: 8px;
  padding-right: 8px;
}
.btn-group > .btn-lg + .dropdown-toggle {
  padding-left: 12px;
  padding-right: 12px;
}
.btn-group.open .dropdown-toggle {
  -webkit-box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
  box-shadow: inset 0 3px 5px rgba(0, 0, 0, 0.125);
}
.btn-group.open .dropdown-toggle.btn-link {
  -webkit-box-shadow: none;
  box-shadow: none;
}
.btn .caret {
  margin-left: 0;
}
.btn-lg .caret {
  border-width: 5px 5px 0;
  border-bottom-width: 0;
}
.dropup .btn-lg .caret {
  border-width: 0 5px 5px;
}
.btn-group-vertical > .btn,
.btn-group-vertical > .btn-group,
.btn-group-vertical > .btn-group > .btn {
  display: block;
  float: none;
  width: 100%;
  max-width: 100%;
}
.btn-group-vertical > .btn-group > .btn {
  float: none;
}
.btn-group-vertical > .btn + .btn,
.btn-group-vertical > .btn + .btn-group,
.btn-group-vertical > .btn-group + .btn,
.btn-group-vertical > .btn-group + .btn-group {
  margin-top: -1px;
  margin-left: 0;
}
.btn-group-vertical > .btn:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.btn-group-vertical > .btn:first-child:not(:last-child) {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn:last-child:not(:first-child) {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
.btn-group-vertical > .btn-group:not(:first-child):not(:last-child) > .btn {
  border-radius: 0;
}
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .btn:last-child,
.btn-group-vertical > .btn-group:first-child:not(:last-child) > .dropdown-toggle {
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.btn-group-vertical > .btn-group:last-child:not(:first-child) > .btn:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.btn-group-justified {
  display: table;
  width: 100%;
  table-layout: fixed;
  border-collapse: separate;
}
.btn-group-justified > .btn,
.btn-group-justified > .btn-group {
  float: none;
  display: table-cell;
  width: 1%;
}
.btn-group-justified > .btn-group .btn {
  width: 100%;
}
.btn-group-justified > .btn-group .dropdown-menu {
  left: auto;
}
[data-toggle="buttons"] > .btn input[type="radio"],
[data-toggle="buttons"] > .btn-group > .btn input[type="radio"],
[data-toggle="buttons"] > .btn input[type="checkbox"],
[data-toggle="buttons"] > .btn-group > .btn input[type="checkbox"] {
  position: absolute;
  clip: rect(0, 0, 0, 0);
  pointer-events: none;
}
.input-group {
  position: relative;
  display: table;
  border-collapse: separate;
}
.input-group[class*="col-"] {
  float: none;
  padding-left: 0;
  padding-right: 0;
}
.input-group .form-control {
  position: relative;
  z-index: 2;
  float: left;
  width: 100%;
  margin-bottom: 0;
}
.input-group .form-control:focus {
  z-index: 3;
}
.input-group-lg > .form-control,
.input-group-lg > .input-group-addon,
.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
  border-radius: 3px;
}
select.input-group-lg > .form-control,
select.input-group-lg > .input-group-addon,
select.input-group-lg > .input-group-btn > .btn {
  height: 45px;
  line-height: 45px;
}
textarea.input-group-lg > .form-control,
textarea.input-group-lg > .input-group-addon,
textarea.input-group-lg > .input-group-btn > .btn,
select[multiple].input-group-lg > .form-control,
select[multiple].input-group-lg > .input-group-addon,
select[multiple].input-group-lg > .input-group-btn > .btn {
  height: auto;
}
.input-group-sm > .form-control,
.input-group-sm > .input-group-addon,
.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
}
select.input-group-sm > .form-control,
select.input-group-sm > .input-group-addon,
select.input-group-sm > .input-group-btn > .btn {
  height: 30px;
  line-height: 30px;
}
textarea.input-group-sm > .form-control,
textarea.input-group-sm > .input-group-addon,
textarea.input-group-sm > .input-group-btn > .btn,
select[multiple].input-group-sm > .form-control,
select[multiple].input-group-sm > .input-group-addon,
select[multiple].input-group-sm > .input-group-btn > .btn {
  height: auto;
}
.input-group-addon,
.input-group-btn,
.input-group .form-control {
  display: table-cell;
}
.input-group-addon:not(:first-child):not(:last-child),
.input-group-btn:not(:first-child):not(:last-child),
.input-group .form-control:not(:first-child):not(:last-child) {
  border-radius: 0;
}
.input-group-addon,
.input-group-btn {
  width: 1%;
  white-space: nowrap;
  vertical-align: middle;
}
.input-group-addon {
  padding: 6px 12px;
  font-size: 13px;
  font-weight: normal;
  line-height: 1;
  color: #555555;
  text-align: center;
  background-color: #eeeeee;
  border: 1px solid #ccc;
  border-radius: 2px;
}
.input-group-addon.input-sm {
  padding: 5px 10px;
  font-size: 12px;
  border-radius: 1px;
}
.input-group-addon.input-lg {
  padding: 10px 16px;
  font-size: 17px;
  border-radius: 3px;
}
.input-group-addon input[type="radio"],
.input-group-addon input[type="checkbox"] {
  margin-top: 0;
}
.input-group .form-control:first-child,
.input-group-addon:first-child,
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group > .btn,
.input-group-btn:first-child > .dropdown-toggle,
.input-group-btn:last-child > .btn:not(:last-child):not(.dropdown-toggle),
.input-group-btn:last-child > .btn-group:not(:last-child) > .btn {
  border-bottom-right-radius: 0;
  border-top-right-radius: 0;
}
.input-group-addon:first-child {
  border-right: 0;
}
.input-group .form-control:last-child,
.input-group-addon:last-child,
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group > .btn,
.input-group-btn:last-child > .dropdown-toggle,
.input-group-btn:first-child > .btn:not(:first-child),
.input-group-btn:first-child > .btn-group:not(:first-child) > .btn {
  border-bottom-left-radius: 0;
  border-top-left-radius: 0;
}
.input-group-addon:last-child {
  border-left: 0;
}
.input-group-btn {
  position: relative;
  font-size: 0;
  white-space: nowrap;
}
.input-group-btn > .btn {
  position: relative;
}
.input-group-btn > .btn + .btn {
  margin-left: -1px;
}
.input-group-btn > .btn:hover,
.input-group-btn > .btn:focus,
.input-group-btn > .btn:active {
  z-index: 2;
}
.input-group-btn:first-child > .btn,
.input-group-btn:first-child > .btn-group {
  margin-right: -1px;
}
.input-group-btn:last-child > .btn,
.input-group-btn:last-child > .btn-group {
  z-index: 2;
  margin-left: -1px;
}
.nav {
  margin-bottom: 0;
  padding-left: 0;
  list-style: none;
}
.nav > li {
  position: relative;
  display: block;
}
.nav > li > a {
  position: relative;
  display: block;
  padding: 10px 15px;
}
.nav > li > a:hover,
.nav > li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.nav > li.disabled > a {
  color: #777777;
}
.nav > li.disabled > a:hover,
.nav > li.disabled > a:focus {
  color: #777777;
  text-decoration: none;
  background-color: transparent;
  cursor: not-allowed;
}
.nav .open > a,
.nav .open > a:hover,
.nav .open > a:focus {
  background-color: #eeeeee;
  border-color: #337ab7;
}
.nav .nav-divider {
  height: 1px;
  margin: 8px 0;
  overflow: hidden;
  background-color: #e5e5e5;
}
.nav > li > a > img {
  max-width: none;
}
.nav-tabs {
  border-bottom: 1px solid #ddd;
}
.nav-tabs > li {
  float: left;
  margin-bottom: -1px;
}
.nav-tabs > li > a {
  margin-right: 2px;
  line-height: 1.42857143;
  border: 1px solid transparent;
  border-radius: 2px 2px 0 0;
}
.nav-tabs > li > a:hover {
  border-color: #eeeeee #eeeeee #ddd;
}
.nav-tabs > li.active > a,
.nav-tabs > li.active > a:hover,
.nav-tabs > li.active > a:focus {
  color: #555555;
  background-color: #fff;
  border: 1px solid #ddd;
  border-bottom-color: transparent;
  cursor: default;
}
.nav-tabs.nav-justified {
  width: 100%;
  border-bottom: 0;
}
.nav-tabs.nav-justified > li {
  float: none;
}
.nav-tabs.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-tabs.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-tabs.nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs.nav-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs.nav-justified > .active > a,
.nav-tabs.nav-justified > .active > a:hover,
.nav-tabs.nav-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs.nav-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs.nav-justified > .active > a,
  .nav-tabs.nav-justified > .active > a:hover,
  .nav-tabs.nav-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.nav-pills > li {
  float: left;
}
.nav-pills > li > a {
  border-radius: 2px;
}
.nav-pills > li + li {
  margin-left: 2px;
}
.nav-pills > li.active > a,
.nav-pills > li.active > a:hover,
.nav-pills > li.active > a:focus {
  color: #fff;
  background-color: #337ab7;
}
.nav-stacked > li {
  float: none;
}
.nav-stacked > li + li {
  margin-top: 2px;
  margin-left: 0;
}
.nav-justified {
  width: 100%;
}
.nav-justified > li {
  float: none;
}
.nav-justified > li > a {
  text-align: center;
  margin-bottom: 5px;
}
.nav-justified > .dropdown .dropdown-menu {
  top: auto;
  left: auto;
}
@media (min-width: 768px) {
  .nav-justified > li {
    display: table-cell;
    width: 1%;
  }
  .nav-justified > li > a {
    margin-bottom: 0;
  }
}
.nav-tabs-justified {
  border-bottom: 0;
}
.nav-tabs-justified > li > a {
  margin-right: 0;
  border-radius: 2px;
}
.nav-tabs-justified > .active > a,
.nav-tabs-justified > .active > a:hover,
.nav-tabs-justified > .active > a:focus {
  border: 1px solid #ddd;
}
@media (min-width: 768px) {
  .nav-tabs-justified > li > a {
    border-bottom: 1px solid #ddd;
    border-radius: 2px 2px 0 0;
  }
  .nav-tabs-justified > .active > a,
  .nav-tabs-justified > .active > a:hover,
  .nav-tabs-justified > .active > a:focus {
    border-bottom-color: #fff;
  }
}
.tab-content > .tab-pane {
  display: none;
}
.tab-content > .active {
  display: block;
}
.nav-tabs .dropdown-menu {
  margin-top: -1px;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar {
  position: relative;
  min-height: 30px;
  margin-bottom: 18px;
  border: 1px solid transparent;
}
@media (min-width: 541px) {
  .navbar {
    border-radius: 2px;
  }
}
@media (min-width: 541px) {
  .navbar-header {
    float: left;
  }
}
.navbar-collapse {
  overflow-x: visible;
  padding-right: 0px;
  padding-left: 0px;
  border-top: 1px solid transparent;
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1);
  -webkit-overflow-scrolling: touch;
}
.navbar-collapse.in {
  overflow-y: auto;
}
@media (min-width: 541px) {
  .navbar-collapse {
    width: auto;
    border-top: 0;
    box-shadow: none;
  }
  .navbar-collapse.collapse {
    display: block !important;
    height: auto !important;
    padding-bottom: 0;
    overflow: visible !important;
  }
  .navbar-collapse.in {
    overflow-y: visible;
  }
  .navbar-fixed-top .navbar-collapse,
  .navbar-static-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    padding-left: 0;
    padding-right: 0;
  }
}
.navbar-fixed-top .navbar-collapse,
.navbar-fixed-bottom .navbar-collapse {
  max-height: 340px;
}
@media (max-device-width: 540px) and (orientation: landscape) {
  .navbar-fixed-top .navbar-collapse,
  .navbar-fixed-bottom .navbar-collapse {
    max-height: 200px;
  }
}
.container > .navbar-header,
.container-fluid > .navbar-header,
.container > .navbar-collapse,
.container-fluid > .navbar-collapse {
  margin-right: 0px;
  margin-left: 0px;
}
@media (min-width: 541px) {
  .container > .navbar-header,
  .container-fluid > .navbar-header,
  .container > .navbar-collapse,
  .container-fluid > .navbar-collapse {
    margin-right: 0;
    margin-left: 0;
  }
}
.navbar-static-top {
  z-index: 1000;
  border-width: 0 0 1px;
}
@media (min-width: 541px) {
  .navbar-static-top {
    border-radius: 0;
  }
}
.navbar-fixed-top,
.navbar-fixed-bottom {
  position: fixed;
  right: 0;
  left: 0;
  z-index: 1030;
}
@media (min-width: 541px) {
  .navbar-fixed-top,
  .navbar-fixed-bottom {
    border-radius: 0;
  }
}
.navbar-fixed-top {
  top: 0;
  border-width: 0 0 1px;
}
.navbar-fixed-bottom {
  bottom: 0;
  margin-bottom: 0;
  border-width: 1px 0 0;
}
.navbar-brand {
  float: left;
  padding: 6px 0px;
  font-size: 17px;
  line-height: 18px;
  height: 30px;
}
.navbar-brand:hover,
.navbar-brand:focus {
  text-decoration: none;
}
.navbar-brand > img {
  display: block;
}
@media (min-width: 541px) {
  .navbar > .container .navbar-brand,
  .navbar > .container-fluid .navbar-brand {
    margin-left: 0px;
  }
}
.navbar-toggle {
  position: relative;
  float: right;
  margin-right: 0px;
  padding: 9px 10px;
  margin-top: -2px;
  margin-bottom: -2px;
  background-color: transparent;
  background-image: none;
  border: 1px solid transparent;
  border-radius: 2px;
}
.navbar-toggle:focus {
  outline: 0;
}
.navbar-toggle .icon-bar {
  display: block;
  width: 22px;
  height: 2px;
  border-radius: 1px;
}
.navbar-toggle .icon-bar + .icon-bar {
  margin-top: 4px;
}
@media (min-width: 541px) {
  .navbar-toggle {
    display: none;
  }
}
.navbar-nav {
  margin: 3px 0px;
}
.navbar-nav > li > a {
  padding-top: 10px;
  padding-bottom: 10px;
  line-height: 18px;
}
@media (max-width: 540px) {
  .navbar-nav .open .dropdown-menu {
    position: static;
    float: none;
    width: auto;
    margin-top: 0;
    background-color: transparent;
    border: 0;
    box-shadow: none;
  }
  .navbar-nav .open .dropdown-menu > li > a,
  .navbar-nav .open .dropdown-menu .dropdown-header {
    padding: 5px 15px 5px 25px;
  }
  .navbar-nav .open .dropdown-menu > li > a {
    line-height: 18px;
  }
  .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-nav .open .dropdown-menu > li > a:focus {
    background-image: none;
  }
}
@media (min-width: 541px) {
  .navbar-nav {
    float: left;
    margin: 0;
  }
  .navbar-nav > li {
    float: left;
  }
  .navbar-nav > li > a {
    padding-top: 6px;
    padding-bottom: 6px;
  }
}
.navbar-form {
  margin-left: 0px;
  margin-right: 0px;
  padding: 10px 0px;
  border-top: 1px solid transparent;
  border-bottom: 1px solid transparent;
  -webkit-box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  box-shadow: inset 0 1px 0 rgba(255, 255, 255, 0.1), 0 1px 0 rgba(255, 255, 255, 0.1);
  margin-top: -1px;
  margin-bottom: -1px;
}
@media (min-width: 768px) {
  .navbar-form .form-group {
    display: inline-block;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .form-control {
    display: inline-block;
    width: auto;
    vertical-align: middle;
  }
  .navbar-form .form-control-static {
    display: inline-block;
  }
  .navbar-form .input-group {
    display: inline-table;
    vertical-align: middle;
  }
  .navbar-form .input-group .input-group-addon,
  .navbar-form .input-group .input-group-btn,
  .navbar-form .input-group .form-control {
    width: auto;
  }
  .navbar-form .input-group > .form-control {
    width: 100%;
  }
  .navbar-form .control-label {
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio,
  .navbar-form .checkbox {
    display: inline-block;
    margin-top: 0;
    margin-bottom: 0;
    vertical-align: middle;
  }
  .navbar-form .radio label,
  .navbar-form .checkbox label {
    padding-left: 0;
  }
  .navbar-form .radio input[type="radio"],
  .navbar-form .checkbox input[type="checkbox"] {
    position: relative;
    margin-left: 0;
  }
  .navbar-form .has-feedback .form-control-feedback {
    top: 0;
  }
}
@media (max-width: 540px) {
  .navbar-form .form-group {
    margin-bottom: 5px;
  }
  .navbar-form .form-group:last-child {
    margin-bottom: 0;
  }
}
@media (min-width: 541px) {
  .navbar-form {
    width: auto;
    border: 0;
    margin-left: 0;
    margin-right: 0;
    padding-top: 0;
    padding-bottom: 0;
    -webkit-box-shadow: none;
    box-shadow: none;
  }
}
.navbar-nav > li > .dropdown-menu {
  margin-top: 0;
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.navbar-fixed-bottom .navbar-nav > li > .dropdown-menu {
  margin-bottom: 0;
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
  border-bottom-right-radius: 0;
  border-bottom-left-radius: 0;
}
.navbar-btn {
  margin-top: -1px;
  margin-bottom: -1px;
}
.navbar-btn.btn-sm {
  margin-top: 0px;
  margin-bottom: 0px;
}
.navbar-btn.btn-xs {
  margin-top: 4px;
  margin-bottom: 4px;
}
.navbar-text {
  margin-top: 6px;
  margin-bottom: 6px;
}
@media (min-width: 541px) {
  .navbar-text {
    float: left;
    margin-left: 0px;
    margin-right: 0px;
  }
}
@media (min-width: 541px) {
  .navbar-left {
    float: left !important;
    float: left;
  }
  .navbar-right {
    float: right !important;
    float: right;
    margin-right: 0px;
  }
  .navbar-right ~ .navbar-right {
    margin-right: 0;
  }
}
.navbar-default {
  background-color: #f8f8f8;
  border-color: #e7e7e7;
}
.navbar-default .navbar-brand {
  color: #777;
}
.navbar-default .navbar-brand:hover,
.navbar-default .navbar-brand:focus {
  color: #5e5e5e;
  background-color: transparent;
}
.navbar-default .navbar-text {
  color: #777;
}
.navbar-default .navbar-nav > li > a {
  color: #777;
}
.navbar-default .navbar-nav > li > a:hover,
.navbar-default .navbar-nav > li > a:focus {
  color: #333;
  background-color: transparent;
}
.navbar-default .navbar-nav > .active > a,
.navbar-default .navbar-nav > .active > a:hover,
.navbar-default .navbar-nav > .active > a:focus {
  color: #555;
  background-color: #e7e7e7;
}
.navbar-default .navbar-nav > .disabled > a,
.navbar-default .navbar-nav > .disabled > a:hover,
.navbar-default .navbar-nav > .disabled > a:focus {
  color: #ccc;
  background-color: transparent;
}
.navbar-default .navbar-toggle {
  border-color: #ddd;
}
.navbar-default .navbar-toggle:hover,
.navbar-default .navbar-toggle:focus {
  background-color: #ddd;
}
.navbar-default .navbar-toggle .icon-bar {
  background-color: #888;
}
.navbar-default .navbar-collapse,
.navbar-default .navbar-form {
  border-color: #e7e7e7;
}
.navbar-default .navbar-nav > .open > a,
.navbar-default .navbar-nav > .open > a:hover,
.navbar-default .navbar-nav > .open > a:focus {
  background-color: #e7e7e7;
  color: #555;
}
@media (max-width: 540px) {
  .navbar-default .navbar-nav .open .dropdown-menu > li > a {
    color: #777;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #333;
    background-color: transparent;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #555;
    background-color: #e7e7e7;
  }
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-default .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #ccc;
    background-color: transparent;
  }
}
.navbar-default .navbar-link {
  color: #777;
}
.navbar-default .navbar-link:hover {
  color: #333;
}
.navbar-default .btn-link {
  color: #777;
}
.navbar-default .btn-link:hover,
.navbar-default .btn-link:focus {
  color: #333;
}
.navbar-default .btn-link[disabled]:hover,
fieldset[disabled] .navbar-default .btn-link:hover,
.navbar-default .btn-link[disabled]:focus,
fieldset[disabled] .navbar-default .btn-link:focus {
  color: #ccc;
}
.navbar-inverse {
  background-color: #222;
  border-color: #080808;
}
.navbar-inverse .navbar-brand {
  color: #9d9d9d;
}
.navbar-inverse .navbar-brand:hover,
.navbar-inverse .navbar-brand:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-text {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a {
  color: #9d9d9d;
}
.navbar-inverse .navbar-nav > li > a:hover,
.navbar-inverse .navbar-nav > li > a:focus {
  color: #fff;
  background-color: transparent;
}
.navbar-inverse .navbar-nav > .active > a,
.navbar-inverse .navbar-nav > .active > a:hover,
.navbar-inverse .navbar-nav > .active > a:focus {
  color: #fff;
  background-color: #080808;
}
.navbar-inverse .navbar-nav > .disabled > a,
.navbar-inverse .navbar-nav > .disabled > a:hover,
.navbar-inverse .navbar-nav > .disabled > a:focus {
  color: #444;
  background-color: transparent;
}
.navbar-inverse .navbar-toggle {
  border-color: #333;
}
.navbar-inverse .navbar-toggle:hover,
.navbar-inverse .navbar-toggle:focus {
  background-color: #333;
}
.navbar-inverse .navbar-toggle .icon-bar {
  background-color: #fff;
}
.navbar-inverse .navbar-collapse,
.navbar-inverse .navbar-form {
  border-color: #101010;
}
.navbar-inverse .navbar-nav > .open > a,
.navbar-inverse .navbar-nav > .open > a:hover,
.navbar-inverse .navbar-nav > .open > a:focus {
  background-color: #080808;
  color: #fff;
}
@media (max-width: 540px) {
  .navbar-inverse .navbar-nav .open .dropdown-menu > .dropdown-header {
    border-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu .divider {
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a {
    color: #9d9d9d;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > li > a:focus {
    color: #fff;
    background-color: transparent;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .active > a:focus {
    color: #fff;
    background-color: #080808;
  }
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:hover,
  .navbar-inverse .navbar-nav .open .dropdown-menu > .disabled > a:focus {
    color: #444;
    background-color: transparent;
  }
}
.navbar-inverse .navbar-link {
  color: #9d9d9d;
}
.navbar-inverse .navbar-link:hover {
  color: #fff;
}
.navbar-inverse .btn-link {
  color: #9d9d9d;
}
.navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link:focus {
  color: #fff;
}
.navbar-inverse .btn-link[disabled]:hover,
fieldset[disabled] .navbar-inverse .btn-link:hover,
.navbar-inverse .btn-link[disabled]:focus,
fieldset[disabled] .navbar-inverse .btn-link:focus {
  color: #444;
}
.breadcrumb {
  padding: 8px 15px;
  margin-bottom: 18px;
  list-style: none;
  background-color: #f5f5f5;
  border-radius: 2px;
}
.breadcrumb > li {
  display: inline-block;
}
.breadcrumb > li + li:before {
  content: "/\00a0";
  padding: 0 5px;
  color: #5e5e5e;
}
.breadcrumb > .active {
  color: #777777;
}
.pagination {
  display: inline-block;
  padding-left: 0;
  margin: 18px 0;
  border-radius: 2px;
}
.pagination > li {
  display: inline;
}
.pagination > li > a,
.pagination > li > span {
  position: relative;
  float: left;
  padding: 6px 12px;
  line-height: 1.42857143;
  text-decoration: none;
  color: #337ab7;
  background-color: #fff;
  border: 1px solid #ddd;
  margin-left: -1px;
}
.pagination > li:first-child > a,
.pagination > li:first-child > span {
  margin-left: 0;
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.pagination > li:last-child > a,
.pagination > li:last-child > span {
  border-bottom-right-radius: 2px;
  border-top-right-radius: 2px;
}
.pagination > li > a:hover,
.pagination > li > span:hover,
.pagination > li > a:focus,
.pagination > li > span:focus {
  z-index: 2;
  color: #23527c;
  background-color: #eeeeee;
  border-color: #ddd;
}
.pagination > .active > a,
.pagination > .active > span,
.pagination > .active > a:hover,
.pagination > .active > span:hover,
.pagination > .active > a:focus,
.pagination > .active > span:focus {
  z-index: 3;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
  cursor: default;
}
.pagination > .disabled > span,
.pagination > .disabled > span:hover,
.pagination > .disabled > span:focus,
.pagination > .disabled > a,
.pagination > .disabled > a:hover,
.pagination > .disabled > a:focus {
  color: #777777;
  background-color: #fff;
  border-color: #ddd;
  cursor: not-allowed;
}
.pagination-lg > li > a,
.pagination-lg > li > span {
  padding: 10px 16px;
  font-size: 17px;
  line-height: 1.3333333;
}
.pagination-lg > li:first-child > a,
.pagination-lg > li:first-child > span {
  border-bottom-left-radius: 3px;
  border-top-left-radius: 3px;
}
.pagination-lg > li:last-child > a,
.pagination-lg > li:last-child > span {
  border-bottom-right-radius: 3px;
  border-top-right-radius: 3px;
}
.pagination-sm > li > a,
.pagination-sm > li > span {
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
}
.pagination-sm > li:first-child > a,
.pagination-sm > li:first-child > span {
  border-bottom-left-radius: 1px;
  border-top-left-radius: 1px;
}
.pagination-sm > li:last-child > a,
.pagination-sm > li:last-child > span {
  border-bottom-right-radius: 1px;
  border-top-right-radius: 1px;
}
.pager {
  padding-left: 0;
  margin: 18px 0;
  list-style: none;
  text-align: center;
}
.pager li {
  display: inline;
}
.pager li > a,
.pager li > span {
  display: inline-block;
  padding: 5px 14px;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 15px;
}
.pager li > a:hover,
.pager li > a:focus {
  text-decoration: none;
  background-color: #eeeeee;
}
.pager .next > a,
.pager .next > span {
  float: right;
}
.pager .previous > a,
.pager .previous > span {
  float: left;
}
.pager .disabled > a,
.pager .disabled > a:hover,
.pager .disabled > a:focus,
.pager .disabled > span {
  color: #777777;
  background-color: #fff;
  cursor: not-allowed;
}
.label {
  display: inline;
  padding: .2em .6em .3em;
  font-size: 75%;
  font-weight: bold;
  line-height: 1;
  color: #fff;
  text-align: center;
  white-space: nowrap;
  vertical-align: baseline;
  border-radius: .25em;
}
a.label:hover,
a.label:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.label:empty {
  display: none;
}
.btn .label {
  position: relative;
  top: -1px;
}
.label-default {
  background-color: #777777;
}
.label-default[href]:hover,
.label-default[href]:focus {
  background-color: #5e5e5e;
}
.label-primary {
  background-color: #337ab7;
}
.label-primary[href]:hover,
.label-primary[href]:focus {
  background-color: #286090;
}
.label-success {
  background-color: #5cb85c;
}
.label-success[href]:hover,
.label-success[href]:focus {
  background-color: #449d44;
}
.label-info {
  background-color: #5bc0de;
}
.label-info[href]:hover,
.label-info[href]:focus {
  background-color: #31b0d5;
}
.label-warning {
  background-color: #f0ad4e;
}
.label-warning[href]:hover,
.label-warning[href]:focus {
  background-color: #ec971f;
}
.label-danger {
  background-color: #d9534f;
}
.label-danger[href]:hover,
.label-danger[href]:focus {
  background-color: #c9302c;
}
.badge {
  display: inline-block;
  min-width: 10px;
  padding: 3px 7px;
  font-size: 12px;
  font-weight: bold;
  color: #fff;
  line-height: 1;
  vertical-align: middle;
  white-space: nowrap;
  text-align: center;
  background-color: #777777;
  border-radius: 10px;
}
.badge:empty {
  display: none;
}
.btn .badge {
  position: relative;
  top: -1px;
}
.btn-xs .badge,
.btn-group-xs > .btn .badge {
  top: 0;
  padding: 1px 5px;
}
a.badge:hover,
a.badge:focus {
  color: #fff;
  text-decoration: none;
  cursor: pointer;
}
.list-group-item.active > .badge,
.nav-pills > .active > a > .badge {
  color: #337ab7;
  background-color: #fff;
}
.list-group-item > .badge {
  float: right;
}
.list-group-item > .badge + .badge {
  margin-right: 5px;
}
.nav-pills > li > a > .badge {
  margin-left: 3px;
}
.jumbotron {
  padding-top: 30px;
  padding-bottom: 30px;
  margin-bottom: 30px;
  color: inherit;
  background-color: #eeeeee;
}
.jumbotron h1,
.jumbotron .h1 {
  color: inherit;
}
.jumbotron p {
  margin-bottom: 15px;
  font-size: 20px;
  font-weight: 200;
}
.jumbotron > hr {
  border-top-color: #d5d5d5;
}
.container .jumbotron,
.container-fluid .jumbotron {
  border-radius: 3px;
  padding-left: 0px;
  padding-right: 0px;
}
.jumbotron .container {
  max-width: 100%;
}
@media screen and (min-width: 768px) {
  .jumbotron {
    padding-top: 48px;
    padding-bottom: 48px;
  }
  .container .jumbotron,
  .container-fluid .jumbotron {
    padding-left: 60px;
    padding-right: 60px;
  }
  .jumbotron h1,
  .jumbotron .h1 {
    font-size: 59px;
  }
}
.thumbnail {
  display: block;
  padding: 4px;
  margin-bottom: 18px;
  line-height: 1.42857143;
  background-color: #fff;
  border: 1px solid #ddd;
  border-radius: 2px;
  -webkit-transition: border 0.2s ease-in-out;
  -o-transition: border 0.2s ease-in-out;
  transition: border 0.2s ease-in-out;
}
.thumbnail > img,
.thumbnail a > img {
  margin-left: auto;
  margin-right: auto;
}
a.thumbnail:hover,
a.thumbnail:focus,
a.thumbnail.active {
  border-color: #337ab7;
}
.thumbnail .caption {
  padding: 9px;
  color: #000;
}
.alert {
  padding: 15px;
  margin-bottom: 18px;
  border: 1px solid transparent;
  border-radius: 2px;
}
.alert h4 {
  margin-top: 0;
  color: inherit;
}
.alert .alert-link {
  font-weight: bold;
}
.alert > p,
.alert > ul {
  margin-bottom: 0;
}
.alert > p + p {
  margin-top: 5px;
}
.alert-dismissable,
.alert-dismissible {
  padding-right: 35px;
}
.alert-dismissable .close,
.alert-dismissible .close {
  position: relative;
  top: -2px;
  right: -21px;
  color: inherit;
}
.alert-success {
  background-color: #dff0d8;
  border-color: #d6e9c6;
  color: #3c763d;
}
.alert-success hr {
  border-top-color: #c9e2b3;
}
.alert-success .alert-link {
  color: #2b542c;
}
.alert-info {
  background-color: #d9edf7;
  border-color: #bce8f1;
  color: #31708f;
}
.alert-info hr {
  border-top-color: #a6e1ec;
}
.alert-info .alert-link {
  color: #245269;
}
.alert-warning {
  background-color: #fcf8e3;
  border-color: #faebcc;
  color: #8a6d3b;
}
.alert-warning hr {
  border-top-color: #f7e1b5;
}
.alert-warning .alert-link {
  color: #66512c;
}
.alert-danger {
  background-color: #f2dede;
  border-color: #ebccd1;
  color: #a94442;
}
.alert-danger hr {
  border-top-color: #e4b9c0;
}
.alert-danger .alert-link {
  color: #843534;
}
@-webkit-keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
@keyframes progress-bar-stripes {
  from {
    background-position: 40px 0;
  }
  to {
    background-position: 0 0;
  }
}
.progress {
  overflow: hidden;
  height: 18px;
  margin-bottom: 18px;
  background-color: #f5f5f5;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
  box-shadow: inset 0 1px 2px rgba(0, 0, 0, 0.1);
}
.progress-bar {
  float: left;
  width: 0%;
  height: 100%;
  font-size: 12px;
  line-height: 18px;
  color: #fff;
  text-align: center;
  background-color: #337ab7;
  -webkit-box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  box-shadow: inset 0 -1px 0 rgba(0, 0, 0, 0.15);
  -webkit-transition: width 0.6s ease;
  -o-transition: width 0.6s ease;
  transition: width 0.6s ease;
}
.progress-striped .progress-bar,
.progress-bar-striped {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-size: 40px 40px;
}
.progress.active .progress-bar,
.progress-bar.active {
  -webkit-animation: progress-bar-stripes 2s linear infinite;
  -o-animation: progress-bar-stripes 2s linear infinite;
  animation: progress-bar-stripes 2s linear infinite;
}
.progress-bar-success {
  background-color: #5cb85c;
}
.progress-striped .progress-bar-success {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-info {
  background-color: #5bc0de;
}
.progress-striped .progress-bar-info {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-warning {
  background-color: #f0ad4e;
}
.progress-striped .progress-bar-warning {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.progress-bar-danger {
  background-color: #d9534f;
}
.progress-striped .progress-bar-danger {
  background-image: -webkit-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: -o-linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
  background-image: linear-gradient(45deg, rgba(255, 255, 255, 0.15) 25%, transparent 25%, transparent 50%, rgba(255, 255, 255, 0.15) 50%, rgba(255, 255, 255, 0.15) 75%, transparent 75%, transparent);
}
.media {
  margin-top: 15px;
}
.media:first-child {
  margin-top: 0;
}
.media,
.media-body {
  zoom: 1;
  overflow: hidden;
}
.media-body {
  width: 10000px;
}
.media-object {
  display: block;
}
.media-object.img-thumbnail {
  max-width: none;
}
.media-right,
.media > .pull-right {
  padding-left: 10px;
}
.media-left,
.media > .pull-left {
  padding-right: 10px;
}
.media-left,
.media-right,
.media-body {
  display: table-cell;
  vertical-align: top;
}
.media-middle {
  vertical-align: middle;
}
.media-bottom {
  vertical-align: bottom;
}
.media-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.media-list {
  padding-left: 0;
  list-style: none;
}
.list-group {
  margin-bottom: 20px;
  padding-left: 0;
}
.list-group-item {
  position: relative;
  display: block;
  padding: 10px 15px;
  margin-bottom: -1px;
  background-color: #fff;
  border: 1px solid #ddd;
}
.list-group-item:first-child {
  border-top-right-radius: 2px;
  border-top-left-radius: 2px;
}
.list-group-item:last-child {
  margin-bottom: 0;
  border-bottom-right-radius: 2px;
  border-bottom-left-radius: 2px;
}
a.list-group-item,
button.list-group-item {
  color: #555;
}
a.list-group-item .list-group-item-heading,
button.list-group-item .list-group-item-heading {
  color: #333;
}
a.list-group-item:hover,
button.list-group-item:hover,
a.list-group-item:focus,
button.list-group-item:focus {
  text-decoration: none;
  color: #555;
  background-color: #f5f5f5;
}
button.list-group-item {
  width: 100%;
  text-align: left;
}
.list-group-item.disabled,
.list-group-item.disabled:hover,
.list-group-item.disabled:focus {
  background-color: #eeeeee;
  color: #777777;
  cursor: not-allowed;
}
.list-group-item.disabled .list-group-item-heading,
.list-group-item.disabled:hover .list-group-item-heading,
.list-group-item.disabled:focus .list-group-item-heading {
  color: inherit;
}
.list-group-item.disabled .list-group-item-text,
.list-group-item.disabled:hover .list-group-item-text,
.list-group-item.disabled:focus .list-group-item-text {
  color: #777777;
}
.list-group-item.active,
.list-group-item.active:hover,
.list-group-item.active:focus {
  z-index: 2;
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.list-group-item.active .list-group-item-heading,
.list-group-item.active:hover .list-group-item-heading,
.list-group-item.active:focus .list-group-item-heading,
.list-group-item.active .list-group-item-heading > small,
.list-group-item.active:hover .list-group-item-heading > small,
.list-group-item.active:focus .list-group-item-heading > small,
.list-group-item.active .list-group-item-heading > .small,
.list-group-item.active:hover .list-group-item-heading > .small,
.list-group-item.active:focus .list-group-item-heading > .small {
  color: inherit;
}
.list-group-item.active .list-group-item-text,
.list-group-item.active:hover .list-group-item-text,
.list-group-item.active:focus .list-group-item-text {
  color: #c7ddef;
}
.list-group-item-success {
  color: #3c763d;
  background-color: #dff0d8;
}
a.list-group-item-success,
button.list-group-item-success {
  color: #3c763d;
}
a.list-group-item-success .list-group-item-heading,
button.list-group-item-success .list-group-item-heading {
  color: inherit;
}
a.list-group-item-success:hover,
button.list-group-item-success:hover,
a.list-group-item-success:focus,
button.list-group-item-success:focus {
  color: #3c763d;
  background-color: #d0e9c6;
}
a.list-group-item-success.active,
button.list-group-item-success.active,
a.list-group-item-success.active:hover,
button.list-group-item-success.active:hover,
a.list-group-item-success.active:focus,
button.list-group-item-success.active:focus {
  color: #fff;
  background-color: #3c763d;
  border-color: #3c763d;
}
.list-group-item-info {
  color: #31708f;
  background-color: #d9edf7;
}
a.list-group-item-info,
button.list-group-item-info {
  color: #31708f;
}
a.list-group-item-info .list-group-item-heading,
button.list-group-item-info .list-group-item-heading {
  color: inherit;
}
a.list-group-item-info:hover,
button.list-group-item-info:hover,
a.list-group-item-info:focus,
button.list-group-item-info:focus {
  color: #31708f;
  background-color: #c4e3f3;
}
a.list-group-item-info.active,
button.list-group-item-info.active,
a.list-group-item-info.active:hover,
button.list-group-item-info.active:hover,
a.list-group-item-info.active:focus,
button.list-group-item-info.active:focus {
  color: #fff;
  background-color: #31708f;
  border-color: #31708f;
}
.list-group-item-warning {
  color: #8a6d3b;
  background-color: #fcf8e3;
}
a.list-group-item-warning,
button.list-group-item-warning {
  color: #8a6d3b;
}
a.list-group-item-warning .list-group-item-heading,
button.list-group-item-warning .list-group-item-heading {
  color: inherit;
}
a.list-group-item-warning:hover,
button.list-group-item-warning:hover,
a.list-group-item-warning:focus,
button.list-group-item-warning:focus {
  color: #8a6d3b;
  background-color: #faf2cc;
}
a.list-group-item-warning.active,
button.list-group-item-warning.active,
a.list-group-item-warning.active:hover,
button.list-group-item-warning.active:hover,
a.list-group-item-warning.active:focus,
button.list-group-item-warning.active:focus {
  color: #fff;
  background-color: #8a6d3b;
  border-color: #8a6d3b;
}
.list-group-item-danger {
  color: #a94442;
  background-color: #f2dede;
}
a.list-group-item-danger,
button.list-group-item-danger {
  color: #a94442;
}
a.list-group-item-danger .list-group-item-heading,
button.list-group-item-danger .list-group-item-heading {
  color: inherit;
}
a.list-group-item-danger:hover,
button.list-group-item-danger:hover,
a.list-group-item-danger:focus,
button.list-group-item-danger:focus {
  color: #a94442;
  background-color: #ebcccc;
}
a.list-group-item-danger.active,
button.list-group-item-danger.active,
a.list-group-item-danger.active:hover,
button.list-group-item-danger.active:hover,
a.list-group-item-danger.active:focus,
button.list-group-item-danger.active:focus {
  color: #fff;
  background-color: #a94442;
  border-color: #a94442;
}
.list-group-item-heading {
  margin-top: 0;
  margin-bottom: 5px;
}
.list-group-item-text {
  margin-bottom: 0;
  line-height: 1.3;
}
.panel {
  margin-bottom: 18px;
  background-color: #fff;
  border: 1px solid transparent;
  border-radius: 2px;
  -webkit-box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: 0 1px 1px rgba(0, 0, 0, 0.05);
}
.panel-body {
  padding: 15px;
}
.panel-heading {
  padding: 10px 15px;
  border-bottom: 1px solid transparent;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel-heading > .dropdown .dropdown-toggle {
  color: inherit;
}
.panel-title {
  margin-top: 0;
  margin-bottom: 0;
  font-size: 15px;
  color: inherit;
}
.panel-title > a,
.panel-title > small,
.panel-title > .small,
.panel-title > small > a,
.panel-title > .small > a {
  color: inherit;
}
.panel-footer {
  padding: 10px 15px;
  background-color: #f5f5f5;
  border-top: 1px solid #ddd;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .list-group,
.panel > .panel-collapse > .list-group {
  margin-bottom: 0;
}
.panel > .list-group .list-group-item,
.panel > .panel-collapse > .list-group .list-group-item {
  border-width: 1px 0;
  border-radius: 0;
}
.panel > .list-group:first-child .list-group-item:first-child,
.panel > .panel-collapse > .list-group:first-child .list-group-item:first-child {
  border-top: 0;
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .list-group:last-child .list-group-item:last-child,
.panel > .panel-collapse > .list-group:last-child .list-group-item:last-child {
  border-bottom: 0;
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .panel-heading + .panel-collapse > .list-group .list-group-item:first-child {
  border-top-right-radius: 0;
  border-top-left-radius: 0;
}
.panel-heading + .list-group .list-group-item:first-child {
  border-top-width: 0;
}
.list-group + .panel-footer {
  border-top-width: 0;
}
.panel > .table,
.panel > .table-responsive > .table,
.panel > .panel-collapse > .table {
  margin-bottom: 0;
}
.panel > .table caption,
.panel > .table-responsive > .table caption,
.panel > .panel-collapse > .table caption {
  padding-left: 15px;
  padding-right: 15px;
}
.panel > .table:first-child,
.panel > .table-responsive:first-child > .table:first-child {
  border-top-right-radius: 1px;
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child {
  border-top-left-radius: 1px;
  border-top-right-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:first-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:first-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:first-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:first-child {
  border-top-left-radius: 1px;
}
.panel > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child td:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child td:last-child,
.panel > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > thead:first-child > tr:first-child th:last-child,
.panel > .table:first-child > tbody:first-child > tr:first-child th:last-child,
.panel > .table-responsive:first-child > .table:first-child > tbody:first-child > tr:first-child th:last-child {
  border-top-right-radius: 1px;
}
.panel > .table:last-child,
.panel > .table-responsive:last-child > .table:last-child {
  border-bottom-right-radius: 1px;
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child {
  border-bottom-left-radius: 1px;
  border-bottom-right-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:first-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:first-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:first-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:first-child {
  border-bottom-left-radius: 1px;
}
.panel > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child td:last-child,
.panel > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tbody:last-child > tr:last-child th:last-child,
.panel > .table:last-child > tfoot:last-child > tr:last-child th:last-child,
.panel > .table-responsive:last-child > .table:last-child > tfoot:last-child > tr:last-child th:last-child {
  border-bottom-right-radius: 1px;
}
.panel > .panel-body + .table,
.panel > .panel-body + .table-responsive,
.panel > .table + .panel-body,
.panel > .table-responsive + .panel-body {
  border-top: 1px solid #ddd;
}
.panel > .table > tbody:first-child > tr:first-child th,
.panel > .table > tbody:first-child > tr:first-child td {
  border-top: 0;
}
.panel > .table-bordered,
.panel > .table-responsive > .table-bordered {
  border: 0;
}
.panel > .table-bordered > thead > tr > th:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:first-child,
.panel > .table-bordered > tbody > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:first-child,
.panel > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:first-child,
.panel > .table-bordered > thead > tr > td:first-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:first-child,
.panel > .table-bordered > tbody > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:first-child,
.panel > .table-bordered > tfoot > tr > td:first-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:first-child {
  border-left: 0;
}
.panel > .table-bordered > thead > tr > th:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > th:last-child,
.panel > .table-bordered > tbody > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > th:last-child,
.panel > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > th:last-child,
.panel > .table-bordered > thead > tr > td:last-child,
.panel > .table-responsive > .table-bordered > thead > tr > td:last-child,
.panel > .table-bordered > tbody > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tbody > tr > td:last-child,
.panel > .table-bordered > tfoot > tr > td:last-child,
.panel > .table-responsive > .table-bordered > tfoot > tr > td:last-child {
  border-right: 0;
}
.panel > .table-bordered > thead > tr:first-child > td,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > td,
.panel > .table-bordered > tbody > tr:first-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > td,
.panel > .table-bordered > thead > tr:first-child > th,
.panel > .table-responsive > .table-bordered > thead > tr:first-child > th,
.panel > .table-bordered > tbody > tr:first-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:first-child > th {
  border-bottom: 0;
}
.panel > .table-bordered > tbody > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > td,
.panel > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > td,
.panel > .table-bordered > tbody > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tbody > tr:last-child > th,
.panel > .table-bordered > tfoot > tr:last-child > th,
.panel > .table-responsive > .table-bordered > tfoot > tr:last-child > th {
  border-bottom: 0;
}
.panel > .table-responsive {
  border: 0;
  margin-bottom: 0;
}
.panel-group {
  margin-bottom: 18px;
}
.panel-group .panel {
  margin-bottom: 0;
  border-radius: 2px;
}
.panel-group .panel + .panel {
  margin-top: 5px;
}
.panel-group .panel-heading {
  border-bottom: 0;
}
.panel-group .panel-heading + .panel-collapse > .panel-body,
.panel-group .panel-heading + .panel-collapse > .list-group {
  border-top: 1px solid #ddd;
}
.panel-group .panel-footer {
  border-top: 0;
}
.panel-group .panel-footer + .panel-collapse .panel-body {
  border-bottom: 1px solid #ddd;
}
.panel-default {
  border-color: #ddd;
}
.panel-default > .panel-heading {
  color: #333333;
  background-color: #f5f5f5;
  border-color: #ddd;
}
.panel-default > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ddd;
}
.panel-default > .panel-heading .badge {
  color: #f5f5f5;
  background-color: #333333;
}
.panel-default > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ddd;
}
.panel-primary {
  border-color: #337ab7;
}
.panel-primary > .panel-heading {
  color: #fff;
  background-color: #337ab7;
  border-color: #337ab7;
}
.panel-primary > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #337ab7;
}
.panel-primary > .panel-heading .badge {
  color: #337ab7;
  background-color: #fff;
}
.panel-primary > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #337ab7;
}
.panel-success {
  border-color: #d6e9c6;
}
.panel-success > .panel-heading {
  color: #3c763d;
  background-color: #dff0d8;
  border-color: #d6e9c6;
}
.panel-success > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #d6e9c6;
}
.panel-success > .panel-heading .badge {
  color: #dff0d8;
  background-color: #3c763d;
}
.panel-success > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #d6e9c6;
}
.panel-info {
  border-color: #bce8f1;
}
.panel-info > .panel-heading {
  color: #31708f;
  background-color: #d9edf7;
  border-color: #bce8f1;
}
.panel-info > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #bce8f1;
}
.panel-info > .panel-heading .badge {
  color: #d9edf7;
  background-color: #31708f;
}
.panel-info > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #bce8f1;
}
.panel-warning {
  border-color: #faebcc;
}
.panel-warning > .panel-heading {
  color: #8a6d3b;
  background-color: #fcf8e3;
  border-color: #faebcc;
}
.panel-warning > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #faebcc;
}
.panel-warning > .panel-heading .badge {
  color: #fcf8e3;
  background-color: #8a6d3b;
}
.panel-warning > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #faebcc;
}
.panel-danger {
  border-color: #ebccd1;
}
.panel-danger > .panel-heading {
  color: #a94442;
  background-color: #f2dede;
  border-color: #ebccd1;
}
.panel-danger > .panel-heading + .panel-collapse > .panel-body {
  border-top-color: #ebccd1;
}
.panel-danger > .panel-heading .badge {
  color: #f2dede;
  background-color: #a94442;
}
.panel-danger > .panel-footer + .panel-collapse > .panel-body {
  border-bottom-color: #ebccd1;
}
.embed-responsive {
  position: relative;
  display: block;
  height: 0;
  padding: 0;
  overflow: hidden;
}
.embed-responsive .embed-responsive-item,
.embed-responsive iframe,
.embed-responsive embed,
.embed-responsive object,
.embed-responsive video {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  height: 100%;
  width: 100%;
  border: 0;
}
.embed-responsive-16by9 {
  padding-bottom: 56.25%;
}
.embed-responsive-4by3 {
  padding-bottom: 75%;
}
.well {
  min-height: 20px;
  padding: 19px;
  margin-bottom: 20px;
  background-color: #f5f5f5;
  border: 1px solid #e3e3e3;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.05);
}
.well blockquote {
  border-color: #ddd;
  border-color: rgba(0, 0, 0, 0.15);
}
.well-lg {
  padding: 24px;
  border-radius: 3px;
}
.well-sm {
  padding: 9px;
  border-radius: 1px;
}
.close {
  float: right;
  font-size: 19.5px;
  font-weight: bold;
  line-height: 1;
  color: #000;
  text-shadow: 0 1px 0 #fff;
  opacity: 0.2;
  filter: alpha(opacity=20);
}
.close:hover,
.close:focus {
  color: #000;
  text-decoration: none;
  cursor: pointer;
  opacity: 0.5;
  filter: alpha(opacity=50);
}
button.close {
  padding: 0;
  cursor: pointer;
  background: transparent;
  border: 0;
  -webkit-appearance: none;
}
.modal-open {
  overflow: hidden;
}
.modal {
  display: none;
  overflow: hidden;
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1050;
  -webkit-overflow-scrolling: touch;
  outline: 0;
}
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, -25%);
  -ms-transform: translate(0, -25%);
  -o-transform: translate(0, -25%);
  transform: translate(0, -25%);
  -webkit-transition: -webkit-transform 0.3s ease-out;
  -moz-transition: -moz-transform 0.3s ease-out;
  -o-transition: -o-transform 0.3s ease-out;
  transition: transform 0.3s ease-out;
}
.modal.in .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
.modal-open .modal {
  overflow-x: hidden;
  overflow-y: auto;
}
.modal-dialog {
  position: relative;
  width: auto;
  margin: 10px;
}
.modal-content {
  position: relative;
  background-color: #fff;
  border: 1px solid #999;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  box-shadow: 0 3px 9px rgba(0, 0, 0, 0.5);
  background-clip: padding-box;
  outline: 0;
}
.modal-backdrop {
  position: fixed;
  top: 0;
  right: 0;
  bottom: 0;
  left: 0;
  z-index: 1040;
  background-color: #000;
}
.modal-backdrop.fade {
  opacity: 0;
  filter: alpha(opacity=0);
}
.modal-backdrop.in {
  opacity: 0.5;
  filter: alpha(opacity=50);
}
.modal-header {
  padding: 15px;
  border-bottom: 1px solid #e5e5e5;
}
.modal-header .close {
  margin-top: -2px;
}
.modal-title {
  margin: 0;
  line-height: 1.42857143;
}
.modal-body {
  position: relative;
  padding: 15px;
}
.modal-footer {
  padding: 15px;
  text-align: right;
  border-top: 1px solid #e5e5e5;
}
.modal-footer .btn + .btn {
  margin-left: 5px;
  margin-bottom: 0;
}
.modal-footer .btn-group .btn + .btn {
  margin-left: -1px;
}
.modal-footer .btn-block + .btn-block {
  margin-left: 0;
}
.modal-scrollbar-measure {
  position: absolute;
  top: -9999px;
  width: 50px;
  height: 50px;
  overflow: scroll;
}
@media (min-width: 768px) {
  .modal-dialog {
    width: 600px;
    margin: 30px auto;
  }
  .modal-content {
    -webkit-box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
    box-shadow: 0 5px 15px rgba(0, 0, 0, 0.5);
  }
  .modal-sm {
    width: 300px;
  }
}
@media (min-width: 992px) {
  .modal-lg {
    width: 900px;
  }
}
.tooltip {
  position: absolute;
  z-index: 1070;
  display: block;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 12px;
  opacity: 0;
  filter: alpha(opacity=0);
}
.tooltip.in {
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.tooltip.top {
  margin-top: -3px;
  padding: 5px 0;
}
.tooltip.right {
  margin-left: 3px;
  padding: 0 5px;
}
.tooltip.bottom {
  margin-top: 3px;
  padding: 5px 0;
}
.tooltip.left {
  margin-left: -3px;
  padding: 0 5px;
}
.tooltip-inner {
  max-width: 200px;
  padding: 3px 8px;
  color: #fff;
  text-align: center;
  background-color: #000;
  border-radius: 2px;
}
.tooltip-arrow {
  position: absolute;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.tooltip.top .tooltip-arrow {
  bottom: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-left .tooltip-arrow {
  bottom: 0;
  right: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.top-right .tooltip-arrow {
  bottom: 0;
  left: 5px;
  margin-bottom: -5px;
  border-width: 5px 5px 0;
  border-top-color: #000;
}
.tooltip.right .tooltip-arrow {
  top: 50%;
  left: 0;
  margin-top: -5px;
  border-width: 5px 5px 5px 0;
  border-right-color: #000;
}
.tooltip.left .tooltip-arrow {
  top: 50%;
  right: 0;
  margin-top: -5px;
  border-width: 5px 0 5px 5px;
  border-left-color: #000;
}
.tooltip.bottom .tooltip-arrow {
  top: 0;
  left: 50%;
  margin-left: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-left .tooltip-arrow {
  top: 0;
  right: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.tooltip.bottom-right .tooltip-arrow {
  top: 0;
  left: 5px;
  margin-top: -5px;
  border-width: 0 5px 5px;
  border-bottom-color: #000;
}
.popover {
  position: absolute;
  top: 0;
  left: 0;
  z-index: 1060;
  display: none;
  max-width: 276px;
  padding: 1px;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
  font-style: normal;
  font-weight: normal;
  letter-spacing: normal;
  line-break: auto;
  line-height: 1.42857143;
  text-align: left;
  text-align: start;
  text-decoration: none;
  text-shadow: none;
  text-transform: none;
  white-space: normal;
  word-break: normal;
  word-spacing: normal;
  word-wrap: normal;
  font-size: 13px;
  background-color: #fff;
  background-clip: padding-box;
  border: 1px solid #ccc;
  border: 1px solid rgba(0, 0, 0, 0.2);
  border-radius: 3px;
  -webkit-box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
  box-shadow: 0 5px 10px rgba(0, 0, 0, 0.2);
}
.popover.top {
  margin-top: -10px;
}
.popover.right {
  margin-left: 10px;
}
.popover.bottom {
  margin-top: 10px;
}
.popover.left {
  margin-left: -10px;
}
.popover-title {
  margin: 0;
  padding: 8px 14px;
  font-size: 13px;
  background-color: #f7f7f7;
  border-bottom: 1px solid #ebebeb;
  border-radius: 2px 2px 0 0;
}
.popover-content {
  padding: 9px 14px;
}
.popover > .arrow,
.popover > .arrow:after {
  position: absolute;
  display: block;
  width: 0;
  height: 0;
  border-color: transparent;
  border-style: solid;
}
.popover > .arrow {
  border-width: 11px;
}
.popover > .arrow:after {
  border-width: 10px;
  content: "";
}
.popover.top > .arrow {
  left: 50%;
  margin-left: -11px;
  border-bottom-width: 0;
  border-top-color: #999999;
  border-top-color: rgba(0, 0, 0, 0.25);
  bottom: -11px;
}
.popover.top > .arrow:after {
  content: " ";
  bottom: 1px;
  margin-left: -10px;
  border-bottom-width: 0;
  border-top-color: #fff;
}
.popover.right > .arrow {
  top: 50%;
  left: -11px;
  margin-top: -11px;
  border-left-width: 0;
  border-right-color: #999999;
  border-right-color: rgba(0, 0, 0, 0.25);
}
.popover.right > .arrow:after {
  content: " ";
  left: 1px;
  bottom: -10px;
  border-left-width: 0;
  border-right-color: #fff;
}
.popover.bottom > .arrow {
  left: 50%;
  margin-left: -11px;
  border-top-width: 0;
  border-bottom-color: #999999;
  border-bottom-color: rgba(0, 0, 0, 0.25);
  top: -11px;
}
.popover.bottom > .arrow:after {
  content: " ";
  top: 1px;
  margin-left: -10px;
  border-top-width: 0;
  border-bottom-color: #fff;
}
.popover.left > .arrow {
  top: 50%;
  right: -11px;
  margin-top: -11px;
  border-right-width: 0;
  border-left-color: #999999;
  border-left-color: rgba(0, 0, 0, 0.25);
}
.popover.left > .arrow:after {
  content: " ";
  right: 1px;
  border-right-width: 0;
  border-left-color: #fff;
  bottom: -10px;
}
.carousel {
  position: relative;
}
.carousel-inner {
  position: relative;
  overflow: hidden;
  width: 100%;
}
.carousel-inner > .item {
  display: none;
  position: relative;
  -webkit-transition: 0.6s ease-in-out left;
  -o-transition: 0.6s ease-in-out left;
  transition: 0.6s ease-in-out left;
}
.carousel-inner > .item > img,
.carousel-inner > .item > a > img {
  line-height: 1;
}
@media all and (transform-3d), (-webkit-transform-3d) {
  .carousel-inner > .item {
    -webkit-transition: -webkit-transform 0.6s ease-in-out;
    -moz-transition: -moz-transform 0.6s ease-in-out;
    -o-transition: -o-transform 0.6s ease-in-out;
    transition: transform 0.6s ease-in-out;
    -webkit-backface-visibility: hidden;
    -moz-backface-visibility: hidden;
    backface-visibility: hidden;
    -webkit-perspective: 1000px;
    -moz-perspective: 1000px;
    perspective: 1000px;
  }
  .carousel-inner > .item.next,
  .carousel-inner > .item.active.right {
    -webkit-transform: translate3d(100%, 0, 0);
    transform: translate3d(100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.prev,
  .carousel-inner > .item.active.left {
    -webkit-transform: translate3d(-100%, 0, 0);
    transform: translate3d(-100%, 0, 0);
    left: 0;
  }
  .carousel-inner > .item.next.left,
  .carousel-inner > .item.prev.right,
  .carousel-inner > .item.active {
    -webkit-transform: translate3d(0, 0, 0);
    transform: translate3d(0, 0, 0);
    left: 0;
  }
}
.carousel-inner > .active,
.carousel-inner > .next,
.carousel-inner > .prev {
  display: block;
}
.carousel-inner > .active {
  left: 0;
}
.carousel-inner > .next,
.carousel-inner > .prev {
  position: absolute;
  top: 0;
  width: 100%;
}
.carousel-inner > .next {
  left: 100%;
}
.carousel-inner > .prev {
  left: -100%;
}
.carousel-inner > .next.left,
.carousel-inner > .prev.right {
  left: 0;
}
.carousel-inner > .active.left {
  left: -100%;
}
.carousel-inner > .active.right {
  left: 100%;
}
.carousel-control {
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0;
  width: 15%;
  opacity: 0.5;
  filter: alpha(opacity=50);
  font-size: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
  background-color: rgba(0, 0, 0, 0);
}
.carousel-control.left {
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.5) 0%, rgba(0, 0, 0, 0.0001) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#80000000', endColorstr='#00000000', GradientType=1);
}
.carousel-control.right {
  left: auto;
  right: 0;
  background-image: -webkit-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: -o-linear-gradient(left, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-image: linear-gradient(to right, rgba(0, 0, 0, 0.0001) 0%, rgba(0, 0, 0, 0.5) 100%);
  background-repeat: repeat-x;
  filter: progid:DXImageTransform.Microsoft.gradient(startColorstr='#00000000', endColorstr='#80000000', GradientType=1);
}
.carousel-control:hover,
.carousel-control:focus {
  outline: 0;
  color: #fff;
  text-decoration: none;
  opacity: 0.9;
  filter: alpha(opacity=90);
}
.carousel-control .icon-prev,
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-left,
.carousel-control .glyphicon-chevron-right {
  position: absolute;
  top: 50%;
  margin-top: -10px;
  z-index: 5;
  display: inline-block;
}
.carousel-control .icon-prev,
.carousel-control .glyphicon-chevron-left {
  left: 50%;
  margin-left: -10px;
}
.carousel-control .icon-next,
.carousel-control .glyphicon-chevron-right {
  right: 50%;
  margin-right: -10px;
}
.carousel-control .icon-prev,
.carousel-control .icon-next {
  width: 20px;
  height: 20px;
  line-height: 1;
  font-family: serif;
}
.carousel-control .icon-prev:before {
  content: '\2039';
}
.carousel-control .icon-next:before {
  content: '\203a';
}
.carousel-indicators {
  position: absolute;
  bottom: 10px;
  left: 50%;
  z-index: 15;
  width: 60%;
  margin-left: -30%;
  padding-left: 0;
  list-style: none;
  text-align: center;
}
.carousel-indicators li {
  display: inline-block;
  width: 10px;
  height: 10px;
  margin: 1px;
  text-indent: -999px;
  border: 1px solid #fff;
  border-radius: 10px;
  cursor: pointer;
  background-color: #000 \9;
  background-color: rgba(0, 0, 0, 0);
}
.carousel-indicators .active {
  margin: 0;
  width: 12px;
  height: 12px;
  background-color: #fff;
}
.carousel-caption {
  position: absolute;
  left: 15%;
  right: 15%;
  bottom: 20px;
  z-index: 10;
  padding-top: 20px;
  padding-bottom: 20px;
  color: #fff;
  text-align: center;
  text-shadow: 0 1px 2px rgba(0, 0, 0, 0.6);
}
.carousel-caption .btn {
  text-shadow: none;
}
@media screen and (min-width: 768px) {
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-prev,
  .carousel-control .icon-next {
    width: 30px;
    height: 30px;
    margin-top: -10px;
    font-size: 30px;
  }
  .carousel-control .glyphicon-chevron-left,
  .carousel-control .icon-prev {
    margin-left: -10px;
  }
  .carousel-control .glyphicon-chevron-right,
  .carousel-control .icon-next {
    margin-right: -10px;
  }
  .carousel-caption {
    left: 20%;
    right: 20%;
    padding-bottom: 30px;
  }
  .carousel-indicators {
    bottom: 20px;
  }
}
.clearfix:before,
.clearfix:after,
.dl-horizontal dd:before,
.dl-horizontal dd:after,
.container:before,
.container:after,
.container-fluid:before,
.container-fluid:after,
.row:before,
.row:after,
.form-horizontal .form-group:before,
.form-horizontal .form-group:after,
.btn-toolbar:before,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:before,
.btn-group-vertical > .btn-group:after,
.nav:before,
.nav:after,
.navbar:before,
.navbar:after,
.navbar-header:before,
.navbar-header:after,
.navbar-collapse:before,
.navbar-collapse:after,
.pager:before,
.pager:after,
.panel-body:before,
.panel-body:after,
.modal-header:before,
.modal-header:after,
.modal-footer:before,
.modal-footer:after,
.item_buttons:before,
.item_buttons:after {
  content: " ";
  display: table;
}
.clearfix:after,
.dl-horizontal dd:after,
.container:after,
.container-fluid:after,
.row:after,
.form-horizontal .form-group:after,
.btn-toolbar:after,
.btn-group-vertical > .btn-group:after,
.nav:after,
.navbar:after,
.navbar-header:after,
.navbar-collapse:after,
.pager:after,
.panel-body:after,
.modal-header:after,
.modal-footer:after,
.item_buttons:after {
  clear: both;
}
.center-block {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.pull-right {
  float: right !important;
}
.pull-left {
  float: left !important;
}
.hide {
  display: none !important;
}
.show {
  display: block !important;
}
.invisible {
  visibility: hidden;
}
.text-hide {
  font: 0/0 a;
  color: transparent;
  text-shadow: none;
  background-color: transparent;
  border: 0;
}
.hidden {
  display: none !important;
}
.affix {
  position: fixed;
}
@-ms-viewport {
  width: device-width;
}
.visible-xs,
.visible-sm,
.visible-md,
.visible-lg {
  display: none !important;
}
.visible-xs-block,
.visible-xs-inline,
.visible-xs-inline-block,
.visible-sm-block,
.visible-sm-inline,
.visible-sm-inline-block,
.visible-md-block,
.visible-md-inline,
.visible-md-inline-block,
.visible-lg-block,
.visible-lg-inline,
.visible-lg-inline-block {
  display: none !important;
}
@media (max-width: 767px) {
  .visible-xs {
    display: block !important;
  }
  table.visible-xs {
    display: table !important;
  }
  tr.visible-xs {
    display: table-row !important;
  }
  th.visible-xs,
  td.visible-xs {
    display: table-cell !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-block {
    display: block !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline {
    display: inline !important;
  }
}
@media (max-width: 767px) {
  .visible-xs-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm {
    display: block !important;
  }
  table.visible-sm {
    display: table !important;
  }
  tr.visible-sm {
    display: table-row !important;
  }
  th.visible-sm,
  td.visible-sm {
    display: table-cell !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-block {
    display: block !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline {
    display: inline !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .visible-sm-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md {
    display: block !important;
  }
  table.visible-md {
    display: table !important;
  }
  tr.visible-md {
    display: table-row !important;
  }
  th.visible-md,
  td.visible-md {
    display: table-cell !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-block {
    display: block !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline {
    display: inline !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .visible-md-inline-block {
    display: inline-block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg {
    display: block !important;
  }
  table.visible-lg {
    display: table !important;
  }
  tr.visible-lg {
    display: table-row !important;
  }
  th.visible-lg,
  td.visible-lg {
    display: table-cell !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-block {
    display: block !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline {
    display: inline !important;
  }
}
@media (min-width: 1200px) {
  .visible-lg-inline-block {
    display: inline-block !important;
  }
}
@media (max-width: 767px) {
  .hidden-xs {
    display: none !important;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  .hidden-sm {
    display: none !important;
  }
}
@media (min-width: 992px) and (max-width: 1199px) {
  .hidden-md {
    display: none !important;
  }
}
@media (min-width: 1200px) {
  .hidden-lg {
    display: none !important;
  }
}
.visible-print {
  display: none !important;
}
@media print {
  .visible-print {
    display: block !important;
  }
  table.visible-print {
    display: table !important;
  }
  tr.visible-print {
    display: table-row !important;
  }
  th.visible-print,
  td.visible-print {
    display: table-cell !important;
  }
}
.visible-print-block {
  display: none !important;
}
@media print {
  .visible-print-block {
    display: block !important;
  }
}
.visible-print-inline {
  display: none !important;
}
@media print {
  .visible-print-inline {
    display: inline !important;
  }
}
.visible-print-inline-block {
  display: none !important;
}
@media print {
  .visible-print-inline-block {
    display: inline-block !important;
  }
}
@media print {
  .hidden-print {
    display: none !important;
  }
}
/*!
*
* Font Awesome
*
*/
/*!
 *  Font Awesome 4.2.0 by @davegandy - http://fontawesome.io - @fontawesome
 *  License - http://fontawesome.io/license (Font: SIL OFL 1.1, CSS: MIT License)
 */
/* FONT PATH
 * -------------------------- */
@font-face {
  font-family: 'FontAwesome';
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?v=4.2.0');
  src: url('../components/font-awesome/fonts/fontawesome-webfont.eot?#iefix&v=4.2.0') format('embedded-opentype'), url('../components/font-awesome/fonts/fontawesome-webfont.woff?v=4.2.0') format('woff'), url('../components/font-awesome/fonts/fontawesome-webfont.ttf?v=4.2.0') format('truetype'), url('../components/font-awesome/fonts/fontawesome-webfont.svg?v=4.2.0#fontawesomeregular') format('svg');
  font-weight: normal;
  font-style: normal;
}
.fa {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
}
/* makes the font 33% larger relative to the icon container */
.fa-lg {
  font-size: 1.33333333em;
  line-height: 0.75em;
  vertical-align: -15%;
}
.fa-2x {
  font-size: 2em;
}
.fa-3x {
  font-size: 3em;
}
.fa-4x {
  font-size: 4em;
}
.fa-5x {
  font-size: 5em;
}
.fa-fw {
  width: 1.28571429em;
  text-align: center;
}
.fa-ul {
  padding-left: 0;
  margin-left: 2.14285714em;
  list-style-type: none;
}
.fa-ul > li {
  position: relative;
}
.fa-li {
  position: absolute;
  left: -2.14285714em;
  width: 2.14285714em;
  top: 0.14285714em;
  text-align: center;
}
.fa-li.fa-lg {
  left: -1.85714286em;
}
.fa-border {
  padding: .2em .25em .15em;
  border: solid 0.08em #eee;
  border-radius: .1em;
}
.pull-right {
  float: right;
}
.pull-left {
  float: left;
}
.fa.pull-left {
  margin-right: .3em;
}
.fa.pull-right {
  margin-left: .3em;
}
.fa-spin {
  -webkit-animation: fa-spin 2s infinite linear;
  animation: fa-spin 2s infinite linear;
}
@-webkit-keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
@keyframes fa-spin {
  0% {
    -webkit-transform: rotate(0deg);
    transform: rotate(0deg);
  }
  100% {
    -webkit-transform: rotate(359deg);
    transform: rotate(359deg);
  }
}
.fa-rotate-90 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=1);
  -webkit-transform: rotate(90deg);
  -ms-transform: rotate(90deg);
  transform: rotate(90deg);
}
.fa-rotate-180 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2);
  -webkit-transform: rotate(180deg);
  -ms-transform: rotate(180deg);
  transform: rotate(180deg);
}
.fa-rotate-270 {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=3);
  -webkit-transform: rotate(270deg);
  -ms-transform: rotate(270deg);
  transform: rotate(270deg);
}
.fa-flip-horizontal {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=0, mirror=1);
  -webkit-transform: scale(-1, 1);
  -ms-transform: scale(-1, 1);
  transform: scale(-1, 1);
}
.fa-flip-vertical {
  filter: progid:DXImageTransform.Microsoft.BasicImage(rotation=2, mirror=1);
  -webkit-transform: scale(1, -1);
  -ms-transform: scale(1, -1);
  transform: scale(1, -1);
}
:root .fa-rotate-90,
:root .fa-rotate-180,
:root .fa-rotate-270,
:root .fa-flip-horizontal,
:root .fa-flip-vertical {
  filter: none;
}
.fa-stack {
  position: relative;
  display: inline-block;
  width: 2em;
  height: 2em;
  line-height: 2em;
  vertical-align: middle;
}
.fa-stack-1x,
.fa-stack-2x {
  position: absolute;
  left: 0;
  width: 100%;
  text-align: center;
}
.fa-stack-1x {
  line-height: inherit;
}
.fa-stack-2x {
  font-size: 2em;
}
.fa-inverse {
  color: #fff;
}
/* Font Awesome uses the Unicode Private Use Area (PUA) to ensure screen
   readers do not read off random characters that represent icons */
.fa-glass:before {
  content: "\f000";
}
.fa-music:before {
  content: "\f001";
}
.fa-search:before {
  content: "\f002";
}
.fa-envelope-o:before {
  content: "\f003";
}
.fa-heart:before {
  content: "\f004";
}
.fa-star:before {
  content: "\f005";
}
.fa-star-o:before {
  content: "\f006";
}
.fa-user:before {
  content: "\f007";
}
.fa-film:before {
  content: "\f008";
}
.fa-th-large:before {
  content: "\f009";
}
.fa-th:before {
  content: "\f00a";
}
.fa-th-list:before {
  content: "\f00b";
}
.fa-check:before {
  content: "\f00c";
}
.fa-remove:before,
.fa-close:before,
.fa-times:before {
  content: "\f00d";
}
.fa-search-plus:before {
  content: "\f00e";
}
.fa-search-minus:before {
  content: "\f010";
}
.fa-power-off:before {
  content: "\f011";
}
.fa-signal:before {
  content: "\f012";
}
.fa-gear:before,
.fa-cog:before {
  content: "\f013";
}
.fa-trash-o:before {
  content: "\f014";
}
.fa-home:before {
  content: "\f015";
}
.fa-file-o:before {
  content: "\f016";
}
.fa-clock-o:before {
  content: "\f017";
}
.fa-road:before {
  content: "\f018";
}
.fa-download:before {
  content: "\f019";
}
.fa-arrow-circle-o-down:before {
  content: "\f01a";
}
.fa-arrow-circle-o-up:before {
  content: "\f01b";
}
.fa-inbox:before {
  content: "\f01c";
}
.fa-play-circle-o:before {
  content: "\f01d";
}
.fa-rotate-right:before,
.fa-repeat:before {
  content: "\f01e";
}
.fa-refresh:before {
  content: "\f021";
}
.fa-list-alt:before {
  content: "\f022";
}
.fa-lock:before {
  content: "\f023";
}
.fa-flag:before {
  content: "\f024";
}
.fa-headphones:before {
  content: "\f025";
}
.fa-volume-off:before {
  content: "\f026";
}
.fa-volume-down:before {
  content: "\f027";
}
.fa-volume-up:before {
  content: "\f028";
}
.fa-qrcode:before {
  content: "\f029";
}
.fa-barcode:before {
  content: "\f02a";
}
.fa-tag:before {
  content: "\f02b";
}
.fa-tags:before {
  content: "\f02c";
}
.fa-book:before {
  content: "\f02d";
}
.fa-bookmark:before {
  content: "\f02e";
}
.fa-print:before {
  content: "\f02f";
}
.fa-camera:before {
  content: "\f030";
}
.fa-font:before {
  content: "\f031";
}
.fa-bold:before {
  content: "\f032";
}
.fa-italic:before {
  content: "\f033";
}
.fa-text-height:before {
  content: "\f034";
}
.fa-text-width:before {
  content: "\f035";
}
.fa-align-left:before {
  content: "\f036";
}
.fa-align-center:before {
  content: "\f037";
}
.fa-align-right:before {
  content: "\f038";
}
.fa-align-justify:before {
  content: "\f039";
}
.fa-list:before {
  content: "\f03a";
}
.fa-dedent:before,
.fa-outdent:before {
  content: "\f03b";
}
.fa-indent:before {
  content: "\f03c";
}
.fa-video-camera:before {
  content: "\f03d";
}
.fa-photo:before,
.fa-image:before,
.fa-picture-o:before {
  content: "\f03e";
}
.fa-pencil:before {
  content: "\f040";
}
.fa-map-marker:before {
  content: "\f041";
}
.fa-adjust:before {
  content: "\f042";
}
.fa-tint:before {
  content: "\f043";
}
.fa-edit:before,
.fa-pencil-square-o:before {
  content: "\f044";
}
.fa-share-square-o:before {
  content: "\f045";
}
.fa-check-square-o:before {
  content: "\f046";
}
.fa-arrows:before {
  content: "\f047";
}
.fa-step-backward:before {
  content: "\f048";
}
.fa-fast-backward:before {
  content: "\f049";
}
.fa-backward:before {
  content: "\f04a";
}
.fa-play:before {
  content: "\f04b";
}
.fa-pause:before {
  content: "\f04c";
}
.fa-stop:before {
  content: "\f04d";
}
.fa-forward:before {
  content: "\f04e";
}
.fa-fast-forward:before {
  content: "\f050";
}
.fa-step-forward:before {
  content: "\f051";
}
.fa-eject:before {
  content: "\f052";
}
.fa-chevron-left:before {
  content: "\f053";
}
.fa-chevron-right:before {
  content: "\f054";
}
.fa-plus-circle:before {
  content: "\f055";
}
.fa-minus-circle:before {
  content: "\f056";
}
.fa-times-circle:before {
  content: "\f057";
}
.fa-check-circle:before {
  content: "\f058";
}
.fa-question-circle:before {
  content: "\f059";
}
.fa-info-circle:before {
  content: "\f05a";
}
.fa-crosshairs:before {
  content: "\f05b";
}
.fa-times-circle-o:before {
  content: "\f05c";
}
.fa-check-circle-o:before {
  content: "\f05d";
}
.fa-ban:before {
  content: "\f05e";
}
.fa-arrow-left:before {
  content: "\f060";
}
.fa-arrow-right:before {
  content: "\f061";
}
.fa-arrow-up:before {
  content: "\f062";
}
.fa-arrow-down:before {
  content: "\f063";
}
.fa-mail-forward:before,
.fa-share:before {
  content: "\f064";
}
.fa-expand:before {
  content: "\f065";
}
.fa-compress:before {
  content: "\f066";
}
.fa-plus:before {
  content: "\f067";
}
.fa-minus:before {
  content: "\f068";
}
.fa-asterisk:before {
  content: "\f069";
}
.fa-exclamation-circle:before {
  content: "\f06a";
}
.fa-gift:before {
  content: "\f06b";
}
.fa-leaf:before {
  content: "\f06c";
}
.fa-fire:before {
  content: "\f06d";
}
.fa-eye:before {
  content: "\f06e";
}
.fa-eye-slash:before {
  content: "\f070";
}
.fa-warning:before,
.fa-exclamation-triangle:before {
  content: "\f071";
}
.fa-plane:before {
  content: "\f072";
}
.fa-calendar:before {
  content: "\f073";
}
.fa-random:before {
  content: "\f074";
}
.fa-comment:before {
  content: "\f075";
}
.fa-magnet:before {
  content: "\f076";
}
.fa-chevron-up:before {
  content: "\f077";
}
.fa-chevron-down:before {
  content: "\f078";
}
.fa-retweet:before {
  content: "\f079";
}
.fa-shopping-cart:before {
  content: "\f07a";
}
.fa-folder:before {
  content: "\f07b";
}
.fa-folder-open:before {
  content: "\f07c";
}
.fa-arrows-v:before {
  content: "\f07d";
}
.fa-arrows-h:before {
  content: "\f07e";
}
.fa-bar-chart-o:before,
.fa-bar-chart:before {
  content: "\f080";
}
.fa-twitter-square:before {
  content: "\f081";
}
.fa-facebook-square:before {
  content: "\f082";
}
.fa-camera-retro:before {
  content: "\f083";
}
.fa-key:before {
  content: "\f084";
}
.fa-gears:before,
.fa-cogs:before {
  content: "\f085";
}
.fa-comments:before {
  content: "\f086";
}
.fa-thumbs-o-up:before {
  content: "\f087";
}
.fa-thumbs-o-down:before {
  content: "\f088";
}
.fa-star-half:before {
  content: "\f089";
}
.fa-heart-o:before {
  content: "\f08a";
}
.fa-sign-out:before {
  content: "\f08b";
}
.fa-linkedin-square:before {
  content: "\f08c";
}
.fa-thumb-tack:before {
  content: "\f08d";
}
.fa-external-link:before {
  content: "\f08e";
}
.fa-sign-in:before {
  content: "\f090";
}
.fa-trophy:before {
  content: "\f091";
}
.fa-github-square:before {
  content: "\f092";
}
.fa-upload:before {
  content: "\f093";
}
.fa-lemon-o:before {
  content: "\f094";
}
.fa-phone:before {
  content: "\f095";
}
.fa-square-o:before {
  content: "\f096";
}
.fa-bookmark-o:before {
  content: "\f097";
}
.fa-phone-square:before {
  content: "\f098";
}
.fa-twitter:before {
  content: "\f099";
}
.fa-facebook:before {
  content: "\f09a";
}
.fa-github:before {
  content: "\f09b";
}
.fa-unlock:before {
  content: "\f09c";
}
.fa-credit-card:before {
  content: "\f09d";
}
.fa-rss:before {
  content: "\f09e";
}
.fa-hdd-o:before {
  content: "\f0a0";
}
.fa-bullhorn:before {
  content: "\f0a1";
}
.fa-bell:before {
  content: "\f0f3";
}
.fa-certificate:before {
  content: "\f0a3";
}
.fa-hand-o-right:before {
  content: "\f0a4";
}
.fa-hand-o-left:before {
  content: "\f0a5";
}
.fa-hand-o-up:before {
  content: "\f0a6";
}
.fa-hand-o-down:before {
  content: "\f0a7";
}
.fa-arrow-circle-left:before {
  content: "\f0a8";
}
.fa-arrow-circle-right:before {
  content: "\f0a9";
}
.fa-arrow-circle-up:before {
  content: "\f0aa";
}
.fa-arrow-circle-down:before {
  content: "\f0ab";
}
.fa-globe:before {
  content: "\f0ac";
}
.fa-wrench:before {
  content: "\f0ad";
}
.fa-tasks:before {
  content: "\f0ae";
}
.fa-filter:before {
  content: "\f0b0";
}
.fa-briefcase:before {
  content: "\f0b1";
}
.fa-arrows-alt:before {
  content: "\f0b2";
}
.fa-group:before,
.fa-users:before {
  content: "\f0c0";
}
.fa-chain:before,
.fa-link:before {
  content: "\f0c1";
}
.fa-cloud:before {
  content: "\f0c2";
}
.fa-flask:before {
  content: "\f0c3";
}
.fa-cut:before,
.fa-scissors:before {
  content: "\f0c4";
}
.fa-copy:before,
.fa-files-o:before {
  content: "\f0c5";
}
.fa-paperclip:before {
  content: "\f0c6";
}
.fa-save:before,
.fa-floppy-o:before {
  content: "\f0c7";
}
.fa-square:before {
  content: "\f0c8";
}
.fa-navicon:before,
.fa-reorder:before,
.fa-bars:before {
  content: "\f0c9";
}
.fa-list-ul:before {
  content: "\f0ca";
}
.fa-list-ol:before {
  content: "\f0cb";
}
.fa-strikethrough:before {
  content: "\f0cc";
}
.fa-underline:before {
  content: "\f0cd";
}
.fa-table:before {
  content: "\f0ce";
}
.fa-magic:before {
  content: "\f0d0";
}
.fa-truck:before {
  content: "\f0d1";
}
.fa-pinterest:before {
  content: "\f0d2";
}
.fa-pinterest-square:before {
  content: "\f0d3";
}
.fa-google-plus-square:before {
  content: "\f0d4";
}
.fa-google-plus:before {
  content: "\f0d5";
}
.fa-money:before {
  content: "\f0d6";
}
.fa-caret-down:before {
  content: "\f0d7";
}
.fa-caret-up:before {
  content: "\f0d8";
}
.fa-caret-left:before {
  content: "\f0d9";
}
.fa-caret-right:before {
  content: "\f0da";
}
.fa-columns:before {
  content: "\f0db";
}
.fa-unsorted:before,
.fa-sort:before {
  content: "\f0dc";
}
.fa-sort-down:before,
.fa-sort-desc:before {
  content: "\f0dd";
}
.fa-sort-up:before,
.fa-sort-asc:before {
  content: "\f0de";
}
.fa-envelope:before {
  content: "\f0e0";
}
.fa-linkedin:before {
  content: "\f0e1";
}
.fa-rotate-left:before,
.fa-undo:before {
  content: "\f0e2";
}
.fa-legal:before,
.fa-gavel:before {
  content: "\f0e3";
}
.fa-dashboard:before,
.fa-tachometer:before {
  content: "\f0e4";
}
.fa-comment-o:before {
  content: "\f0e5";
}
.fa-comments-o:before {
  content: "\f0e6";
}
.fa-flash:before,
.fa-bolt:before {
  content: "\f0e7";
}
.fa-sitemap:before {
  content: "\f0e8";
}
.fa-umbrella:before {
  content: "\f0e9";
}
.fa-paste:before,
.fa-clipboard:before {
  content: "\f0ea";
}
.fa-lightbulb-o:before {
  content: "\f0eb";
}
.fa-exchange:before {
  content: "\f0ec";
}
.fa-cloud-download:before {
  content: "\f0ed";
}
.fa-cloud-upload:before {
  content: "\f0ee";
}
.fa-user-md:before {
  content: "\f0f0";
}
.fa-stethoscope:before {
  content: "\f0f1";
}
.fa-suitcase:before {
  content: "\f0f2";
}
.fa-bell-o:before {
  content: "\f0a2";
}
.fa-coffee:before {
  content: "\f0f4";
}
.fa-cutlery:before {
  content: "\f0f5";
}
.fa-file-text-o:before {
  content: "\f0f6";
}
.fa-building-o:before {
  content: "\f0f7";
}
.fa-hospital-o:before {
  content: "\f0f8";
}
.fa-ambulance:before {
  content: "\f0f9";
}
.fa-medkit:before {
  content: "\f0fa";
}
.fa-fighter-jet:before {
  content: "\f0fb";
}
.fa-beer:before {
  content: "\f0fc";
}
.fa-h-square:before {
  content: "\f0fd";
}
.fa-plus-square:before {
  content: "\f0fe";
}
.fa-angle-double-left:before {
  content: "\f100";
}
.fa-angle-double-right:before {
  content: "\f101";
}
.fa-angle-double-up:before {
  content: "\f102";
}
.fa-angle-double-down:before {
  content: "\f103";
}
.fa-angle-left:before {
  content: "\f104";
}
.fa-angle-right:before {
  content: "\f105";
}
.fa-angle-up:before {
  content: "\f106";
}
.fa-angle-down:before {
  content: "\f107";
}
.fa-desktop:before {
  content: "\f108";
}
.fa-laptop:before {
  content: "\f109";
}
.fa-tablet:before {
  content: "\f10a";
}
.fa-mobile-phone:before,
.fa-mobile:before {
  content: "\f10b";
}
.fa-circle-o:before {
  content: "\f10c";
}
.fa-quote-left:before {
  content: "\f10d";
}
.fa-quote-right:before {
  content: "\f10e";
}
.fa-spinner:before {
  content: "\f110";
}
.fa-circle:before {
  content: "\f111";
}
.fa-mail-reply:before,
.fa-reply:before {
  content: "\f112";
}
.fa-github-alt:before {
  content: "\f113";
}
.fa-folder-o:before {
  content: "\f114";
}
.fa-folder-open-o:before {
  content: "\f115";
}
.fa-smile-o:before {
  content: "\f118";
}
.fa-frown-o:before {
  content: "\f119";
}
.fa-meh-o:before {
  content: "\f11a";
}
.fa-gamepad:before {
  content: "\f11b";
}
.fa-keyboard-o:before {
  content: "\f11c";
}
.fa-flag-o:before {
  content: "\f11d";
}
.fa-flag-checkered:before {
  content: "\f11e";
}
.fa-terminal:before {
  content: "\f120";
}
.fa-code:before {
  content: "\f121";
}
.fa-mail-reply-all:before,
.fa-reply-all:before {
  content: "\f122";
}
.fa-star-half-empty:before,
.fa-star-half-full:before,
.fa-star-half-o:before {
  content: "\f123";
}
.fa-location-arrow:before {
  content: "\f124";
}
.fa-crop:before {
  content: "\f125";
}
.fa-code-fork:before {
  content: "\f126";
}
.fa-unlink:before,
.fa-chain-broken:before {
  content: "\f127";
}
.fa-question:before {
  content: "\f128";
}
.fa-info:before {
  content: "\f129";
}
.fa-exclamation:before {
  content: "\f12a";
}
.fa-superscript:before {
  content: "\f12b";
}
.fa-subscript:before {
  content: "\f12c";
}
.fa-eraser:before {
  content: "\f12d";
}
.fa-puzzle-piece:before {
  content: "\f12e";
}
.fa-microphone:before {
  content: "\f130";
}
.fa-microphone-slash:before {
  content: "\f131";
}
.fa-shield:before {
  content: "\f132";
}
.fa-calendar-o:before {
  content: "\f133";
}
.fa-fire-extinguisher:before {
  content: "\f134";
}
.fa-rocket:before {
  content: "\f135";
}
.fa-maxcdn:before {
  content: "\f136";
}
.fa-chevron-circle-left:before {
  content: "\f137";
}
.fa-chevron-circle-right:before {
  content: "\f138";
}
.fa-chevron-circle-up:before {
  content: "\f139";
}
.fa-chevron-circle-down:before {
  content: "\f13a";
}
.fa-html5:before {
  content: "\f13b";
}
.fa-css3:before {
  content: "\f13c";
}
.fa-anchor:before {
  content: "\f13d";
}
.fa-unlock-alt:before {
  content: "\f13e";
}
.fa-bullseye:before {
  content: "\f140";
}
.fa-ellipsis-h:before {
  content: "\f141";
}
.fa-ellipsis-v:before {
  content: "\f142";
}
.fa-rss-square:before {
  content: "\f143";
}
.fa-play-circle:before {
  content: "\f144";
}
.fa-ticket:before {
  content: "\f145";
}
.fa-minus-square:before {
  content: "\f146";
}
.fa-minus-square-o:before {
  content: "\f147";
}
.fa-level-up:before {
  content: "\f148";
}
.fa-level-down:before {
  content: "\f149";
}
.fa-check-square:before {
  content: "\f14a";
}
.fa-pencil-square:before {
  content: "\f14b";
}
.fa-external-link-square:before {
  content: "\f14c";
}
.fa-share-square:before {
  content: "\f14d";
}
.fa-compass:before {
  content: "\f14e";
}
.fa-toggle-down:before,
.fa-caret-square-o-down:before {
  content: "\f150";
}
.fa-toggle-up:before,
.fa-caret-square-o-up:before {
  content: "\f151";
}
.fa-toggle-right:before,
.fa-caret-square-o-right:before {
  content: "\f152";
}
.fa-euro:before,
.fa-eur:before {
  content: "\f153";
}
.fa-gbp:before {
  content: "\f154";
}
.fa-dollar:before,
.fa-usd:before {
  content: "\f155";
}
.fa-rupee:before,
.fa-inr:before {
  content: "\f156";
}
.fa-cny:before,
.fa-rmb:before,
.fa-yen:before,
.fa-jpy:before {
  content: "\f157";
}
.fa-ruble:before,
.fa-rouble:before,
.fa-rub:before {
  content: "\f158";
}
.fa-won:before,
.fa-krw:before {
  content: "\f159";
}
.fa-bitcoin:before,
.fa-btc:before {
  content: "\f15a";
}
.fa-file:before {
  content: "\f15b";
}
.fa-file-text:before {
  content: "\f15c";
}
.fa-sort-alpha-asc:before {
  content: "\f15d";
}
.fa-sort-alpha-desc:before {
  content: "\f15e";
}
.fa-sort-amount-asc:before {
  content: "\f160";
}
.fa-sort-amount-desc:before {
  content: "\f161";
}
.fa-sort-numeric-asc:before {
  content: "\f162";
}
.fa-sort-numeric-desc:before {
  content: "\f163";
}
.fa-thumbs-up:before {
  content: "\f164";
}
.fa-thumbs-down:before {
  content: "\f165";
}
.fa-youtube-square:before {
  content: "\f166";
}
.fa-youtube:before {
  content: "\f167";
}
.fa-xing:before {
  content: "\f168";
}
.fa-xing-square:before {
  content: "\f169";
}
.fa-youtube-play:before {
  content: "\f16a";
}
.fa-dropbox:before {
  content: "\f16b";
}
.fa-stack-overflow:before {
  content: "\f16c";
}
.fa-instagram:before {
  content: "\f16d";
}
.fa-flickr:before {
  content: "\f16e";
}
.fa-adn:before {
  content: "\f170";
}
.fa-bitbucket:before {
  content: "\f171";
}
.fa-bitbucket-square:before {
  content: "\f172";
}
.fa-tumblr:before {
  content: "\f173";
}
.fa-tumblr-square:before {
  content: "\f174";
}
.fa-long-arrow-down:before {
  content: "\f175";
}
.fa-long-arrow-up:before {
  content: "\f176";
}
.fa-long-arrow-left:before {
  content: "\f177";
}
.fa-long-arrow-right:before {
  content: "\f178";
}
.fa-apple:before {
  content: "\f179";
}
.fa-windows:before {
  content: "\f17a";
}
.fa-android:before {
  content: "\f17b";
}
.fa-linux:before {
  content: "\f17c";
}
.fa-dribbble:before {
  content: "\f17d";
}
.fa-skype:before {
  content: "\f17e";
}
.fa-foursquare:before {
  content: "\f180";
}
.fa-trello:before {
  content: "\f181";
}
.fa-female:before {
  content: "\f182";
}
.fa-male:before {
  content: "\f183";
}
.fa-gittip:before {
  content: "\f184";
}
.fa-sun-o:before {
  content: "\f185";
}
.fa-moon-o:before {
  content: "\f186";
}
.fa-archive:before {
  content: "\f187";
}
.fa-bug:before {
  content: "\f188";
}
.fa-vk:before {
  content: "\f189";
}
.fa-weibo:before {
  content: "\f18a";
}
.fa-renren:before {
  content: "\f18b";
}
.fa-pagelines:before {
  content: "\f18c";
}
.fa-stack-exchange:before {
  content: "\f18d";
}
.fa-arrow-circle-o-right:before {
  content: "\f18e";
}
.fa-arrow-circle-o-left:before {
  content: "\f190";
}
.fa-toggle-left:before,
.fa-caret-square-o-left:before {
  content: "\f191";
}
.fa-dot-circle-o:before {
  content: "\f192";
}
.fa-wheelchair:before {
  content: "\f193";
}
.fa-vimeo-square:before {
  content: "\f194";
}
.fa-turkish-lira:before,
.fa-try:before {
  content: "\f195";
}
.fa-plus-square-o:before {
  content: "\f196";
}
.fa-space-shuttle:before {
  content: "\f197";
}
.fa-slack:before {
  content: "\f198";
}
.fa-envelope-square:before {
  content: "\f199";
}
.fa-wordpress:before {
  content: "\f19a";
}
.fa-openid:before {
  content: "\f19b";
}
.fa-institution:before,
.fa-bank:before,
.fa-university:before {
  content: "\f19c";
}
.fa-mortar-board:before,
.fa-graduation-cap:before {
  content: "\f19d";
}
.fa-yahoo:before {
  content: "\f19e";
}
.fa-google:before {
  content: "\f1a0";
}
.fa-reddit:before {
  content: "\f1a1";
}
.fa-reddit-square:before {
  content: "\f1a2";
}
.fa-stumbleupon-circle:before {
  content: "\f1a3";
}
.fa-stumbleupon:before {
  content: "\f1a4";
}
.fa-delicious:before {
  content: "\f1a5";
}
.fa-digg:before {
  content: "\f1a6";
}
.fa-pied-piper:before {
  content: "\f1a7";
}
.fa-pied-piper-alt:before {
  content: "\f1a8";
}
.fa-drupal:before {
  content: "\f1a9";
}
.fa-joomla:before {
  content: "\f1aa";
}
.fa-language:before {
  content: "\f1ab";
}
.fa-fax:before {
  content: "\f1ac";
}
.fa-building:before {
  content: "\f1ad";
}
.fa-child:before {
  content: "\f1ae";
}
.fa-paw:before {
  content: "\f1b0";
}
.fa-spoon:before {
  content: "\f1b1";
}
.fa-cube:before {
  content: "\f1b2";
}
.fa-cubes:before {
  content: "\f1b3";
}
.fa-behance:before {
  content: "\f1b4";
}
.fa-behance-square:before {
  content: "\f1b5";
}
.fa-steam:before {
  content: "\f1b6";
}
.fa-steam-square:before {
  content: "\f1b7";
}
.fa-recycle:before {
  content: "\f1b8";
}
.fa-automobile:before,
.fa-car:before {
  content: "\f1b9";
}
.fa-cab:before,
.fa-taxi:before {
  content: "\f1ba";
}
.fa-tree:before {
  content: "\f1bb";
}
.fa-spotify:before {
  content: "\f1bc";
}
.fa-deviantart:before {
  content: "\f1bd";
}
.fa-soundcloud:before {
  content: "\f1be";
}
.fa-database:before {
  content: "\f1c0";
}
.fa-file-pdf-o:before {
  content: "\f1c1";
}
.fa-file-word-o:before {
  content: "\f1c2";
}
.fa-file-excel-o:before {
  content: "\f1c3";
}
.fa-file-powerpoint-o:before {
  content: "\f1c4";
}
.fa-file-photo-o:before,
.fa-file-picture-o:before,
.fa-file-image-o:before {
  content: "\f1c5";
}
.fa-file-zip-o:before,
.fa-file-archive-o:before {
  content: "\f1c6";
}
.fa-file-sound-o:before,
.fa-file-audio-o:before {
  content: "\f1c7";
}
.fa-file-movie-o:before,
.fa-file-video-o:before {
  content: "\f1c8";
}
.fa-file-code-o:before {
  content: "\f1c9";
}
.fa-vine:before {
  content: "\f1ca";
}
.fa-codepen:before {
  content: "\f1cb";
}
.fa-jsfiddle:before {
  content: "\f1cc";
}
.fa-life-bouy:before,
.fa-life-buoy:before,
.fa-life-saver:before,
.fa-support:before,
.fa-life-ring:before {
  content: "\f1cd";
}
.fa-circle-o-notch:before {
  content: "\f1ce";
}
.fa-ra:before,
.fa-rebel:before {
  content: "\f1d0";
}
.fa-ge:before,
.fa-empire:before {
  content: "\f1d1";
}
.fa-git-square:before {
  content: "\f1d2";
}
.fa-git:before {
  content: "\f1d3";
}
.fa-hacker-news:before {
  content: "\f1d4";
}
.fa-tencent-weibo:before {
  content: "\f1d5";
}
.fa-qq:before {
  content: "\f1d6";
}
.fa-wechat:before,
.fa-weixin:before {
  content: "\f1d7";
}
.fa-send:before,
.fa-paper-plane:before {
  content: "\f1d8";
}
.fa-send-o:before,
.fa-paper-plane-o:before {
  content: "\f1d9";
}
.fa-history:before {
  content: "\f1da";
}
.fa-circle-thin:before {
  content: "\f1db";
}
.fa-header:before {
  content: "\f1dc";
}
.fa-paragraph:before {
  content: "\f1dd";
}
.fa-sliders:before {
  content: "\f1de";
}
.fa-share-alt:before {
  content: "\f1e0";
}
.fa-share-alt-square:before {
  content: "\f1e1";
}
.fa-bomb:before {
  content: "\f1e2";
}
.fa-soccer-ball-o:before,
.fa-futbol-o:before {
  content: "\f1e3";
}
.fa-tty:before {
  content: "\f1e4";
}
.fa-binoculars:before {
  content: "\f1e5";
}
.fa-plug:before {
  content: "\f1e6";
}
.fa-slideshare:before {
  content: "\f1e7";
}
.fa-twitch:before {
  content: "\f1e8";
}
.fa-yelp:before {
  content: "\f1e9";
}
.fa-newspaper-o:before {
  content: "\f1ea";
}
.fa-wifi:before {
  content: "\f1eb";
}
.fa-calculator:before {
  content: "\f1ec";
}
.fa-paypal:before {
  content: "\f1ed";
}
.fa-google-wallet:before {
  content: "\f1ee";
}
.fa-cc-visa:before {
  content: "\f1f0";
}
.fa-cc-mastercard:before {
  content: "\f1f1";
}
.fa-cc-discover:before {
  content: "\f1f2";
}
.fa-cc-amex:before {
  content: "\f1f3";
}
.fa-cc-paypal:before {
  content: "\f1f4";
}
.fa-cc-stripe:before {
  content: "\f1f5";
}
.fa-bell-slash:before {
  content: "\f1f6";
}
.fa-bell-slash-o:before {
  content: "\f1f7";
}
.fa-trash:before {
  content: "\f1f8";
}
.fa-copyright:before {
  content: "\f1f9";
}
.fa-at:before {
  content: "\f1fa";
}
.fa-eyedropper:before {
  content: "\f1fb";
}
.fa-paint-brush:before {
  content: "\f1fc";
}
.fa-birthday-cake:before {
  content: "\f1fd";
}
.fa-area-chart:before {
  content: "\f1fe";
}
.fa-pie-chart:before {
  content: "\f200";
}
.fa-line-chart:before {
  content: "\f201";
}
.fa-lastfm:before {
  content: "\f202";
}
.fa-lastfm-square:before {
  content: "\f203";
}
.fa-toggle-off:before {
  content: "\f204";
}
.fa-toggle-on:before {
  content: "\f205";
}
.fa-bicycle:before {
  content: "\f206";
}
.fa-bus:before {
  content: "\f207";
}
.fa-ioxhost:before {
  content: "\f208";
}
.fa-angellist:before {
  content: "\f209";
}
.fa-cc:before {
  content: "\f20a";
}
.fa-shekel:before,
.fa-sheqel:before,
.fa-ils:before {
  content: "\f20b";
}
.fa-meanpath:before {
  content: "\f20c";
}
/*!
*
* IPython base
*
*/
.modal.fade .modal-dialog {
  -webkit-transform: translate(0, 0);
  -ms-transform: translate(0, 0);
  -o-transform: translate(0, 0);
  transform: translate(0, 0);
}
code {
  color: #000;
}
pre {
  font-size: inherit;
  line-height: inherit;
}
label {
  font-weight: normal;
}
/* Make the page background atleast 100% the height of the view port */
/* Make the page itself atleast 70% the height of the view port */
.border-box-sizing {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.corner-all {
  border-radius: 2px;
}
.no-padding {
  padding: 0px;
}
/* Flexible box model classes */
/* Taken from Alex Russell http://infrequently.org/2009/08/css-3-progress/ */
/* This file is a compatability layer.  It allows the usage of flexible box 
model layouts accross multiple browsers, including older browsers.  The newest,
universal implementation of the flexible box model is used when available (see
`Modern browsers` comments below).  Browsers that are known to implement this 
new spec completely include:

    Firefox 28.0+
    Chrome 29.0+
    Internet Explorer 11+ 
    Opera 17.0+

Browsers not listed, including Safari, are supported via the styling under the
`Old browsers` comments below.
*/
.hbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
.hbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.vbox {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
.vbox > * {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
}
.hbox.reverse,
.vbox.reverse,
.reverse {
  /* Old browsers */
  -webkit-box-direction: reverse;
  -moz-box-direction: reverse;
  box-direction: reverse;
  /* Modern browsers */
  flex-direction: row-reverse;
}
.hbox.box-flex0,
.vbox.box-flex0,
.box-flex0 {
  /* Old browsers */
  -webkit-box-flex: 0;
  -moz-box-flex: 0;
  box-flex: 0;
  /* Modern browsers */
  flex: none;
  width: auto;
}
.hbox.box-flex1,
.vbox.box-flex1,
.box-flex1 {
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex,
.vbox.box-flex,
.box-flex {
  /* Old browsers */
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
.hbox.box-flex2,
.vbox.box-flex2,
.box-flex2 {
  /* Old browsers */
  -webkit-box-flex: 2;
  -moz-box-flex: 2;
  box-flex: 2;
  /* Modern browsers */
  flex: 2;
}
.box-group1 {
  /*  Deprecated */
  -webkit-box-flex-group: 1;
  -moz-box-flex-group: 1;
  box-flex-group: 1;
}
.box-group2 {
  /* Deprecated */
  -webkit-box-flex-group: 2;
  -moz-box-flex-group: 2;
  box-flex-group: 2;
}
.hbox.start,
.vbox.start,
.start {
  /* Old browsers */
  -webkit-box-pack: start;
  -moz-box-pack: start;
  box-pack: start;
  /* Modern browsers */
  justify-content: flex-start;
}
.hbox.end,
.vbox.end,
.end {
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
}
.hbox.center,
.vbox.center,
.center {
  /* Old browsers */
  -webkit-box-pack: center;
  -moz-box-pack: center;
  box-pack: center;
  /* Modern browsers */
  justify-content: center;
}
.hbox.baseline,
.vbox.baseline,
.baseline {
  /* Old browsers */
  -webkit-box-pack: baseline;
  -moz-box-pack: baseline;
  box-pack: baseline;
  /* Modern browsers */
  justify-content: baseline;
}
.hbox.stretch,
.vbox.stretch,
.stretch {
  /* Old browsers */
  -webkit-box-pack: stretch;
  -moz-box-pack: stretch;
  box-pack: stretch;
  /* Modern browsers */
  justify-content: stretch;
}
.hbox.align-start,
.vbox.align-start,
.align-start {
  /* Old browsers */
  -webkit-box-align: start;
  -moz-box-align: start;
  box-align: start;
  /* Modern browsers */
  align-items: flex-start;
}
.hbox.align-end,
.vbox.align-end,
.align-end {
  /* Old browsers */
  -webkit-box-align: end;
  -moz-box-align: end;
  box-align: end;
  /* Modern browsers */
  align-items: flex-end;
}
.hbox.align-center,
.vbox.align-center,
.align-center {
  /* Old browsers */
  -webkit-box-align: center;
  -moz-box-align: center;
  box-align: center;
  /* Modern browsers */
  align-items: center;
}
.hbox.align-baseline,
.vbox.align-baseline,
.align-baseline {
  /* Old browsers */
  -webkit-box-align: baseline;
  -moz-box-align: baseline;
  box-align: baseline;
  /* Modern browsers */
  align-items: baseline;
}
.hbox.align-stretch,
.vbox.align-stretch,
.align-stretch {
  /* Old browsers */
  -webkit-box-align: stretch;
  -moz-box-align: stretch;
  box-align: stretch;
  /* Modern browsers */
  align-items: stretch;
}
div.error {
  margin: 2em;
  text-align: center;
}
div.error > h1 {
  font-size: 500%;
  line-height: normal;
}
div.error > p {
  font-size: 200%;
  line-height: normal;
}
div.traceback-wrapper {
  text-align: left;
  max-width: 800px;
  margin: auto;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
body {
  background-color: #fff;
  /* This makes sure that the body covers the entire window and needs to
       be in a different element than the display: box in wrapper below */
  position: absolute;
  left: 0px;
  right: 0px;
  top: 0px;
  bottom: 0px;
  overflow: visible;
}
body > #header {
  /* Initially hidden to prevent FLOUC */
  display: none;
  background-color: #fff;
  /* Display over codemirror */
  position: relative;
  z-index: 100;
}
body > #header #header-container {
  padding-bottom: 5px;
  padding-top: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
body > #header .header-bar {
  width: 100%;
  height: 1px;
  background: #e7e7e7;
  margin-bottom: -1px;
}
@media print {
  body > #header {
    display: none !important;
  }
}
#header-spacer {
  width: 100%;
  visibility: hidden;
}
@media print {
  #header-spacer {
    display: none;
  }
}
#ipython_notebook {
  padding-left: 0px;
  padding-top: 1px;
  padding-bottom: 1px;
}
@media (max-width: 991px) {
  #ipython_notebook {
    margin-left: 10px;
  }
}
[dir="rtl"] #ipython_notebook {
  float: right !important;
}
#noscript {
  width: auto;
  padding-top: 16px;
  padding-bottom: 16px;
  text-align: center;
  font-size: 22px;
  color: red;
  font-weight: bold;
}
#ipython_notebook img {
  height: 28px;
}
#site {
  width: 100%;
  display: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  overflow: auto;
}
@media print {
  #site {
    height: auto !important;
  }
}
/* Smaller buttons */
.ui-button .ui-button-text {
  padding: 0.2em 0.8em;
  font-size: 77%;
}
input.ui-button {
  padding: 0.3em 0.9em;
}
span#login_widget {
  float: right;
}
span#login_widget > .button,
#logout {
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button:focus,
#logout:focus,
span#login_widget > .button.focus,
#logout.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
span#login_widget > .button:hover,
#logout:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
span#login_widget > .button:active:hover,
#logout:active:hover,
span#login_widget > .button.active:hover,
#logout.active:hover,
.open > .dropdown-togglespan#login_widget > .button:hover,
.open > .dropdown-toggle#logout:hover,
span#login_widget > .button:active:focus,
#logout:active:focus,
span#login_widget > .button.active:focus,
#logout.active:focus,
.open > .dropdown-togglespan#login_widget > .button:focus,
.open > .dropdown-toggle#logout:focus,
span#login_widget > .button:active.focus,
#logout:active.focus,
span#login_widget > .button.active.focus,
#logout.active.focus,
.open > .dropdown-togglespan#login_widget > .button.focus,
.open > .dropdown-toggle#logout.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
span#login_widget > .button:active,
#logout:active,
span#login_widget > .button.active,
#logout.active,
.open > .dropdown-togglespan#login_widget > .button,
.open > .dropdown-toggle#logout {
  background-image: none;
}
span#login_widget > .button.disabled:hover,
#logout.disabled:hover,
span#login_widget > .button[disabled]:hover,
#logout[disabled]:hover,
fieldset[disabled] span#login_widget > .button:hover,
fieldset[disabled] #logout:hover,
span#login_widget > .button.disabled:focus,
#logout.disabled:focus,
span#login_widget > .button[disabled]:focus,
#logout[disabled]:focus,
fieldset[disabled] span#login_widget > .button:focus,
fieldset[disabled] #logout:focus,
span#login_widget > .button.disabled.focus,
#logout.disabled.focus,
span#login_widget > .button[disabled].focus,
#logout[disabled].focus,
fieldset[disabled] span#login_widget > .button.focus,
fieldset[disabled] #logout.focus {
  background-color: #fff;
  border-color: #ccc;
}
span#login_widget > .button .badge,
#logout .badge {
  color: #fff;
  background-color: #333;
}
.nav-header {
  text-transform: none;
}
#header > span {
  margin-top: 10px;
}
.modal_stretch .modal-dialog {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  min-height: 80vh;
}
.modal_stretch .modal-dialog .modal-body {
  max-height: calc(100vh - 200px);
  overflow: auto;
  flex: 1;
}
@media (min-width: 768px) {
  .modal .modal-dialog {
    width: 700px;
  }
}
@media (min-width: 768px) {
  select.form-control {
    margin-left: 12px;
    margin-right: 12px;
  }
}
/*!
*
* IPython auth
*
*/
.center-nav {
  display: inline-block;
  margin-bottom: -4px;
}
/*!
*
* IPython tree view
*
*/
/* We need an invisible input field on top of the sentense*/
/* "Drag file onto the list ..." */
.alternate_upload {
  background-color: none;
  display: inline;
}
.alternate_upload.form {
  padding: 0;
  margin: 0;
}
.alternate_upload input.fileinput {
  text-align: center;
  vertical-align: middle;
  display: inline;
  opacity: 0;
  z-index: 2;
  width: 12ex;
  margin-right: -12ex;
}
.alternate_upload .btn-upload {
  height: 22px;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
[dir="rtl"] #tabs li {
  float: right;
}
ul#tabs {
  margin-bottom: 4px;
}
[dir="rtl"] ul#tabs {
  margin-right: 0px;
}
ul#tabs a {
  padding-top: 6px;
  padding-bottom: 4px;
}
ul.breadcrumb a:focus,
ul.breadcrumb a:hover {
  text-decoration: none;
}
ul.breadcrumb i.icon-home {
  font-size: 16px;
  margin-right: 4px;
}
ul.breadcrumb span {
  color: #5e5e5e;
}
.list_toolbar {
  padding: 4px 0 4px 0;
  vertical-align: middle;
}
.list_toolbar .tree-buttons {
  padding-top: 1px;
}
[dir="rtl"] .list_toolbar .tree-buttons {
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-right {
  padding-top: 1px;
  float: left !important;
}
[dir="rtl"] .list_toolbar .pull-left {
  float: right !important;
}
.dynamic-buttons {
  padding-top: 3px;
  display: inline-block;
}
.list_toolbar [class*="span"] {
  min-height: 24px;
}
.list_header {
  font-weight: bold;
  background-color: #EEE;
}
.list_placeholder {
  font-weight: bold;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
}
.list_container {
  margin-top: 4px;
  margin-bottom: 20px;
  border: 1px solid #ddd;
  border-radius: 2px;
}
.list_container > div {
  border-bottom: 1px solid #ddd;
}
.list_container > div:hover .list-item {
  background-color: red;
}
.list_container > div:last-child {
  border: none;
}
.list_item:hover .list_item {
  background-color: #ddd;
}
.list_item a {
  text-decoration: none;
}
.list_item:hover {
  background-color: #fafafa;
}
.list_header > div,
.list_item > div {
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
.list_header > div input,
.list_item > div input {
  margin-right: 7px;
  margin-left: 14px;
  vertical-align: baseline;
  line-height: 22px;
  position: relative;
  top: -1px;
}
.list_header > div .item_link,
.list_item > div .item_link {
  margin-left: -1px;
  vertical-align: baseline;
  line-height: 22px;
}
.new-file input[type=checkbox] {
  visibility: hidden;
}
.item_name {
  line-height: 22px;
  height: 24px;
}
.item_icon {
  font-size: 14px;
  color: #5e5e5e;
  margin-right: 7px;
  margin-left: 7px;
  line-height: 22px;
  vertical-align: baseline;
}
.item_buttons {
  line-height: 1em;
  margin-left: -5px;
}
.item_buttons .btn,
.item_buttons .btn-group,
.item_buttons .input-group {
  float: left;
}
.item_buttons > .btn,
.item_buttons > .btn-group,
.item_buttons > .input-group {
  margin-left: 5px;
}
.item_buttons .btn {
  min-width: 13ex;
}
.item_buttons .running-indicator {
  padding-top: 4px;
  color: #5cb85c;
}
.item_buttons .kernel-name {
  padding-top: 4px;
  color: #5bc0de;
  margin-right: 7px;
  float: left;
}
.toolbar_info {
  height: 24px;
  line-height: 24px;
}
.list_item input:not([type=checkbox]) {
  padding-top: 3px;
  padding-bottom: 3px;
  height: 22px;
  line-height: 14px;
  margin: 0px;
}
.highlight_text {
  color: blue;
}
#project_name {
  display: inline-block;
  padding-left: 7px;
  margin-left: -2px;
}
#project_name > .breadcrumb {
  padding: 0px;
  margin-bottom: 0px;
  background-color: transparent;
  font-weight: bold;
}
#tree-selector {
  padding-right: 0px;
}
[dir="rtl"] #tree-selector a {
  float: right;
}
#button-select-all {
  min-width: 50px;
}
#select-all {
  margin-left: 7px;
  margin-right: 2px;
}
.menu_icon {
  margin-right: 2px;
}
.tab-content .row {
  margin-left: 0px;
  margin-right: 0px;
}
.folder_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f114";
}
.folder_icon:before.pull-left {
  margin-right: .3em;
}
.folder_icon:before.pull-right {
  margin-left: .3em;
}
.notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
}
.notebook_icon:before.pull-left {
  margin-right: .3em;
}
.notebook_icon:before.pull-right {
  margin-left: .3em;
}
.running_notebook_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f02d";
  position: relative;
  top: -1px;
  color: #5cb85c;
}
.running_notebook_icon:before.pull-left {
  margin-right: .3em;
}
.running_notebook_icon:before.pull-right {
  margin-left: .3em;
}
.file_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f016";
  position: relative;
  top: -2px;
}
.file_icon:before.pull-left {
  margin-right: .3em;
}
.file_icon:before.pull-right {
  margin-left: .3em;
}
#notebook_toolbar .pull-right {
  padding-top: 0px;
  margin-right: -1px;
}
ul#new-menu {
  left: auto;
  right: 0;
}
[dir="rtl"] #new-menu {
  text-align: right;
}
.kernel-menu-icon {
  padding-right: 12px;
  width: 24px;
  content: "\f096";
}
.kernel-menu-icon:before {
  content: "\f096";
}
.kernel-menu-icon-current:before {
  content: "\f00c";
}
#tab_content {
  padding-top: 20px;
}
#running .panel-group .panel {
  margin-top: 3px;
  margin-bottom: 1em;
}
#running .panel-group .panel .panel-heading {
  background-color: #EEE;
  padding-top: 4px;
  padding-bottom: 4px;
  padding-left: 7px;
  padding-right: 7px;
  line-height: 22px;
}
#running .panel-group .panel .panel-heading a:focus,
#running .panel-group .panel .panel-heading a:hover {
  text-decoration: none;
}
#running .panel-group .panel .panel-body {
  padding: 0px;
}
#running .panel-group .panel .panel-body .list_container {
  margin-top: 0px;
  margin-bottom: 0px;
  border: 0px;
  border-radius: 0px;
}
#running .panel-group .panel .panel-body .list_container .list_item {
  border-bottom: 1px solid #ddd;
}
#running .panel-group .panel .panel-body .list_container .list_item:last-child {
  border-bottom: 0px;
}
[dir="rtl"] #running .col-sm-8 {
  float: right !important;
}
.delete-button {
  display: none;
}
.duplicate-button {
  display: none;
}
.rename-button {
  display: none;
}
.shutdown-button {
  display: none;
}
.dynamic-instructions {
  display: inline-block;
  padding-top: 4px;
}
/*!
*
* IPython text editor webapp
*
*/
.selected-keymap i.fa {
  padding: 0px 5px;
}
.selected-keymap i.fa:before {
  content: "\f00c";
}
#mode-menu {
  overflow: auto;
  max-height: 20em;
}
.edit_app #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.edit_app #menubar .navbar {
  /* Use a negative 1 bottom margin, so the border overlaps the border of the
    header */
  margin-bottom: -1px;
}
.dirty-indicator {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator.pull-left {
  margin-right: .3em;
}
.dirty-indicator.pull-right {
  margin-left: .3em;
}
.dirty-indicator-dirty {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-dirty.pull-left {
  margin-right: .3em;
}
.dirty-indicator-dirty.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  width: 20px;
}
.dirty-indicator-clean.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean.pull-right {
  margin-left: .3em;
}
.dirty-indicator-clean:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f00c";
}
.dirty-indicator-clean:before.pull-left {
  margin-right: .3em;
}
.dirty-indicator-clean:before.pull-right {
  margin-left: .3em;
}
#filename {
  font-size: 16pt;
  display: table;
  padding: 0px 5px;
}
#current-mode {
  padding-left: 5px;
  padding-right: 5px;
}
#texteditor-backdrop {
  padding-top: 20px;
  padding-bottom: 20px;
}
@media not print {
  #texteditor-backdrop {
    background-color: #EEE;
  }
}
@media print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container .CodeMirror-gutter,
  #texteditor-backdrop #texteditor-container .CodeMirror-gutters {
    background-color: #fff;
  }
}
@media not print {
  #texteditor-backdrop #texteditor-container {
    padding: 0px;
    background-color: #fff;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
/*!
*
* IPython notebook
*
*/
/* CSS font colors for translated ANSI colors. */
.ansibold {
  font-weight: bold;
}
/* use dark versions for foreground, to improve visibility */
.ansiblack {
  color: black;
}
.ansired {
  color: darkred;
}
.ansigreen {
  color: darkgreen;
}
.ansiyellow {
  color: #c4a000;
}
.ansiblue {
  color: darkblue;
}
.ansipurple {
  color: darkviolet;
}
.ansicyan {
  color: steelblue;
}
.ansigray {
  color: gray;
}
/* and light for background, for the same reason */
.ansibgblack {
  background-color: black;
}
.ansibgred {
  background-color: red;
}
.ansibggreen {
  background-color: green;
}
.ansibgyellow {
  background-color: yellow;
}
.ansibgblue {
  background-color: blue;
}
.ansibgpurple {
  background-color: magenta;
}
.ansibgcyan {
  background-color: cyan;
}
.ansibggray {
  background-color: gray;
}
div.cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  border-radius: 2px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  border-width: 1px;
  border-style: solid;
  border-color: transparent;
  width: 100%;
  padding: 5px;
  /* This acts as a spacer between cells, that is outside the border */
  margin: 0px;
  outline: none;
  border-left-width: 1px;
  padding-left: 5px;
  background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%);
}
div.cell.jupyter-soft-selected {
  border-left-color: #90CAF9;
  border-left-color: #E3F2FD;
  border-left-width: 1px;
  padding-left: 5px;
  border-right-color: #E3F2FD;
  border-right-width: 1px;
  background: #E3F2FD;
}
@media print {
  div.cell.jupyter-soft-selected {
    border-color: transparent;
  }
}
div.cell.selected {
  border-color: #ababab;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%);
}
@media print {
  div.cell.selected {
    border-color: transparent;
  }
}
div.cell.selected.jupyter-soft-selected {
  border-left-width: 0;
  padding-left: 6px;
  background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%);
}
.edit_mode div.cell.selected {
  border-color: #66BB6A;
  border-left-width: 0px;
  padding-left: 6px;
  background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%);
}
@media print {
  .edit_mode div.cell.selected {
    border-color: transparent;
  }
}
.prompt {
  /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */
  min-width: 14ex;
  /* This padding is tuned to match the padding on the CodeMirror editor. */
  padding: 0.4em;
  margin: 0px;
  font-family: monospace;
  text-align: right;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
  /* Don't highlight prompt number selection */
  -webkit-touch-callout: none;
  -webkit-user-select: none;
  -khtml-user-select: none;
  -moz-user-select: none;
  -ms-user-select: none;
  user-select: none;
  /* Use default cursor */
  cursor: default;
}
@media (max-width: 540px) {
  .prompt {
    text-align: left;
  }
}
div.inner_cell {
  min-width: 0;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_area {
  border: 1px solid #cfcfcf;
  border-radius: 2px;
  background: #f7f7f7;
  line-height: 1.21429em;
}
/* This is needed so that empty prompt areas can collapse to zero height when there
   is no content in the output_subarea and the prompt. The main purpose of this is
   to make sure that empty JavaScript output_subareas have no height. */
div.prompt:empty {
  padding-top: 0;
  padding-bottom: 0;
}
div.unrecognized_cell {
  padding: 5px 5px 5px 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.unrecognized_cell .inner_cell {
  border-radius: 2px;
  padding: 5px;
  font-weight: bold;
  color: red;
  border: 1px solid #cfcfcf;
  background: #eaeaea;
}
div.unrecognized_cell .inner_cell a {
  color: inherit;
  text-decoration: none;
}
div.unrecognized_cell .inner_cell a:hover {
  color: inherit;
  text-decoration: none;
}
@media (max-width: 540px) {
  div.unrecognized_cell > div.prompt {
    display: none;
  }
}
div.code_cell {
  /* avoid page breaking on code cells when printing */
}
@media print {
  div.code_cell {
    page-break-inside: avoid;
  }
}
/* any special styling for code cells that are currently running goes here */
div.input {
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.input {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
/* input_area and input_prompt must match in top border and margin for alignment */
div.input_prompt {
  color: #303F9F;
  border-top: 1px solid transparent;
}
div.input_area > div.highlight {
  margin: 0.4em;
  border: none;
  padding: 0px;
  background-color: transparent;
}
div.input_area > div.highlight > pre {
  margin: 0px;
  border: none;
  padding: 0px;
  background-color: transparent;
}
/* The following gets added to the <head> if it is detected that the user has a
 * monospace font with inconsistent normal/bold/italic height.  See
 * notebookmain.js.  Such fonts will have keywords vertically offset with
 * respect to the rest of the text.  The user should select a better font.
 * See: https://github.com/ipython/ipython/issues/1503
 *
 * .CodeMirror span {
 *      vertical-align: bottom;
 * }
 */
.CodeMirror {
  line-height: 1.21429em;
  /* Changed from 1em to our global default */
  font-size: 14px;
  height: auto;
  /* Changed to auto to autogrow */
  background: none;
  /* Changed from white to allow our bg to show through */
}
.CodeMirror-scroll {
  /*  The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/
  /*  We have found that if it is visible, vertical scrollbars appear with font size changes.*/
  overflow-y: hidden;
  overflow-x: auto;
}
.CodeMirror-lines {
  /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */
  /* we have set a different line-height and want this to scale with that. */
  padding: 0.4em;
}
.CodeMirror-linenumber {
  padding: 0 8px 0 4px;
}
.CodeMirror-gutters {
  border-bottom-left-radius: 2px;
  border-top-left-radius: 2px;
}
.CodeMirror pre {
  /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */
  /* .CodeMirror-lines */
  padding: 0;
  border: 0;
  border-radius: 0;
}
/*

Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org>
Adapted from GitHub theme

*/
.highlight-base {
  color: #000;
}
.highlight-variable {
  color: #000;
}
.highlight-variable-2 {
  color: #1a1a1a;
}
.highlight-variable-3 {
  color: #333333;
}
.highlight-string {
  color: #BA2121;
}
.highlight-comment {
  color: #408080;
  font-style: italic;
}
.highlight-number {
  color: #080;
}
.highlight-atom {
  color: #88F;
}
.highlight-keyword {
  color: #008000;
  font-weight: bold;
}
.highlight-builtin {
  color: #008000;
}
.highlight-error {
  color: #f00;
}
.highlight-operator {
  color: #AA22FF;
  font-weight: bold;
}
.highlight-meta {
  color: #AA22FF;
}
/* previously not defined, copying from default codemirror */
.highlight-def {
  color: #00f;
}
.highlight-string-2 {
  color: #f50;
}
.highlight-qualifier {
  color: #555;
}
.highlight-bracket {
  color: #997;
}
.highlight-tag {
  color: #170;
}
.highlight-attribute {
  color: #00c;
}
.highlight-header {
  color: blue;
}
.highlight-quote {
  color: #090;
}
.highlight-link {
  color: #00c;
}
/* apply the same style to codemirror */
.cm-s-ipython span.cm-keyword {
  color: #008000;
  font-weight: bold;
}
.cm-s-ipython span.cm-atom {
  color: #88F;
}
.cm-s-ipython span.cm-number {
  color: #080;
}
.cm-s-ipython span.cm-def {
  color: #00f;
}
.cm-s-ipython span.cm-variable {
  color: #000;
}
.cm-s-ipython span.cm-operator {
  color: #AA22FF;
  font-weight: bold;
}
.cm-s-ipython span.cm-variable-2 {
  color: #1a1a1a;
}
.cm-s-ipython span.cm-variable-3 {
  color: #333333;
}
.cm-s-ipython span.cm-comment {
  color: #408080;
  font-style: italic;
}
.cm-s-ipython span.cm-string {
  color: #BA2121;
}
.cm-s-ipython span.cm-string-2 {
  color: #f50;
}
.cm-s-ipython span.cm-meta {
  color: #AA22FF;
}
.cm-s-ipython span.cm-qualifier {
  color: #555;
}
.cm-s-ipython span.cm-builtin {
  color: #008000;
}
.cm-s-ipython span.cm-bracket {
  color: #997;
}
.cm-s-ipython span.cm-tag {
  color: #170;
}
.cm-s-ipython span.cm-attribute {
  color: #00c;
}
.cm-s-ipython span.cm-header {
  color: blue;
}
.cm-s-ipython span.cm-quote {
  color: #090;
}
.cm-s-ipython span.cm-link {
  color: #00c;
}
.cm-s-ipython span.cm-error {
  color: #f00;
}
.cm-s-ipython span.cm-tab {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=);
  background-position: right;
  background-repeat: no-repeat;
}
div.output_wrapper {
  /* this position must be relative to enable descendents to be absolute within it */
  position: relative;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
  z-index: 1;
}
/* class for the output area when it should be height-limited */
div.output_scroll {
  /* ideally, this would be max-height, but FF barfs all over that */
  height: 24em;
  /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */
  width: 100%;
  overflow: auto;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8);
  display: block;
}
/* output div while it is collapsed */
div.output_collapsed {
  margin: 0px;
  padding: 0px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
div.out_prompt_overlay {
  height: 100%;
  padding: 0px 0.4em;
  position: absolute;
  border-radius: 2px;
}
div.out_prompt_overlay:hover {
  /* use inner shadow to get border that is computed the same on WebKit/FF */
  -webkit-box-shadow: inset 0 0 1px #000;
  box-shadow: inset 0 0 1px #000;
  background: rgba(240, 240, 240, 0.5);
}
div.output_prompt {
  color: #D84315;
}
/* This class is the outer container of all output sections. */
div.output_area {
  padding: 0px;
  page-break-inside: avoid;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
div.output_area .MathJax_Display {
  text-align: left !important;
}
div.output_area .rendered_html table {
  margin-left: 0;
  margin-right: 0;
}
div.output_area .rendered_html img {
  margin-left: 0;
  margin-right: 0;
}
div.output_area img,
div.output_area svg {
  max-width: 100%;
  height: auto;
}
div.output_area img.unconfined,
div.output_area svg.unconfined {
  max-width: none;
}
/* This is needed to protect the pre formating from global settings such
   as that of bootstrap */
.output {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: vertical;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: vertical;
  -moz-box-align: stretch;
  display: box;
  box-orient: vertical;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: column;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.output_area {
    /* Old browsers */
    display: -webkit-box;
    -webkit-box-orient: vertical;
    -webkit-box-align: stretch;
    display: -moz-box;
    -moz-box-orient: vertical;
    -moz-box-align: stretch;
    display: box;
    box-orient: vertical;
    box-align: stretch;
    /* Modern browsers */
    display: flex;
    flex-direction: column;
    align-items: stretch;
  }
}
div.output_area pre {
  margin: 0;
  padding: 0;
  border: 0;
  vertical-align: baseline;
  color: black;
  background-color: transparent;
  border-radius: 0;
}
/* This class is for the output subarea inside the output_area and after
   the prompt div. */
div.output_subarea {
  overflow-x: auto;
  padding: 0.4em;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
  max-width: calc(100% - 14ex);
}
div.output_scroll div.output_subarea {
  overflow-x: visible;
}
/* The rest of the output_* classes are for special styling of the different
   output types */
/* all text output has this class: */
div.output_text {
  text-align: left;
  color: #000;
  /* This has to match that of the the CodeMirror class line-height below */
  line-height: 1.21429em;
}
/* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */
div.output_stderr {
  background: #fdd;
  /* very light red background for stderr */
}
div.output_latex {
  text-align: left;
}
/* Empty output_javascript divs should have no height */
div.output_javascript:empty {
  padding: 0;
}
.js-error {
  color: darkred;
}
/* raw_input styles */
div.raw_input_container {
  line-height: 1.21429em;
  padding-top: 5px;
}
pre.raw_input_prompt {
  /* nothing needed here. */
}
input.raw_input {
  font-family: monospace;
  font-size: inherit;
  color: inherit;
  width: auto;
  /* make sure input baseline aligns with prompt */
  vertical-align: baseline;
  /* padding + margin = 0.5em between prompt and cursor */
  padding: 0em 0.25em;
  margin: 0em 0.25em;
}
input.raw_input:focus {
  box-shadow: none;
}
p.p-space {
  margin-bottom: 10px;
}
div.output_unrecognized {
  padding: 5px;
  font-weight: bold;
  color: red;
}
div.output_unrecognized a {
  color: inherit;
  text-decoration: none;
}
div.output_unrecognized a:hover {
  color: inherit;
  text-decoration: none;
}
.rendered_html {
  color: #000;
  /* any extras will just be numbers: */
}
.rendered_html em {
  font-style: italic;
}
.rendered_html strong {
  font-weight: bold;
}
.rendered_html u {
  text-decoration: underline;
}
.rendered_html :link {
  text-decoration: underline;
}
.rendered_html :visited {
  text-decoration: underline;
}
.rendered_html h1 {
  font-size: 185.7%;
  margin: 1.08em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h2 {
  font-size: 157.1%;
  margin: 1.27em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h3 {
  font-size: 128.6%;
  margin: 1.55em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h4 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
}
.rendered_html h5 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h6 {
  font-size: 100%;
  margin: 2em 0 0 0;
  font-weight: bold;
  line-height: 1.0;
  font-style: italic;
}
.rendered_html h1:first-child {
  margin-top: 0.538em;
}
.rendered_html h2:first-child {
  margin-top: 0.636em;
}
.rendered_html h3:first-child {
  margin-top: 0.777em;
}
.rendered_html h4:first-child {
  margin-top: 1em;
}
.rendered_html h5:first-child {
  margin-top: 1em;
}
.rendered_html h6:first-child {
  margin-top: 1em;
}
.rendered_html ul {
  list-style: disc;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ul ul {
  list-style: square;
  margin: 0em 2em;
}
.rendered_html ul ul ul {
  list-style: circle;
  margin: 0em 2em;
}
.rendered_html ol {
  list-style: decimal;
  margin: 0em 2em;
  padding-left: 0px;
}
.rendered_html ol ol {
  list-style: upper-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol {
  list-style: lower-alpha;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol {
  list-style: lower-roman;
  margin: 0em 2em;
}
.rendered_html ol ol ol ol ol {
  list-style: decimal;
  margin: 0em 2em;
}
.rendered_html * + ul {
  margin-top: 1em;
}
.rendered_html * + ol {
  margin-top: 1em;
}
.rendered_html hr {
  color: black;
  background-color: black;
}
.rendered_html pre {
  margin: 1em 2em;
}
.rendered_html pre,
.rendered_html code {
  border: 0;
  background-color: #fff;
  color: #000;
  font-size: 100%;
  padding: 0px;
}
.rendered_html blockquote {
  margin: 1em 2em;
}
.rendered_html table {
  margin-left: auto;
  margin-right: auto;
  border: 1px solid black;
  border-collapse: collapse;
}
.rendered_html tr,
.rendered_html th,
.rendered_html td {
  border: 1px solid black;
  border-collapse: collapse;
  margin: 1em 2em;
}
.rendered_html td,
.rendered_html th {
  text-align: left;
  vertical-align: middle;
  padding: 4px;
}
.rendered_html th {
  font-weight: bold;
}
.rendered_html * + table {
  margin-top: 1em;
}
.rendered_html p {
  text-align: left;
}
.rendered_html * + p {
  margin-top: 1em;
}
.rendered_html img {
  display: block;
  margin-left: auto;
  margin-right: auto;
}
.rendered_html * + img {
  margin-top: 1em;
}
.rendered_html img,
.rendered_html svg {
  max-width: 100%;
  height: auto;
}
.rendered_html img.unconfined,
.rendered_html svg.unconfined {
  max-width: none;
}
div.text_cell {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
}
@media (max-width: 540px) {
  div.text_cell > div.prompt {
    display: none;
  }
}
div.text_cell_render {
  /*font-family: "Helvetica Neue", Arial, Helvetica, Geneva, sans-serif;*/
  outline: none;
  resize: none;
  width: inherit;
  border-style: none;
  padding: 0.5em 0.5em 0.5em 0.4em;
  color: #000;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
a.anchor-link:link {
  text-decoration: none;
  padding: 0px 20px;
  visibility: hidden;
}
h1:hover .anchor-link,
h2:hover .anchor-link,
h3:hover .anchor-link,
h4:hover .anchor-link,
h5:hover .anchor-link,
h6:hover .anchor-link {
  visibility: visible;
}
.text_cell.rendered .input_area {
  display: none;
}
.text_cell.rendered .rendered_html {
  overflow-x: auto;
  overflow-y: hidden;
}
.text_cell.unrendered .text_cell_render {
  display: none;
}
.cm-header-1,
.cm-header-2,
.cm-header-3,
.cm-header-4,
.cm-header-5,
.cm-header-6 {
  font-weight: bold;
  font-family: "Helvetica Neue", Helvetica, Arial, sans-serif;
}
.cm-header-1 {
  font-size: 185.7%;
}
.cm-header-2 {
  font-size: 157.1%;
}
.cm-header-3 {
  font-size: 128.6%;
}
.cm-header-4 {
  font-size: 110%;
}
.cm-header-5 {
  font-size: 100%;
  font-style: italic;
}
.cm-header-6 {
  font-size: 100%;
  font-style: italic;
}
/*!
*
* IPython notebook webapp
*
*/
@media (max-width: 767px) {
  .notebook_app {
    padding-left: 0px;
    padding-right: 0px;
  }
}
#ipython-main-app {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook_panel {
  margin: 0px;
  padding: 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  height: 100%;
}
div#notebook {
  font-size: 14px;
  line-height: 20px;
  overflow-y: hidden;
  overflow-x: auto;
  width: 100%;
  /* This spaces the page away from the edge of the notebook area */
  padding-top: 20px;
  margin: 0px;
  outline: none;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  min-height: 100%;
}
@media not print {
  #notebook-container {
    padding: 15px;
    background-color: #fff;
    min-height: 0;
    -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
    box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  }
}
@media print {
  #notebook-container {
    width: 100%;
  }
}
div.ui-widget-content {
  border: 1px solid #ababab;
  outline: none;
}
pre.dialog {
  background-color: #f7f7f7;
  border: 1px solid #ddd;
  border-radius: 2px;
  padding: 0.4em;
  padding-left: 2em;
}
p.dialog {
  padding: 0.2em;
}
/* Word-wrap output correctly.  This is the CSS3 spelling, though Firefox seems
   to not honor it correctly.  Webkit browsers (Chrome, rekonq, Safari) do.
 */
pre,
code,
kbd,
samp {
  white-space: pre-wrap;
}
#fonttest {
  font-family: monospace;
}
p {
  margin-bottom: 0;
}
.end_space {
  min-height: 100px;
  transition: height .2s ease;
}
.notebook_app > #header {
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
@media not print {
  .notebook_app {
    background-color: #EEE;
  }
}
kbd {
  border-style: solid;
  border-width: 1px;
  box-shadow: none;
  margin: 2px;
  padding-left: 2px;
  padding-right: 2px;
  padding-top: 1px;
  padding-bottom: 1px;
}
/* CSS for the cell toolbar */
.celltoolbar {
  border: thin solid #CFCFCF;
  border-bottom: none;
  background: #EEE;
  border-radius: 2px 2px 0px 0px;
  width: 100%;
  height: 29px;
  padding-right: 4px;
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  /* Old browsers */
  -webkit-box-pack: end;
  -moz-box-pack: end;
  box-pack: end;
  /* Modern browsers */
  justify-content: flex-end;
  display: -webkit-flex;
}
@media print {
  .celltoolbar {
    display: none;
  }
}
.ctb_hideshow {
  display: none;
  vertical-align: bottom;
}
/* ctb_show is added to the ctb_hideshow div to show the cell toolbar.
   Cell toolbars are only shown when the ctb_global_show class is also set.
*/
.ctb_global_show .ctb_show.ctb_hideshow {
  display: block;
}
.ctb_global_show .ctb_show + .input_area,
.ctb_global_show .ctb_show + div.text_cell_input,
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border-top-right-radius: 0px;
  border-top-left-radius: 0px;
}
.ctb_global_show .ctb_show ~ div.text_cell_render {
  border: 1px solid #cfcfcf;
}
.celltoolbar {
  font-size: 87%;
  padding-top: 3px;
}
.celltoolbar select {
  display: block;
  width: 100%;
  height: 32px;
  padding: 6px 12px;
  font-size: 13px;
  line-height: 1.42857143;
  color: #555555;
  background-color: #fff;
  background-image: none;
  border: 1px solid #ccc;
  border-radius: 2px;
  -webkit-box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  box-shadow: inset 0 1px 1px rgba(0, 0, 0, 0.075);
  -webkit-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  -o-transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  transition: border-color ease-in-out .15s, box-shadow ease-in-out .15s;
  height: 30px;
  padding: 5px 10px;
  font-size: 12px;
  line-height: 1.5;
  border-radius: 1px;
  width: inherit;
  font-size: inherit;
  height: 22px;
  padding: 0px;
  display: inline-block;
}
.celltoolbar select:focus {
  border-color: #66afe9;
  outline: 0;
  -webkit-box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
  box-shadow: inset 0 1px 1px rgba(0,0,0,.075), 0 0 8px rgba(102, 175, 233, 0.6);
}
.celltoolbar select::-moz-placeholder {
  color: #999;
  opacity: 1;
}
.celltoolbar select:-ms-input-placeholder {
  color: #999;
}
.celltoolbar select::-webkit-input-placeholder {
  color: #999;
}
.celltoolbar select::-ms-expand {
  border: 0;
  background-color: transparent;
}
.celltoolbar select[disabled],
.celltoolbar select[readonly],
fieldset[disabled] .celltoolbar select {
  background-color: #eeeeee;
  opacity: 1;
}
.celltoolbar select[disabled],
fieldset[disabled] .celltoolbar select {
  cursor: not-allowed;
}
textarea.celltoolbar select {
  height: auto;
}
select.celltoolbar select {
  height: 30px;
  line-height: 30px;
}
textarea.celltoolbar select,
select[multiple].celltoolbar select {
  height: auto;
}
.celltoolbar label {
  margin-left: 5px;
  margin-right: 5px;
}
.completions {
  position: absolute;
  z-index: 110;
  overflow: hidden;
  border: 1px solid #ababab;
  border-radius: 2px;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  line-height: 1;
}
.completions select {
  background: white;
  outline: none;
  border: none;
  padding: 0px;
  margin: 0px;
  overflow: auto;
  font-family: monospace;
  font-size: 110%;
  color: #000;
  width: auto;
}
.completions select option.context {
  color: #286090;
}
#kernel_logo_widget {
  float: right !important;
  float: right;
}
#kernel_logo_widget .current_kernel_logo {
  display: none;
  margin-top: -1px;
  margin-bottom: -1px;
  width: 32px;
  height: 32px;
}
#menubar {
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
  margin-top: 1px;
}
#menubar .navbar {
  border-top: 1px;
  border-radius: 0px 0px 2px 2px;
  margin-bottom: 0px;
}
#menubar .navbar-toggle {
  float: left;
  padding-top: 7px;
  padding-bottom: 7px;
  border: none;
}
#menubar .navbar-collapse {
  clear: left;
}
.nav-wrapper {
  border-bottom: 1px solid #e7e7e7;
}
i.menu-icon {
  padding-top: 4px;
}
ul#help_menu li a {
  overflow: hidden;
  padding-right: 2.2em;
}
ul#help_menu li a i {
  margin-right: -1.2em;
}
.dropdown-submenu {
  position: relative;
}
.dropdown-submenu > .dropdown-menu {
  top: 0;
  left: 100%;
  margin-top: -6px;
  margin-left: -1px;
}
.dropdown-submenu:hover > .dropdown-menu {
  display: block;
}
.dropdown-submenu > a:after {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  display: block;
  content: "\f0da";
  float: right;
  color: #333333;
  margin-top: 2px;
  margin-right: -10px;
}
.dropdown-submenu > a:after.pull-left {
  margin-right: .3em;
}
.dropdown-submenu > a:after.pull-right {
  margin-left: .3em;
}
.dropdown-submenu:hover > a:after {
  color: #262626;
}
.dropdown-submenu.pull-left {
  float: none;
}
.dropdown-submenu.pull-left > .dropdown-menu {
  left: -100%;
  margin-left: 10px;
}
#notification_area {
  float: right !important;
  float: right;
  z-index: 10;
}
.indicator_area {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#kernel_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  border-left: 1px solid;
}
#kernel_indicator .kernel_indicator_name {
  padding-left: 5px;
  padding-right: 5px;
}
#modal_indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
}
#readonly-indicator {
  float: right !important;
  float: right;
  color: #777;
  margin-left: 5px;
  margin-right: 5px;
  width: 11px;
  z-index: 10;
  text-align: center;
  width: auto;
  margin-top: 2px;
  margin-bottom: 0px;
  margin-left: 0px;
  margin-right: 0px;
  display: none;
}
.modal_indicator:before {
  width: 1.28571429em;
  text-align: center;
}
.edit_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f040";
}
.edit_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.edit_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.command_mode .modal_indicator:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: ' ';
}
.command_mode .modal_indicator:before.pull-left {
  margin-right: .3em;
}
.command_mode .modal_indicator:before.pull-right {
  margin-left: .3em;
}
.kernel_idle_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f10c";
}
.kernel_idle_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_idle_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_busy_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f111";
}
.kernel_busy_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_busy_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_dead_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f1e2";
}
.kernel_dead_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_dead_icon:before.pull-right {
  margin-left: .3em;
}
.kernel_disconnected_icon:before {
  display: inline-block;
  font: normal normal normal 14px/1 FontAwesome;
  font-size: inherit;
  text-rendering: auto;
  -webkit-font-smoothing: antialiased;
  -moz-osx-font-smoothing: grayscale;
  content: "\f127";
}
.kernel_disconnected_icon:before.pull-left {
  margin-right: .3em;
}
.kernel_disconnected_icon:before.pull-right {
  margin-left: .3em;
}
.notification_widget {
  color: #777;
  z-index: 10;
  background: rgba(240, 240, 240, 0.5);
  margin-right: 4px;
  color: #333;
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget:focus,
.notification_widget.focus {
  color: #333;
  background-color: #e6e6e6;
  border-color: #8c8c8c;
}
.notification_widget:hover {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  color: #333;
  background-color: #e6e6e6;
  border-color: #adadad;
}
.notification_widget:active:hover,
.notification_widget.active:hover,
.open > .dropdown-toggle.notification_widget:hover,
.notification_widget:active:focus,
.notification_widget.active:focus,
.open > .dropdown-toggle.notification_widget:focus,
.notification_widget:active.focus,
.notification_widget.active.focus,
.open > .dropdown-toggle.notification_widget.focus {
  color: #333;
  background-color: #d4d4d4;
  border-color: #8c8c8c;
}
.notification_widget:active,
.notification_widget.active,
.open > .dropdown-toggle.notification_widget {
  background-image: none;
}
.notification_widget.disabled:hover,
.notification_widget[disabled]:hover,
fieldset[disabled] .notification_widget:hover,
.notification_widget.disabled:focus,
.notification_widget[disabled]:focus,
fieldset[disabled] .notification_widget:focus,
.notification_widget.disabled.focus,
.notification_widget[disabled].focus,
fieldset[disabled] .notification_widget.focus {
  background-color: #fff;
  border-color: #ccc;
}
.notification_widget .badge {
  color: #fff;
  background-color: #333;
}
.notification_widget.warning {
  color: #fff;
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning:focus,
.notification_widget.warning.focus {
  color: #fff;
  background-color: #ec971f;
  border-color: #985f0d;
}
.notification_widget.warning:hover {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  color: #fff;
  background-color: #ec971f;
  border-color: #d58512;
}
.notification_widget.warning:active:hover,
.notification_widget.warning.active:hover,
.open > .dropdown-toggle.notification_widget.warning:hover,
.notification_widget.warning:active:focus,
.notification_widget.warning.active:focus,
.open > .dropdown-toggle.notification_widget.warning:focus,
.notification_widget.warning:active.focus,
.notification_widget.warning.active.focus,
.open > .dropdown-toggle.notification_widget.warning.focus {
  color: #fff;
  background-color: #d58512;
  border-color: #985f0d;
}
.notification_widget.warning:active,
.notification_widget.warning.active,
.open > .dropdown-toggle.notification_widget.warning {
  background-image: none;
}
.notification_widget.warning.disabled:hover,
.notification_widget.warning[disabled]:hover,
fieldset[disabled] .notification_widget.warning:hover,
.notification_widget.warning.disabled:focus,
.notification_widget.warning[disabled]:focus,
fieldset[disabled] .notification_widget.warning:focus,
.notification_widget.warning.disabled.focus,
.notification_widget.warning[disabled].focus,
fieldset[disabled] .notification_widget.warning.focus {
  background-color: #f0ad4e;
  border-color: #eea236;
}
.notification_widget.warning .badge {
  color: #f0ad4e;
  background-color: #fff;
}
.notification_widget.success {
  color: #fff;
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success:focus,
.notification_widget.success.focus {
  color: #fff;
  background-color: #449d44;
  border-color: #255625;
}
.notification_widget.success:hover {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  color: #fff;
  background-color: #449d44;
  border-color: #398439;
}
.notification_widget.success:active:hover,
.notification_widget.success.active:hover,
.open > .dropdown-toggle.notification_widget.success:hover,
.notification_widget.success:active:focus,
.notification_widget.success.active:focus,
.open > .dropdown-toggle.notification_widget.success:focus,
.notification_widget.success:active.focus,
.notification_widget.success.active.focus,
.open > .dropdown-toggle.notification_widget.success.focus {
  color: #fff;
  background-color: #398439;
  border-color: #255625;
}
.notification_widget.success:active,
.notification_widget.success.active,
.open > .dropdown-toggle.notification_widget.success {
  background-image: none;
}
.notification_widget.success.disabled:hover,
.notification_widget.success[disabled]:hover,
fieldset[disabled] .notification_widget.success:hover,
.notification_widget.success.disabled:focus,
.notification_widget.success[disabled]:focus,
fieldset[disabled] .notification_widget.success:focus,
.notification_widget.success.disabled.focus,
.notification_widget.success[disabled].focus,
fieldset[disabled] .notification_widget.success.focus {
  background-color: #5cb85c;
  border-color: #4cae4c;
}
.notification_widget.success .badge {
  color: #5cb85c;
  background-color: #fff;
}
.notification_widget.info {
  color: #fff;
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info:focus,
.notification_widget.info.focus {
  color: #fff;
  background-color: #31b0d5;
  border-color: #1b6d85;
}
.notification_widget.info:hover {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  color: #fff;
  background-color: #31b0d5;
  border-color: #269abc;
}
.notification_widget.info:active:hover,
.notification_widget.info.active:hover,
.open > .dropdown-toggle.notification_widget.info:hover,
.notification_widget.info:active:focus,
.notification_widget.info.active:focus,
.open > .dropdown-toggle.notification_widget.info:focus,
.notification_widget.info:active.focus,
.notification_widget.info.active.focus,
.open > .dropdown-toggle.notification_widget.info.focus {
  color: #fff;
  background-color: #269abc;
  border-color: #1b6d85;
}
.notification_widget.info:active,
.notification_widget.info.active,
.open > .dropdown-toggle.notification_widget.info {
  background-image: none;
}
.notification_widget.info.disabled:hover,
.notification_widget.info[disabled]:hover,
fieldset[disabled] .notification_widget.info:hover,
.notification_widget.info.disabled:focus,
.notification_widget.info[disabled]:focus,
fieldset[disabled] .notification_widget.info:focus,
.notification_widget.info.disabled.focus,
.notification_widget.info[disabled].focus,
fieldset[disabled] .notification_widget.info.focus {
  background-color: #5bc0de;
  border-color: #46b8da;
}
.notification_widget.info .badge {
  color: #5bc0de;
  background-color: #fff;
}
.notification_widget.danger {
  color: #fff;
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger:focus,
.notification_widget.danger.focus {
  color: #fff;
  background-color: #c9302c;
  border-color: #761c19;
}
.notification_widget.danger:hover {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  color: #fff;
  background-color: #c9302c;
  border-color: #ac2925;
}
.notification_widget.danger:active:hover,
.notification_widget.danger.active:hover,
.open > .dropdown-toggle.notification_widget.danger:hover,
.notification_widget.danger:active:focus,
.notification_widget.danger.active:focus,
.open > .dropdown-toggle.notification_widget.danger:focus,
.notification_widget.danger:active.focus,
.notification_widget.danger.active.focus,
.open > .dropdown-toggle.notification_widget.danger.focus {
  color: #fff;
  background-color: #ac2925;
  border-color: #761c19;
}
.notification_widget.danger:active,
.notification_widget.danger.active,
.open > .dropdown-toggle.notification_widget.danger {
  background-image: none;
}
.notification_widget.danger.disabled:hover,
.notification_widget.danger[disabled]:hover,
fieldset[disabled] .notification_widget.danger:hover,
.notification_widget.danger.disabled:focus,
.notification_widget.danger[disabled]:focus,
fieldset[disabled] .notification_widget.danger:focus,
.notification_widget.danger.disabled.focus,
.notification_widget.danger[disabled].focus,
fieldset[disabled] .notification_widget.danger.focus {
  background-color: #d9534f;
  border-color: #d43f3a;
}
.notification_widget.danger .badge {
  color: #d9534f;
  background-color: #fff;
}
div#pager {
  background-color: #fff;
  font-size: 14px;
  line-height: 20px;
  overflow: hidden;
  display: none;
  position: fixed;
  bottom: 0px;
  width: 100%;
  max-height: 50%;
  padding-top: 8px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  /* Display over codemirror */
  z-index: 100;
  /* Hack which prevents jquery ui resizable from changing top. */
  top: auto !important;
}
div#pager pre {
  line-height: 1.21429em;
  color: #000;
  background-color: #f7f7f7;
  padding: 0.4em;
}
div#pager #pager-button-area {
  position: absolute;
  top: 8px;
  right: 20px;
}
div#pager #pager-contents {
  position: relative;
  overflow: auto;
  width: 100%;
  height: 100%;
}
div#pager #pager-contents #pager-container {
  position: relative;
  padding: 15px 0px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
div#pager .ui-resizable-handle {
  top: 0px;
  height: 8px;
  background: #f7f7f7;
  border-top: 1px solid #cfcfcf;
  border-bottom: 1px solid #cfcfcf;
  /* This injects handle bars (a short, wide = symbol) for 
        the resize handle. */
}
div#pager .ui-resizable-handle::after {
  content: '';
  top: 2px;
  left: 50%;
  height: 3px;
  width: 30px;
  margin-left: -15px;
  position: absolute;
  border-top: 1px solid #cfcfcf;
}
.quickhelp {
  /* Old browsers */
  display: -webkit-box;
  -webkit-box-orient: horizontal;
  -webkit-box-align: stretch;
  display: -moz-box;
  -moz-box-orient: horizontal;
  -moz-box-align: stretch;
  display: box;
  box-orient: horizontal;
  box-align: stretch;
  /* Modern browsers */
  display: flex;
  flex-direction: row;
  align-items: stretch;
  line-height: 1.8em;
}
.shortcut_key {
  display: inline-block;
  width: 21ex;
  text-align: right;
  font-family: monospace;
}
.shortcut_descr {
  display: inline-block;
  /* Old browsers */
  -webkit-box-flex: 1;
  -moz-box-flex: 1;
  box-flex: 1;
  /* Modern browsers */
  flex: 1;
}
span.save_widget {
  margin-top: 6px;
}
span.save_widget span.filename {
  height: 1em;
  line-height: 1em;
  padding: 3px;
  margin-left: 16px;
  border: none;
  font-size: 146.5%;
  border-radius: 2px;
}
span.save_widget span.filename:hover {
  background-color: #e6e6e6;
}
span.checkpoint_status,
span.autosave_status {
  font-size: small;
}
@media (max-width: 767px) {
  span.save_widget {
    font-size: small;
  }
  span.checkpoint_status,
  span.autosave_status {
    display: none;
  }
}
@media (min-width: 768px) and (max-width: 991px) {
  span.checkpoint_status {
    display: none;
  }
  span.autosave_status {
    font-size: x-small;
  }
}
.toolbar {
  padding: 0px;
  margin-left: -5px;
  margin-top: 2px;
  margin-bottom: 5px;
  box-sizing: border-box;
  -moz-box-sizing: border-box;
  -webkit-box-sizing: border-box;
}
.toolbar select,
.toolbar label {
  width: auto;
  vertical-align: middle;
  margin-right: 2px;
  margin-bottom: 0px;
  display: inline;
  font-size: 92%;
  margin-left: 0.3em;
  margin-right: 0.3em;
  padding: 0px;
  padding-top: 3px;
}
.toolbar .btn {
  padding: 2px 8px;
}
.toolbar .btn-group {
  margin-top: 0px;
  margin-left: 5px;
}
#maintoolbar {
  margin-bottom: -3px;
  margin-top: -8px;
  border: 0px;
  min-height: 27px;
  margin-left: 0px;
  padding-top: 11px;
  padding-bottom: 3px;
}
#maintoolbar .navbar-text {
  float: none;
  vertical-align: middle;
  text-align: right;
  margin-left: 5px;
  margin-right: 0px;
  margin-top: 0px;
}
.select-xs {
  height: 24px;
}
.pulse,
.dropdown-menu > li > a.pulse,
li.pulse > a.dropdown-toggle,
li.pulse.open > a.dropdown-toggle {
  background-color: #F37626;
  color: white;
}
/**
 * Primary styles
 *
 * Author: Jupyter Development Team
 */
/** WARNING IF YOU ARE EDITTING THIS FILE, if this is a .css file, It has a lot
 * of chance of beeing generated from the ../less/[samename].less file, you can
 * try to get back the less file by reverting somme commit in history
 **/
/*
 * We'll try to get something pretty, so we
 * have some strange css to have the scroll bar on
 * the left with fix button on the top right of the tooltip
 */
@-moz-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-webkit-keyframes fadeOut {
  from {
    opacity: 1;
  }
  to {
    opacity: 0;
  }
}
@-moz-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
@-webkit-keyframes fadeIn {
  from {
    opacity: 0;
  }
  to {
    opacity: 1;
  }
}
/*properties of tooltip after "expand"*/
.bigtooltip {
  overflow: auto;
  height: 200px;
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
}
/*properties of tooltip before "expand"*/
.smalltooltip {
  -webkit-transition-property: height;
  -webkit-transition-duration: 500ms;
  -moz-transition-property: height;
  -moz-transition-duration: 500ms;
  transition-property: height;
  transition-duration: 500ms;
  text-overflow: ellipsis;
  overflow: hidden;
  height: 80px;
}
.tooltipbuttons {
  position: absolute;
  padding-right: 15px;
  top: 0px;
  right: 0px;
}
.tooltiptext {
  /*avoid the button to overlap on some docstring*/
  padding-right: 30px;
}
.ipython_tooltip {
  max-width: 700px;
  /*fade-in animation when inserted*/
  -webkit-animation: fadeOut 400ms;
  -moz-animation: fadeOut 400ms;
  animation: fadeOut 400ms;
  -webkit-animation: fadeIn 400ms;
  -moz-animation: fadeIn 400ms;
  animation: fadeIn 400ms;
  vertical-align: middle;
  background-color: #f7f7f7;
  overflow: visible;
  border: #ababab 1px solid;
  outline: none;
  padding: 3px;
  margin: 0px;
  padding-left: 7px;
  font-family: monospace;
  min-height: 50px;
  -moz-box-shadow: 0px 6px 10px -1px #adadad;
  -webkit-box-shadow: 0px 6px 10px -1px #adadad;
  box-shadow: 0px 6px 10px -1px #adadad;
  border-radius: 2px;
  position: absolute;
  z-index: 1000;
}
.ipython_tooltip a {
  float: right;
}
.ipython_tooltip .tooltiptext pre {
  border: 0;
  border-radius: 0;
  font-size: 100%;
  background-color: #f7f7f7;
}
.pretooltiparrow {
  left: 0px;
  margin: 0px;
  top: -16px;
  width: 40px;
  height: 16px;
  overflow: hidden;
  position: absolute;
}
.pretooltiparrow:before {
  background-color: #f7f7f7;
  border: 1px #ababab solid;
  z-index: 11;
  content: "";
  position: absolute;
  left: 15px;
  top: 10px;
  width: 25px;
  height: 25px;
  -webkit-transform: rotate(45deg);
  -moz-transform: rotate(45deg);
  -ms-transform: rotate(45deg);
  -o-transform: rotate(45deg);
}
ul.typeahead-list i {
  margin-left: -10px;
  width: 18px;
}
ul.typeahead-list {
  max-height: 80vh;
  overflow: auto;
}
ul.typeahead-list > li > a {
  /** Firefox bug **/
  /* see https://github.com/jupyter/notebook/issues/559 */
  white-space: normal;
}
.cmd-palette .modal-body {
  padding: 7px;
}
.cmd-palette form {
  background: white;
}
.cmd-palette input {
  outline: none;
}
.no-shortcut {
  display: none;
}
.command-shortcut:before {
  content: "(command)";
  padding-right: 3px;
  color: #777777;
}
.edit-shortcut:before {
  content: "(edit)";
  padding-right: 3px;
  color: #777777;
}
#find-and-replace #replace-preview .match,
#find-and-replace #replace-preview .insert {
  background-color: #BBDEFB;
  border-color: #90CAF9;
  border-style: solid;
  border-width: 1px;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .match {
  background-color: #FFCDD2;
  border-color: #EF9A9A;
  border-radius: 0px;
}
#find-and-replace #replace-preview .replace .insert {
  background-color: #C8E6C9;
  border-color: #A5D6A7;
  border-radius: 0px;
}
#find-and-replace #replace-preview {
  max-height: 60vh;
  overflow: auto;
}
#find-and-replace #replace-preview pre {
  padding: 5px 10px;
}
.terminal-app {
  background: #EEE;
}
.terminal-app #header {
  background: #fff;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.2);
}
.terminal-app .terminal {
  width: 100%;
  float: left;
  font-family: monospace;
  color: white;
  background: black;
  padding: 0.4em;
  border-radius: 2px;
  -webkit-box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
  box-shadow: 0px 0px 12px 1px rgba(87, 87, 87, 0.4);
}
.terminal-app .terminal,
.terminal-app .terminal dummy-screen {
  line-height: 1em;
  font-size: 14px;
}
.terminal-app .terminal .xterm-rows {
  padding: 10px;
}
.terminal-app .terminal-cursor {
  color: black;
  background: white;
}
.terminal-app #terminado-container {
  margin-top: 20px;
}
/*# sourceMappingURL=style.min.css.map */
    </style>
<style type="text/css">
    .highlight .hll { background-color: #ffffcc }
.highlight  { background: #f8f8f8; }
.highlight .c { color: #408080; font-style: italic } /* Comment */
.highlight .err { border: 1px solid #FF0000 } /* Error */
.highlight .k { color: #008000; font-weight: bold } /* Keyword */
.highlight .o { color: #666666 } /* Operator */
.highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */
.highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */
.highlight .cp { color: #BC7A00 } /* Comment.Preproc */
.highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */
.highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */
.highlight .cs { color: #408080; font-style: italic } /* Comment.Special */
.highlight .gd { color: #A00000 } /* Generic.Deleted */
.highlight .ge { font-style: italic } /* Generic.Emph */
.highlight .gr { color: #FF0000 } /* Generic.Error */
.highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */
.highlight .gi { color: #00A000 } /* Generic.Inserted */
.highlight .go { color: #888888 } /* Generic.Output */
.highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */
.highlight .gs { font-weight: bold } /* Generic.Strong */
.highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */
.highlight .gt { color: #0044DD } /* Generic.Traceback */
.highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */
.highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */
.highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */
.highlight .kp { color: #008000 } /* Keyword.Pseudo */
.highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */
.highlight .kt { color: #B00040 } /* Keyword.Type */
.highlight .m { color: #666666 } /* Literal.Number */
.highlight .s { color: #BA2121 } /* Literal.String */
.highlight .na { color: #7D9029 } /* Name.Attribute */
.highlight .nb { color: #008000 } /* Name.Builtin */
.highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */
.highlight .no { color: #880000 } /* Name.Constant */
.highlight .nd { color: #AA22FF } /* Name.Decorator */
.highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */
.highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */
.highlight .nf { color: #0000FF } /* Name.Function */
.highlight .nl { color: #A0A000 } /* Name.Label */
.highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */
.highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */
.highlight .nv { color: #19177C } /* Name.Variable */
.highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */
.highlight .w { color: #bbbbbb } /* Text.Whitespace */
.highlight .mb { color: #666666 } /* Literal.Number.Bin */
.highlight .mf { color: #666666 } /* Literal.Number.Float */
.highlight .mh { color: #666666 } /* Literal.Number.Hex */
.highlight .mi { color: #666666 } /* Literal.Number.Integer */
.highlight .mo { color: #666666 } /* Literal.Number.Oct */
.highlight .sa { color: #BA2121 } /* Literal.String.Affix */
.highlight .sb { color: #BA2121 } /* Literal.String.Backtick */
.highlight .sc { color: #BA2121 } /* Literal.String.Char */
.highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */
.highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */
.highlight .s2 { color: #BA2121 } /* Literal.String.Double */
.highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */
.highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */
.highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */
.highlight .sx { color: #008000 } /* Literal.String.Other */
.highlight .sr { color: #BB6688 } /* Literal.String.Regex */
.highlight .s1 { color: #BA2121 } /* Literal.String.Single */
.highlight .ss { color: #19177C } /* Literal.String.Symbol */
.highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */
.highlight .fm { color: #0000FF } /* Name.Function.Magic */
.highlight .vc { color: #19177C } /* Name.Variable.Class */
.highlight .vg { color: #19177C } /* Name.Variable.Global */
.highlight .vi { color: #19177C } /* Name.Variable.Instance */
.highlight .vm { color: #19177C } /* Name.Variable.Magic */
.highlight .il { color: #666666 } /* Literal.Number.Integer.Long */
    </style>
<style type="text/css">
    
/* Temporary definitions which will become obsolete with Notebook release 5.0 */
.ansi-black-fg { color: #3E424D; }
.ansi-black-bg { background-color: #3E424D; }
.ansi-black-intense-fg { color: #282C36; }
.ansi-black-intense-bg { background-color: #282C36; }
.ansi-red-fg { color: #E75C58; }
.ansi-red-bg { background-color: #E75C58; }
.ansi-red-intense-fg { color: #B22B31; }
.ansi-red-intense-bg { background-color: #B22B31; }
.ansi-green-fg { color: #00A250; }
.ansi-green-bg { background-color: #00A250; }
.ansi-green-intense-fg { color: #007427; }
.ansi-green-intense-bg { background-color: #007427; }
.ansi-yellow-fg { color: #DDB62B; }
.ansi-yellow-bg { background-color: #DDB62B; }
.ansi-yellow-intense-fg { color: #B27D12; }
.ansi-yellow-intense-bg { background-color: #B27D12; }
.ansi-blue-fg { color: #208FFB; }
.ansi-blue-bg { background-color: #208FFB; }
.ansi-blue-intense-fg { color: #0065CA; }
.ansi-blue-intense-bg { background-color: #0065CA; }
.ansi-magenta-fg { color: #D160C4; }
.ansi-magenta-bg { background-color: #D160C4; }
.ansi-magenta-intense-fg { color: #A03196; }
.ansi-magenta-intense-bg { background-color: #A03196; }
.ansi-cyan-fg { color: #60C6C8; }
.ansi-cyan-bg { background-color: #60C6C8; }
.ansi-cyan-intense-fg { color: #258F8F; }
.ansi-cyan-intense-bg { background-color: #258F8F; }
.ansi-white-fg { color: #C5C1B4; }
.ansi-white-bg { background-color: #C5C1B4; }
.ansi-white-intense-fg { color: #A1A6B2; }
.ansi-white-intense-bg { background-color: #A1A6B2; }

.ansi-bold { font-weight: bold; }

    </style>


<style type="text/css">
/* Overrides of notebook CSS for static HTML export */
body {
  overflow: visible;
  padding: 8px;
}

div#notebook {
  overflow: visible;
  border-top: none;
}@media print {
  div.cell {
    display: block;
    page-break-inside: avoid;
  } 
  div.output_wrapper { 
    display: block;
    page-break-inside: avoid; 
  }
  div.output { 
    display: block;
    page-break-inside: avoid; 
  }
}
</style>

<!-- Custom stylesheet, it must be in the same directory as the html file -->
<link rel="stylesheet" href="custom.css">

<!-- Loading mathjax macro -->
<!-- Load mathjax -->
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_HTML"></script>
    <!-- MathJax configuration -->
    <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ['$','$'], ["\\(","\\)"] ],
            displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
            processEscapes: true,
            processEnvironments: true
        },
        // Center justify equations in code and markdown cells. Elsewhere
        // we use CSS to left justify single line equations in code cells.
        displayAlign: 'center',
        "HTML-CSS": {
            styles: {'.MathJax_Display': {"margin": 0}},
            linebreaks: { automatic: true }
        }
    });
    </script>
    <!-- End of mathjax configuration --></head>
<body>
  <div tabindex="-1" id="notebook" class="border-box-sizing">
    <div class="container" id="notebook-container">

<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h1 id="Machine-Learning-Engineer-Nanodegree">Machine Learning Engineer Nanodegree<a class="anchor-link" href="#Machine-Learning-Engineer-Nanodegree">&#182;</a></h1><h2 id="Reinforcement-Learning">Reinforcement Learning<a class="anchor-link" href="#Reinforcement-Learning">&#182;</a></h2><h2 id="Project:-Train-a-Smartcab-to-Drive">Project: Train a Smartcab to Drive<a class="anchor-link" href="#Project:-Train-a-Smartcab-to-Drive">&#182;</a></h2><p>Welcome to the fourth project of the Machine Learning Engineer Nanodegree! In this notebook, template code has already been provided for you to aid in your analysis of the <em>Smartcab</em> and your implemented learning algorithm. You will not need to modify the included code beyond what is requested. There will be questions that you must answer which relate to the project and the visualizations provided in the notebook. Each section where you will answer a question is preceded by a <strong>'Question X'</strong> header. Carefully read each question and provide thorough answers in the following text boxes that begin with <strong>'Answer:'</strong>. Your project submission will be evaluated based on your answers to each of the questions and the implementation you provide in <code>agent.py</code>.</p>
<blockquote><p><strong>Note:</strong> Code and Markdown cells can be executed using the <strong>Shift + Enter</strong> keyboard shortcut. In addition, Markdown cells can be edited by typically double-clicking the cell to enter edit mode.</p>
</blockquote>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Getting-Started">Getting Started<a class="anchor-link" href="#Getting-Started">&#182;</a></h2><p>In this project, you will work towards constructing an optimized Q-Learning driving agent that will navigate a <em>Smartcab</em> through its environment towards a goal. Since the <em>Smartcab</em> is expected to drive passengers from one location to another, the driving agent will be evaluated on two very important metrics: <strong>Safety</strong> and <strong>Reliability</strong>. A driving agent that gets the <em>Smartcab</em> to its destination while running red lights or narrowly avoiding accidents would be considered <strong>unsafe</strong>. Similarly, a driving agent that frequently fails to reach the destination in time would be considered <strong>unreliable</strong>. Maximizing the driving agent's <strong>safety</strong> and <strong>reliability</strong> would ensure that <em>Smartcabs</em> have a permanent place in the transportation industry.</p>
<p><strong>Safety</strong> and <strong>Reliability</strong> are measured using a letter-grade system as follows:</p>
<table>
<thead><tr>
<th style="text-align:center">Grade</th>
<th style="text-align:center">Safety</th>
<th style="text-align:center">Reliability</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">A+</td>
<td style="text-align:center">Agent commits no traffic violations,<br/>and always chooses the correct action.</td>
<td style="text-align:center">Agent reaches the destination in time<br />for 100% of trips.</td>
</tr>
<tr>
<td style="text-align:center">A</td>
<td style="text-align:center">Agent commits few minor traffic violations,<br/>such as failing to move on a green light.</td>
<td style="text-align:center">Agent reaches the destination on time<br />for at least 90% of trips.</td>
</tr>
<tr>
<td style="text-align:center">B</td>
<td style="text-align:center">Agent commits frequent minor traffic violations,<br/>such as failing to move on a green light.</td>
<td style="text-align:center">Agent reaches the destination on time<br />for at least 80% of trips.</td>
</tr>
<tr>
<td style="text-align:center">C</td>
<td style="text-align:center">Agent commits at least one major traffic violation,<br/> such as driving through a red light.</td>
<td style="text-align:center">Agent reaches the destination on time<br />for at least 70% of trips.</td>
</tr>
<tr>
<td style="text-align:center">D</td>
<td style="text-align:center">Agent causes at least one minor accident,<br/> such as turning left on green with oncoming traffic.</td>
<td style="text-align:center">Agent reaches the destination on time<br />for at least 60% of trips.</td>
</tr>
<tr>
<td style="text-align:center">F</td>
<td style="text-align:center">Agent causes at least one major accident,<br />such as driving through a red light with cross-traffic.</td>
<td style="text-align:center">Agent fails to reach the destination on time<br />for at least 60% of trips.</td>
</tr>
</tbody>
</table>
<p>To assist evaluating these important metrics, you will need to load visualization code that will be used later on in the project. Run the code cell below to import this code which is required for your analysis.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[1]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># Import the visualization code</span>
<span class="kn">import</span> <span class="nn">visuals</span> <span class="kn">as</span> <span class="nn">vs</span>

<span class="c1"># Pretty display for notebooks</span>
<span class="o">%</span><span class="k">matplotlib</span> inline
</pre></div>

</div>
</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Understand-the-World">Understand the World<a class="anchor-link" href="#Understand-the-World">&#182;</a></h3><p>Before starting to work on implementing your driving agent, it's necessary to first understand the world (environment) which the <em>Smartcab</em> and driving agent work in. One of the major components to building a self-learning agent is understanding the characteristics about the agent, which includes how the agent operates. To begin, simply run the <code>agent.py</code> agent code exactly how it is -- no need to make any additions whatsoever. Let the resulting simulation run for some time to see the various working components. Note that in the visual simulation (if enabled), the <strong>white vehicle</strong> is the <em>Smartcab</em>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-1">Question 1<a class="anchor-link" href="#Question-1">&#182;</a></h3><p>In a few sentences, describe what you observe during the simulation when running the default <code>agent.py</code> agent code. Some things you could consider:</p>
<ul>
<li><em>Does the Smartcab move at all during the simulation?</em></li>
<li><em>What kind of rewards is the driving agent receiving?</em></li>
<li><em>How does the light changing color affect the rewards?</em>  </li>
</ul>
<p><strong>Hint:</strong> From the <code>/smartcab/</code> top-level directory (where this notebook is located), run the command</p>
<div class="highlight"><pre><span></span><span class="s1">&#39;python smartcab/agent.py&#39;</span>
</pre></div>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Answer:</strong></p>
<ul>
<li>No, the Smartcab doesn't move.</li>
<li>Positive rewards and negative rewards</li>
<li>When the smartcab stops and wait at a red light, it receive rewards and it loses rewards when it stops and wait at a green light.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Understand-the-Code">Understand the Code<a class="anchor-link" href="#Understand-the-Code">&#182;</a></h3><p>In addition to understanding the world, it is also necessary to understand the code itself that governs how the world, simulation, and so on operate. Attempting to create a driving agent would be difficult without having at least explored the <em>"hidden"</em> devices that make everything work. In the <code>/smartcab/</code> top-level directory, there are two folders: <code>/logs/</code> (which will be used later) and <code>/smartcab/</code>. Open the <code>/smartcab/</code> folder and explore each Python file included, then answer the following question.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-2">Question 2<a class="anchor-link" href="#Question-2">&#182;</a></h3><ul>
<li><em>In the </em><code>agent.py</code><em> Python file, choose three flags that can be set and explain how they change the simulation.</em></li>
<li><em>In the </em><code>environment.py</code><em> Python file, what Environment class function is called when an agent performs an action?</em></li>
<li><em>In the </em><code>simulator.py</code><em> Python file, what is the difference between the </em><code>'render_text()'</code><em> function and the </em><code>'render()'</code><em> function?</em></li>
<li><em>In the </em><code>planner.py</code><em> Python file, will the </em><code>'next_waypoint()</code><em> function consider the North-South or East-West direction first?</em></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Answer:</strong></p>
<p><strong>agent.py</strong></p>

<pre><code>- Learning: It is available using LearningAgent class. Its by default False, not to use Q-Learning and if set to True, agent uses Q-Learning
- num_dummies: It is available as part of Environment object, used to create dummy agents in the environment
- grid_size : It is available as part of Environment object, used to create number of intersections (columns, rows), default is (8,6)

</code></pre>
<p><strong>environment.py</strong></p>

<pre><code>- act() function is called when an agent performs an action

</code></pre>
<p><strong>simulator.py</strong></p>

<pre><code>- render_text() function is used when we need non-GUI display of the simulation in the terminal
- render() function helps to view the simulations in GUI

</code></pre>
<p><strong>planner.py</strong></p>

<pre><code>- 'next_waypoint() function considers East-West direction first</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Implement-a-Basic-Driving-Agent">Implement a Basic Driving Agent<a class="anchor-link" href="#Implement-a-Basic-Driving-Agent">&#182;</a></h2><p>The first step to creating an optimized Q-Learning driving agent is getting the agent to actually take valid actions. In this case, a valid action is one of <code>None</code>, (do nothing) <code>'left'</code> (turn left), <code>right'</code> (turn right), or <code>'forward'</code> (go forward). For your first implementation, navigate to the <code>'choose_action()'</code> agent function and make the driving agent randomly choose one of these actions. Note that you have access to several class variables that will help you write this functionality, such as <code>'self.learning'</code> and <code>'self.valid_actions'</code>. Once implemented, run the agent file and simulation briefly to confirm that your driving agent is taking a random action each time step.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Basic-Agent-Simulation-Results">Basic Agent Simulation Results<a class="anchor-link" href="#Basic-Agent-Simulation-Results">&#182;</a></h3><p>To obtain results from the initial simulation, you will need to adjust following flags:</p>
<ul>
<li><code>'enforce_deadline'</code> - Set this to <code>True</code> to force the driving agent to capture whether it reaches the destination in time.</li>
<li><code>'update_delay'</code> - Set this to a small value (such as <code>0.01</code>) to reduce the time between steps in each trial.</li>
<li><code>'log_metrics'</code> - Set this to <code>True</code> to log the simluation results as a <code>.csv</code> file in <code>/logs/</code>.</li>
<li><code>'n_test'</code> - Set this to <code>'10'</code> to perform 10 testing trials.</li>
</ul>
<p>Optionally, you may disable to the visual simulation (which can make the trials go faster) by setting the <code>'display'</code> flag to <code>False</code>. Flags that have been set here should be returned to their default setting when debugging. It is important that you understand what each flag does and how it affects the simulation!</p>
<p>Once you have successfully completed the initial simulation (there should have been 20 training trials and 10 testing trials), run the code cell below to visualize the results. Note that log files are overwritten when identical simulations are run, so be careful with what log file is being loaded!
Run the agent.py file after setting the flags from projects/smartcab folder instead of projects/smartcab/smartcab.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[11]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="o">%</span><span class="k">run</span> smartcab/agent.py
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
/-------------------------
| Training trial 1
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.50)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.35)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.07)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.94)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.77)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.76)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.80)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.33)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.68)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.38)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.95)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.40)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.65)
35% of time remaining to reach destination.
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.65)
35% of time remaining to reach destination.
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.65)
35% of time remaining to reach destination.
Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.65)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.22)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.86)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.29)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 1.17)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.34)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded -0.00)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 1.12)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 2
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of right. (rewarded 1.33)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.22)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.98)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 1.20)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of right. (rewarded 1.71)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove left instead of right. (rewarded 1.34)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.49)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.45)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.35)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove left instead of forward. (rewarded 1.60)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.70)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.22)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded -0.10)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.67)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.67)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.33)
47% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 3
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.12)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent attempted driving forward through a red light. (rewarded -10.63)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.85)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.48)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.79)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.96)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.97)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.00)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.75)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.90)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.82)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.30)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.66)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent drove right instead of left. (rewarded -0.07)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 1.70)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.39)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.74)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.01)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove left instead of right. (rewarded 1.42)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.93)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.24)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.20)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove left instead of right. (rewarded -0.53)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.77)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.91)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 4
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.73)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.63)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.97)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.17)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.86)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.11)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.51)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.65)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.42)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.82)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.42)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.27)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.50)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.31)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.82)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent drove right instead of forward. (rewarded 0.60)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.91)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.44)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.33)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.99)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 5
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.17)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.23)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.79)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent followed the waypoint right. (rewarded 2.09)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.13)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent drove right instead of left. (rewarded 0.37)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.76)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.64)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 0.95)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.69)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.70)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.47)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of forward. (rewarded -0.10)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.66)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 0.93)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.53)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.36)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.40)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent followed the waypoint left. (rewarded 0.38)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 6
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.50)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.02)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.54)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint left. (rewarded 1.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.74)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.17)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.86)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.76)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.68)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.26)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.54)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.82)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.82)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.81)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.85)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.04)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.75)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.47)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.81)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.40)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 7
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.18)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.52)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.19)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent followed the waypoint left. (rewarded 2.01)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.96)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.14)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.51)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove right instead of left. (rewarded 0.80)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of right. (rewarded 0.61)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove left instead of forward. (rewarded 0.02)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.57)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.39)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.05)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.95)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent drove right instead of forward. (rewarded 0.12)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.68)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.41)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.36)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.37)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove right instead of left. (rewarded 1.23)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.43)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.10)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove left instead of right. (rewarded 0.16)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.47)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.80)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.99)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove left instead of forward. (rewarded 0.23)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.54)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.14)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.45)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.14)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.64)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.11)
6% of time remaining to reach destination.

/-------------------
| Step 33 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.47)
3% of time remaining to reach destination.

/-------------------
| Step 34 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.83)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 8
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.93)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove left instead of right. (rewarded 0.00)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.46)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.67)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.49)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.48)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.78)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.21)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.53)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.49)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.53)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.42)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.09)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.82)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.24)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.35)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.72)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.48)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.36)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded -0.02)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.74)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.34)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.11)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light. (rewarded -10.81)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light. (rewarded -10.50)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 9
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.22)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.34)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.38)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.30)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 0.72)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.65)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.82)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.46)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.81)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.54)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.20)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.93)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;right&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.88)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.44)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of right. (rewarded 1.55)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.27)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove left instead of forward. (rewarded 1.10)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.66)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of right. (rewarded 1.29)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.60)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.67)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.72)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.90)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.67)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.11)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.68)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.45)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.09)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded -0.12)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.39)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 10
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.95)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.60)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.84)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 0.32)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.11)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove left instead of right. (rewarded 0.97)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 1.34)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.41)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.68)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.45)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.17)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.95)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.74)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.96)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.99)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.13)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.39)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.83)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.35)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.52)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 11
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.82)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.82)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent followed the waypoint right. (rewarded 1.75)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.12)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.82)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of left. (rewarded 0.75)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.78)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.88)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.75)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of left. (rewarded -0.11)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.32)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.31)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.49)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.20)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None)
Agent drove forward instead of left. (rewarded -0.09)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.78)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.32)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.08)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove left instead of right. (rewarded -0.26)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove left instead of forward. (rewarded 0.76)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 12
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 0.84)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.80)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.76)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove forward instead of right. (rewarded 0.17)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of right. (rewarded 1.55)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.04)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.43)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.41)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 13
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove left instead of right. (rewarded 0.61)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.52)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.42)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.31)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.08)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove left instead of forward. (rewarded 1.37)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.40)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.73)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.02)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove left instead of right. (rewarded 0.38)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.33)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.64)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.46)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove left instead of right. (rewarded 0.65)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent followed the waypoint left. (rewarded 2.17)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.38)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.31)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove left instead of forward. (rewarded 0.39)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.60)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.31)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 14
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.83)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of right. (rewarded 1.61)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.30)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.25)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.13)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.48)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.08)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of left. (rewarded 0.57)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.35)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.14)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove right instead of left. (rewarded 1.50)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.63)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.32)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.25)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.72)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.99)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.30)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.09)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.36)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.41)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 15
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 2.05)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.08)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.87)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.57)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent drove forward instead of left. (rewarded 0.52)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 0.03)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.63)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.45)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.65)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.18)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.58)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.03)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.94)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.81)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove left instead of forward. (rewarded 0.76)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.03)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 2.44)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.56)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.54)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.25)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.69)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.78)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.66)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded -0.62)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.39)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 16
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.27)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.10)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.27)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.99)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.97)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.94)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.19)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.22)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.83)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of left. (rewarded 0.27)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.26)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.60)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.55)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.87)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.94)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.01)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.70)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.98)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.07)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.14)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 17
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.58)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent followed the waypoint right. (rewarded 1.84)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light. (rewarded -10.09)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 0.45)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.67)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 1.18)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.95)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.48)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.97)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.88)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.17)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.45)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.07)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.49)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent drove left instead of forward. (rewarded 0.38)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.54)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 18
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.97)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.44)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.20)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.54)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.03)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.54)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.37)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.28)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.10)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.77)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.02)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.69)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.84)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.06)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.25)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of left. (rewarded 1.38)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.86)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.11)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.05)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.06)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.49)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.29)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint left. (rewarded 0.45)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.38)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.06)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 19
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove left instead of forward. (rewarded 0.36)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.83)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.26)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.96)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of right. (rewarded 0.39)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 0.98)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.49)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.25)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.71)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove left instead of right. (rewarded 1.28)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.67)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.91)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.29)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.18)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.03)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.24)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.81)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.03)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.73)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.55)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.27)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.97)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.07)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.56)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.67)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 20
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.06)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.03)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.85)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.20)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.79)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 2.79)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.89)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.24)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent drove right instead of forward. (rewarded 1.02)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.08)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.71)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent drove right instead of left. (rewarded 1.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent drove right instead of left. (rewarded 1.37)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.88)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.02)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.71)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.40)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove right instead of left. (rewarded -0.23)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.42)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.01)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.21)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.24)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.37)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint right. (rewarded 0.84)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.34)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove right instead of left. (rewarded -0.13)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.53)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.32)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.00)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.89)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove right instead of left. (rewarded 1.00)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 1
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 1.67)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.86)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.91)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove left instead of forward. (rewarded 0.25)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.56)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.04)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.75)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.59)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.63)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.03)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.84)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.40)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.34)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.45)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.10)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.88)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove right instead of left. (rewarded 0.88)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.02)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.03)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove forward instead of left. (rewarded 0.43)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 0.79)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 1.89)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent drove right instead of forward. (rewarded 0.55)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove right instead of left. (rewarded -0.75)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.95)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 2
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.33)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.48)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.95)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.00)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.77)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent drove right instead of left. (rewarded 0.78)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.36)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.47)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.76)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.80)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.09)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent drove right instead of forward. (rewarded 0.53)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.08)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.81)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 1.37)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent drove right instead of left. (rewarded 1.61)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.41)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.04)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 0.40)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.15)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.11)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded -0.48)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.79)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent drove left instead of forward. (rewarded -0.60)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.56)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 3
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove left instead of right. (rewarded 0.67)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 1.34)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove left instead of right. (rewarded 0.13)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light. (rewarded -9.79)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.26)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent drove right instead of left. (rewarded 1.73)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.09)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent followed the waypoint forward. (rewarded 2.60)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.33)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.67)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of left. (rewarded 1.62)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.20)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent attempted driving forward through a red light. (rewarded -10.31)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.35)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.27)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.36)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of left. (rewarded 1.27)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of left. (rewarded 0.52)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.97)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove left instead of forward. (rewarded -0.09)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 4
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent followed the waypoint left. (rewarded 2.44)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.48)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.39)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove left instead of forward. (rewarded 0.90)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove left instead of right. (rewarded 1.32)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.72)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.32)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.09)
64% of time remaining to reach destination.
Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.09)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.27)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.75)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 0.90)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.93)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.65)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.80)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.67)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.39)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.40)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.99)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.30)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.58)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.06)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint forward. (rewarded 0.81)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint forward. (rewarded 0.21)
4% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 5
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.29)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.14)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.33)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.08)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.88)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 2.65)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent drove right instead of forward. (rewarded 1.12)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent drove right instead of left. (rewarded 1.68)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded -0.11)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.06)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent followed the waypoint right. (rewarded 2.11)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.22)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 1.40)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 1.40)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.43)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 0.05)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.36)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.85)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.16)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 6
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove left instead of right. (rewarded 1.43)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.50)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.34)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.00)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.38)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.05)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -9.72)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent followed the waypoint forward. (rewarded 1.07)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.10)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.57)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.42)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.26)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 2.50)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 2.71)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent properly idled at a red light. (rewarded 1.64)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.18)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove right instead of left. (rewarded 0.81)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove forward instead of left. (rewarded 0.79)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.59)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent drove right instead of left. (rewarded 0.10)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.11)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove left instead of right. (rewarded 0.15)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.66)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.88)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent drove right instead of forward. (rewarded -0.61)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 7
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.23)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.61)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.93)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of left. (rewarded 0.80)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.80)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.89)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.28)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.66)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.45)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of left. (rewarded -0.07)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.92)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.68)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;right&#39;, None)
Agent followed the waypoint forward. (rewarded 1.80)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.32)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.19)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.56)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent drove right instead of left. (rewarded 0.28)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.94)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove forward instead of right. (rewarded 0.08)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove left instead of right. (rewarded -0.06)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 1.35)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 0.14)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.81)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.11)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -9.42)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 8
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.35)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.89)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint forward. (rewarded 2.63)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.58)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of forward. (rewarded 1.86)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint left. (rewarded 2.22)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove right instead of forward. (rewarded 1.20)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.57)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent drove right instead of left. (rewarded 1.46)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent drove forward instead of left. (rewarded 0.67)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.65)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of left. (rewarded 1.04)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.65)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent drove right instead of left. (rewarded 0.60)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light. (rewarded -9.73)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent properly idled at a red light. (rewarded 1.01)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.10)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.14)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent drove right instead of forward. (rewarded 0.83)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light. (rewarded -10.47)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.62)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.55)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.30)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.05)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 9
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.30)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.81)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 1.41)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light. (rewarded -10.89)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;forward&#39;, None)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.88)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.44)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint left. (rewarded 1.08)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;forward&#39;, None)
Agent drove forward instead of right. (rewarded 0.11)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove forward instead of right. (rewarded 0.38)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.91)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent attempted driving forward through a red light. (rewarded -10.23)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.15)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.15)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint right. (rewarded 1.07)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.43)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.83)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent properly idled at a red light. (rewarded 2.21)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent drove left instead of forward. (rewarded -0.05)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove forward instead of right. (rewarded 0.88)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent drove left instead of right. (rewarded 1.04)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 10
\-------------------------

Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.
Simulating trial. . . 
Agent not set to learn.

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, None, None)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.56)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;red&#39;, &#39;right&#39;, None)
Agent drove right instead of left. (rewarded 0.60)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, &#39;left&#39;, None)
Agent attempted driving forward through a red light. (rewarded -10.89)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent idled at a green light with no oncoming traffic. (rewarded -4.16)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent idled at a green light with no oncoming traffic. (rewarded -5.69)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent properly idled at a red light. (rewarded 0.22)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;red&#39;, None, None)
Agent followed the waypoint right. (rewarded 1.00)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.80)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.72)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.45)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.06)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;left&#39;, &#39;green&#39;, &#39;left&#39;, None)
Agent followed the waypoint left. (rewarded 1.24)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;right&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint right. (rewarded 2.10)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;forward&#39;, &#39;green&#39;, None, None)
Agent followed the waypoint forward. (rewarded 1.72)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

Simulation ended. . . 
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>&lt;matplotlib.figure.Figure at 0x1088b50d0&gt;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[12]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># Load the &#39;sim_no-learning&#39; log file from the initial simulation results</span>
<span class="n">vs</span><span class="o">.</span><span class="n">plot_trials</span><span class="p">(</span><span class="s1">&#39;sim_no-learning.csv&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-3">Question 3<a class="anchor-link" href="#Question-3">&#182;</a></h3><p>Using the visualization above that was produced from your initial simulation, provide an analysis and make several observations about the driving agent. Be sure that you are making at least one observation about each panel present in the visualization. Some things you could consider:</p>
<ul>
<li><em>How frequently is the driving agent making bad decisions? How many of those bad decisions cause accidents?</em></li>
<li><em>Given that the agent is driving randomly, does the rate of reliability make sense?</em></li>
<li><em>What kind of rewards is the agent receiving for its actions? Do the rewards suggest it has been penalized heavily?</em></li>
<li><em>As the number of trials increases, does the outcome of results change significantly?</em></li>
<li><em>Would this Smartcab be considered safe and/or reliable for its passengers? Why or why not?</em></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Answer:</strong></p>

<pre><code>- Approximately 45% of the time, the drivingagent makes bad decisions, which includes approximately 5% major accident and 21% major violation.
- Yes, the rate of reliability is F, the agent is driving randomly
- The average rewards is -4 and the agent receives negative rewards across the trials.
- The outcome of results doesn't change significantly, with the increase in number of trials.
- The safety rating is F and reliability rating is F, the smartcab is not safe.</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Inform-the-Driving-Agent">Inform the Driving Agent<a class="anchor-link" href="#Inform-the-Driving-Agent">&#182;</a></h2><p>The second step to creating an optimized Q-learning driving agent is defining a set of states that the agent can occupy in the environment. Depending on the input, sensory data, and additional variables available to the driving agent, a set of states can be defined for the agent so that it can eventually <em>learn</em> what action it should take when occupying a state. The condition of <code>'if state then action'</code> for each state is called a <strong>policy</strong>, and is ultimately what the driving agent is expected to learn. Without defining states, the driving agent would never understand which action is most optimal -- or even what environmental variables and conditions it cares about!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Identify-States">Identify States<a class="anchor-link" href="#Identify-States">&#182;</a></h3><p>Inspecting the <code>'build_state()'</code> agent function shows that the driving agent is given the following data from the environment:</p>
<ul>
<li><code>'waypoint'</code>, which is the direction the <em>Smartcab</em> should drive leading to the destination, relative to the <em>Smartcab</em>'s heading.</li>
<li><code>'inputs'</code>, which is the sensor data from the <em>Smartcab</em>. It includes <ul>
<li><code>'light'</code>, the color of the light.</li>
<li><code>'left'</code>, the intended direction of travel for a vehicle to the <em>Smartcab</em>'s left. Returns <code>None</code> if no vehicle is present.</li>
<li><code>'right'</code>, the intended direction of travel for a vehicle to the <em>Smartcab</em>'s right. Returns <code>None</code> if no vehicle is present.</li>
<li><code>'oncoming'</code>, the intended direction of travel for a vehicle across the intersection from the <em>Smartcab</em>. Returns <code>None</code> if no vehicle is present.</li>
</ul>
</li>
<li><code>'deadline'</code>, which is the number of actions remaining for the <em>Smartcab</em> to reach the destination before running out of time.</li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-4">Question 4<a class="anchor-link" href="#Question-4">&#182;</a></h3><p><em>Which features available to the agent are most relevant for learning both <strong>safety</strong> and <strong>efficiency</strong>? Why are these features appropriate for modeling the </em>Smartcab<em> in the environment? If you did not choose some features, why are those features</em> not <em>appropriate? Please note that whatever features you eventually choose for your agent's state, must be argued for here. That is: your code in agent.py should reflect the features chosen in this answer.</em></p>
<p>NOTE: You are not allowed to engineer new features for the smartcab.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Answer:</strong></p>
<p>The below features seems to be most relevant for learning both safety and efficiency:</p>

<pre><code>- waypoint: It is important and used to make the Smartcab follow the direction leading to destination
- light: It is important and used to assign rewards, for example, giving negative rewards not stoping at red light and stopping at green light
- oncoming: It is important when the smartcab is at the intersection to see if a vehicle is coming to take left and can give negative rewards if it violates
- left: It is important, when agent shoudn't take the left when there is a red light, can give negative rewards if it takes and it will learn

</code></pre>
<p>Features which are not appropriate:</p>

<pre><code>- deadline: There may be violations if the number of actions exhausted, when considering safety
- right: It doesn't affect much if we are taking right</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Define-a-State-Space">Define a State Space<a class="anchor-link" href="#Define-a-State-Space">&#182;</a></h3><p>When defining a set of states that the agent can occupy, it is necessary to consider the <em>size</em> of the state space. That is to say, if you expect the driving agent to learn a <strong>policy</strong> for each state, you would need to have an optimal action for <em>every</em> state the agent can occupy. If the number of all possible states is very large, it might be the case that the driving agent never learns what to do in some states, which can lead to uninformed decisions. For example, consider a case where the following features are used to define the state of the <em>Smartcab</em>:</p>
<p><code>('is_raining', 'is_foggy', 'is_red_light', 'turn_left', 'no_traffic', 'previous_turn_left', 'time_of_day')</code>.</p>
<p>How frequently would the agent occupy a state like <code>(False, True, True, True, False, False, '3AM')</code>? Without a near-infinite amount of time for training, it's doubtful the agent would ever learn the proper action!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-5">Question 5<a class="anchor-link" href="#Question-5">&#182;</a></h3><p><em>If a state is defined using the features you've selected from <strong>Question 4</strong>, what would be the size of the state space? Given what you know about the environment and how it is simulated, do you think the driving agent could learn a policy for each possible state within a reasonable number of training trials?</em><br>
<strong>Hint:</strong> Consider the <em>combinations</em> of features to calculate the total number of states!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Answer:</strong></p>
<p>State Space based on Q4 would be 96.</p>

<pre><code>- waypoint:3 (left, right, forward)
- light:2 (red, green)
- left:4 (forward, left, right, and None)
- oncoming:4 (forward, left, right, and None).
So 3*2*4*4 = 96. 

</code></pre>
<p>I think the agent could learn a policy for each possible state within a reasonal number of training trials.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Update-the-Driving-Agent-State">Update the Driving Agent State<a class="anchor-link" href="#Update-the-Driving-Agent-State">&#182;</a></h3><p>For your second implementation, navigate to the <code>'build_state()'</code> agent function. With the justification you've provided in <strong>Question 4</strong>, you will now set the <code>'state'</code> variable to a tuple of all the features necessary for Q-Learning. Confirm your driving agent is updating its state by running the agent file and simulation briefly and note whether the state is displaying. If the visual simulation is used, confirm that the updated state corresponds with what is seen in the simulation.</p>
<p><strong>Note:</strong> Remember to reset simulation flags to their default setting when making this observation!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Implement-a-Q-Learning-Driving-Agent">Implement a Q-Learning Driving Agent<a class="anchor-link" href="#Implement-a-Q-Learning-Driving-Agent">&#182;</a></h2><p>The third step to creating an optimized Q-Learning agent is to begin implementing the functionality of Q-Learning itself. The concept of Q-Learning is fairly straightforward: For every state the agent visits, create an entry in the Q-table for all state-action pairs available. Then, when the agent encounters a state and performs an action, update the Q-value associated with that state-action pair based on the reward received and the iterative update rule implemented. Of course, additional benefits come from Q-Learning, such that we can have the agent choose the <em>best</em> action for each state based on the Q-values of each state-action pair possible. For this project, you will be implementing a <em>decaying,</em> $\epsilon$<em>-greedy</em> Q-learning algorithm with <em>no</em> discount factor. Follow the implementation instructions under each <strong>TODO</strong> in the agent functions.</p>
<p>Note that the agent attribute <code>self.Q</code> is a dictionary: This is how the Q-table will be formed. Each state will be a key of the <code>self.Q</code> dictionary, and each value will then be another dictionary that holds the <em>action</em> and <em>Q-value</em>. Here is an example:</p>

<pre><code>{ 'state-1': { 
    'action-1' : Qvalue-1,
    'action-2' : Qvalue-2,
     ...
   },
  'state-2': {
    'action-1' : Qvalue-1,
     ...
   },
   ...
}</code></pre>
<p>Furthermore, note that you are expected to use a <em>decaying</em> $\epsilon$ <em>(exploration) factor</em>. Hence, as the number of trials increases, $\epsilon$ should decrease towards 0. This is because the agent is expected to learn from its behavior and begin acting on its learned behavior. Additionally, The agent will be tested on what it has learned after $\epsilon$ has passed a certain threshold (the default threshold is 0.05). For the initial Q-Learning implementation, you will be implementing a linear decaying function for $\epsilon$.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Q-Learning-Simulation-Results">Q-Learning Simulation Results<a class="anchor-link" href="#Q-Learning-Simulation-Results">&#182;</a></h3><p>To obtain results from the initial Q-Learning implementation, you will need to adjust the following flags and setup:</p>
<ul>
<li><code>'enforce_deadline'</code> - Set this to <code>True</code> to force the driving agent to capture whether it reaches the destination in time.</li>
<li><code>'update_delay'</code> - Set this to a small value (such as <code>0.01</code>) to reduce the time between steps in each trial.</li>
<li><code>'log_metrics'</code> - Set this to <code>True</code> to log the simluation results as a <code>.csv</code> file and the Q-table as a <code>.txt</code> file in <code>/logs/</code>.</li>
<li><code>'n_test'</code> - Set this to <code>'10'</code> to perform 10 testing trials.</li>
<li><code>'learning'</code> - Set this to <code>'True'</code> to tell the driving agent to use your Q-Learning implementation.</li>
</ul>
<p>In addition, use the following decay function for $\epsilon$:</p>
<p>$$ \epsilon_{t+1} = \epsilon_{t} - 0.05, \hspace{10px}\textrm{for trial number } t$$</p>
<p>If you have difficulty getting your implementation to work, try setting the <code>'verbose'</code> flag to <code>True</code> to help debug. Flags that have been set here should be returned to their default setting when debugging. It is important that you understand what each flag does and how it affects the simulation!</p>
<p>Once you have successfully completed the initial Q-Learning simulation, run the code cell below to visualize the results. Note that log files are overwritten when identical simulations are run, so be careful with what log file is being loaded!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[17]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="o">%</span><span class="k">run</span> smartcab/agent.py
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
/-------------------------
| Training trial 1
\-------------------------

Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.51)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.51)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.78)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.33)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.38)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.09)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.81)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.38)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.32)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.30)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.65)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.85)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.25)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.35)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.09)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.63)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.38)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.27)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.99)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.47)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.11)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.01)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.33)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.52)
20% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.52)
20% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.52)
20% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.52)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.69)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.44)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.69)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.49)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.36)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.03)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 2
\-------------------------

Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.09)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.27)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.47)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.94)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.96)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.93)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.85)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.40)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.26)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.36)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.23)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.89)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.03)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.40)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.89)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.44)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.29)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.07)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.33)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.43)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 3
\-------------------------

Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.54)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.70)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.82)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.10)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.32)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.68)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.41)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.32)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.94)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.01)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.06)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.72)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.27)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.88)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.50)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.34)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.66)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.70)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.93)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.36)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.54)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.43)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.23)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.24)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.91)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.21)
10% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 4
\-------------------------

Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.07)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.41)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.22)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.92)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.14)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.66)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.88)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.79)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.00)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.57)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.02)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.29)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.21)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.80)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.34)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.50)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded -0.45)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.57)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.51)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.47)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 5
\-------------------------

Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.34)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.05)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.56)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.00)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.52)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.15)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.35)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.31)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.08)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.06)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.04)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.78)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.33)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.45)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.01)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.99)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.10)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.52)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.31)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.37)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.04)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.95)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.36)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.99)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 6
\-------------------------

Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.97)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.30)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.24)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.23)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.70)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.47)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.64)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.41)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.65)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.80)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.60)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.07)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.50)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.05)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.89)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.99)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.30)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.30)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.21)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.03)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.86)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.78)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.82)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.38)
4% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.38)
4% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.38)
4% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.38)
4% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.38)
4% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.38)
4% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.38)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.27)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 7
\-------------------------

Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.72)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.87)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.08)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.37)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.34)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.30)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.08)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.44)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.64)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.98)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.95)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.24)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.77)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.22)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.29)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.99)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.60)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.63)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.17)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 8
\-------------------------

Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.41)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.16)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.19)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.09)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.04)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.44)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.35)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.93)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.21)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.13)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.05)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.24)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.38)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.15)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.44)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.94)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.17)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.42)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.22)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.33)
33% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.33)
33% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.33)
33% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.33)
33% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.33)
33% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.33)
33% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.33)
33% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.33)
33% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.33)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.28)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.36)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.19)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.82)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.49)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.68)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.08)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.94)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.91)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.50)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 9
\-------------------------

Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.37)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.78)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.36)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.04)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.34)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.88)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.03)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.30)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.23)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.57)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.77)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.85)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.85)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.51)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.16)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.54)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.36)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.47)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.33)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.95)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.30)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.52)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.09)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded -0.33)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 10
\-------------------------

Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.03)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.49)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.75)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.32)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.48)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.18)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.78)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.34)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.55)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.09)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.10)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.14)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.06)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.45)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.73)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.89)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.58)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.84)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.05)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 11
\-------------------------

Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.40)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.30)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.33)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.90)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.49)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.84)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.63)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.59)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.52)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.86)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.36)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.92)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.74)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.96)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.39)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.33)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.22)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.13)
5% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 12
\-------------------------

Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.60)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.96)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.72)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.55)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.95)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.86)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.16)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.31)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.12)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.03)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.12)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.24)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.32)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.74)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.96)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.86)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.54)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.28)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.87)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.00)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.39)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.40)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.33)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.98)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.58)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.47)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.48)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded -0.31)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.65)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.99)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 13
\-------------------------

Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.86)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.64)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.90)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.49)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.50)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.74)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.72)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.82)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.30)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.62)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.13)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 14
\-------------------------

Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.94)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.63)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.10)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.95)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.46)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.84)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.15)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.77)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.53)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.66)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.95)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.57)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.85)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.89)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.12)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.20)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.14)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.16)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.26)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.03)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.47)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.00)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.48)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.83)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.88)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.99)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.16)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.44)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.49)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.23)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 15
\-------------------------

Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.70)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.28)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.53)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.33)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.58)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.60)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.26)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.83)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.73)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.17)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.63)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.17)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.89)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.22)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.53)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.42)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.72)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.93)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.21)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.92)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.25)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.50)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.85)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 16
\-------------------------

Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.72)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.28)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.87)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.47)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.46)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.48)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.71)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.39)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.26)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.17)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.03)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.12)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.57)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.96)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.95)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.88)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.94)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.92)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.35)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.79)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.27)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.30)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.05)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.46)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 17
\-------------------------

Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.13)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.42)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.57)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.94)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.67)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.01)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.67)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.79)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.20)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.27)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.37)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.05)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.72)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.76)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.65)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.08)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.52)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.43)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.04)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.33)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.91)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.84)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.86)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.52)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 18
\-------------------------

Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.75)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.30)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.73)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.31)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.93)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.05)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.76)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.54)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.44)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.33)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.05)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.81)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.56)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded -0.05)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.02)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.04)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.32)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.34)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.64)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.88)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.20)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.53)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.58)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.06)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.98)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.71)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.11)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.21)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.23)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.92)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.16)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.52)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.59)
6% of time remaining to reach destination.

/-------------------
| Step 33 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.37)
3% of time remaining to reach destination.

/-------------------
| Step 34 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.72)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 19
\-------------------------

Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.12)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.99)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.47)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.23)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.75)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.50)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.33)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.37)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.95)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.02)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.62)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.79)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.71)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.01)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.06)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.10)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.80)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.63)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.77)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.00)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.43)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.18)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.25)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.91)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.66)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 20
\-------------------------

Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.79)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.97)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.24)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.88)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.91)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.63)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 21
\-------------------------

Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.75)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.20)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.64)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.94)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.61)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.56)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.57)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.03)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.10)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.63)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.02)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.65)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.53)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.23)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 22
\-------------------------

Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.30)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.47)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.69)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.76)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.74)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.27)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.07)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.79)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.14)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.61)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.60)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.56)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.05)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.53)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.02)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.33)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.14)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.25)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.21)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.79)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.61)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.20)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.99)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.19)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.07)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.86)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.93)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.82)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.49)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.34)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 23
\-------------------------

Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.80)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.96)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.24)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.95)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.26)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.11)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.42)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.84)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.69)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.16)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.64)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.18)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.62)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.60)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.92)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.35)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.01)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.99)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.18)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.54)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.56)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.32)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.56)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.67)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.90)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.27)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded -0.04)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.25)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 24
\-------------------------

Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.86)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.68)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.08)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.83)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.18)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.93)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.21)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.61)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.91)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.33)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.52)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.43)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.21)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.01)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.03)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.45)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.65)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.40)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.26)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.85)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.02)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.61)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded -0.27)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.79)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.37)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 25
\-------------------------

Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.64)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.83)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.28)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.01)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.37)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.54)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.79)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.10)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.47)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.64)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.27)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.02)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.95)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.22)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.49)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.17)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.38)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.48)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.40)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.58)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.49)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.68)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.65)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.16)
4% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 26
\-------------------------

Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.69)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.47)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.13)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.02)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.37)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.59)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.34)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.68)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.96)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.49)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.95)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.02)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.09)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.21)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.34)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.97)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.01)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.04)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.56)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.05)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.85)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.87)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.99)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.49)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.63)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.56)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.05)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.69)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.94)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.32)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 27
\-------------------------

Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.91)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.79)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.75)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.60)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.77)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.27)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.71)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.61)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.23)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.25)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.33)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.17)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.44)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.85)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.29)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.85)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.05)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.87)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.68)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.56)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.26)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.38)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.29)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.36)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.68)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 28
\-------------------------

Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.67)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.25)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.15)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.67)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.50)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.82)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.59)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.04)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.91)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.28)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.23)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.84)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.38)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.99)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.20)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.21)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.32)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.25)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.20)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.07)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 29
\-------------------------

Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.90)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.84)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.19)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.99)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.09)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.59)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.96)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.21)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.00)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.84)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.09)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.26)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.51)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.33)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.13)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.43)
15% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 30
\-------------------------

Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.63)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.03)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.66)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.77)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.37)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.63)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.83)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.27)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.81)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.39)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.27)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.58)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.40)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.23)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.06)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.66)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.06)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 31
\-------------------------

Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.59)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.88)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.05)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.65)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.86)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.39)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.97)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.01)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.79)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.15)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.85)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.17)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.28)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.00)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.25)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.92)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.21)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.86)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.49)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.45)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.28)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.45)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.24)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.92)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.16)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.55)
13% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 32
\-------------------------

Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.29)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.00)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.79)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.92)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.00)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.74)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.72)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.82)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.48)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.43)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.41)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.93)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.74)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.89)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.68)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
32% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
32% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
32% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
32% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
32% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
32% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
32% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
32% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
32% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
32% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
32% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.21)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 33
\-------------------------

Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.91)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.95)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.49)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.65)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.61)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.79)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.92)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.69)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.21)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.69)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.73)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.85)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.66)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.79)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.86)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.95)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.99)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.25)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.79)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.15)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.06)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.42)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.93)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.10)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 34
\-------------------------

Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.36)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.85)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.02)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.61)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.13)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.43)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.95)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.78)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.07)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.12)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.84)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.49)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.31)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.63)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 35
\-------------------------

Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.05)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.95)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.43)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.16)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.58)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.35)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.33)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.06)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.56)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.42)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.18)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.38)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.07)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.42)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.58)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.66)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.80)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.91)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.87)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.42)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.45)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.18)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.40)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.43)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.89)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 36
\-------------------------

Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.46)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.35)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.94)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.32)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.50)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.71)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.40)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.21)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.82)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.94)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.22)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.14)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.70)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.70)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.30)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.33)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.71)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.66)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.37)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.27)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.89)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.53)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.98)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.95)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 37
\-------------------------

Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.91)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.06)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.60)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.36)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.30)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.89)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.07)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.85)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.28)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.60)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.16)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.47)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.85)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.15)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.24)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.56)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.30)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.04)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.58)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.50)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 38
\-------------------------

Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.44)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.16)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.15)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.43)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.75)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.70)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.47)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.26)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.58)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 39
\-------------------------

Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.45)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.30)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.35)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.72)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.41)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.19)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.03)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.64)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.15)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.63)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.73)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.86)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.01)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.90)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.78)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.13)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.82)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.85)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.87)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.28)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.14)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.44)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.82)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 40
\-------------------------

Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.58)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.22)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.34)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.09)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.09)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.89)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.55)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.19)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.52)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.50)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 41
\-------------------------

Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.08)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.50)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.28)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.84)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.81)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.76)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.74)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.76)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.74)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.79)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.96)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.93)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.59)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 42
\-------------------------

Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.71)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.78)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.53)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.46)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.53)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.99)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.10)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.04)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.15)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.62)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.60)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.45)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.22)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.31)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.24)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.23)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.19)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.26)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.05)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.03)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.13)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.32)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.89)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.45)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 43
\-------------------------

Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.90)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.97)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.70)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.74)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.50)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.37)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.64)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.02)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.30)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.87)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.04)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.69)
40% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.69)
40% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.69)
40% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.69)
40% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.69)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.60)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.24)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.40)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.26)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.70)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.41)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.04)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 44
\-------------------------

Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.76)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.68)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.85)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.10)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.83)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.93)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.30)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.62)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.07)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.87)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.51)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.09)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.82)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.53)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.91)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.20)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 45
\-------------------------

Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.76)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.91)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.50)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.67)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.56)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.04)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.16)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.78)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.19)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.93)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.18)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.78)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.95)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.62)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.32)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.00)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.66)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.07)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.23)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.22)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.87)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.63)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.36)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.72)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 46
\-------------------------

Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.64)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.43)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.27)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.00)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.24)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.57)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.23)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.01)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.99)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.55)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.91)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.76)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.18)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.47)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.49)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.39)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.46)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.09)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.03)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.49)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.75)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.78)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.17)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.18)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.88)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 47
\-------------------------

Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.89)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.70)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.76)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.24)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.34)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.02)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.60)
73% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 48
\-------------------------

Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.10)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.52)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.74)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.94)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.37)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.52)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.82)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.92)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.34)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.95)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.57)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.02)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.94)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.20)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.75)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.76)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.10)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.17)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.02)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.47)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.95)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.18)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded -0.60)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.65)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 49
\-------------------------

Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.79)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.16)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.14)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.20)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.97)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.24)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.99)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.26)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.38)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.28)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.61)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.45)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.08)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.86)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.13)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.11)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.33)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.69)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.02)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.18)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.16)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.53)
8% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 50
\-------------------------

Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.02)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.33)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.53)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.23)
80% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.23)
80% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.23)
80% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.23)
80% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.23)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.81)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.66)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.93)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.73)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 51
\-------------------------

Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.88)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.69)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.05)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.04)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.38)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.30)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.19)
72% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 52
\-------------------------

Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.86)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.12)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.76)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.16)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.76)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.82)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.06)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.64)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.57)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.72)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.58)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.80)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.03)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.27)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.51)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.80)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.11)
5% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 53
\-------------------------

Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.98)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.61)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.90)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.18)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.70)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.27)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.49)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.48)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.14)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.14)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.39)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.16)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.95)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.20)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.30)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.16)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.69)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.40)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.39)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.84)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.37)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.29)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.14)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.05)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 54
\-------------------------

Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.45)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.15)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.58)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.14)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.53)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.67)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.91)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.54)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.67)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.84)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.06)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.25)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 55
\-------------------------

Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.49)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.25)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.65)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.27)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 56
\-------------------------

Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.72)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.20)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.05)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.18)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.86)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.54)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.22)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.48)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.46)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.30)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.18)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.52)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.43)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.86)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.72)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.56)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.18)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded -0.45)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.41)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.99)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded -0.26)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.89)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.64)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 57
\-------------------------

Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.69)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.10)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.92)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.09)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.59)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.27)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.17)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.51)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.04)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.38)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.97)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.68)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.83)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.11)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.53)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.56)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.39)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.33)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.42)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.79)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.57)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.34)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.09)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.41)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.92)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 58
\-------------------------

Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.82)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.96)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.05)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.01)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.32)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.03)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.74)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.94)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.47)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.06)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.43)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.57)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.65)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.20)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.28)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.31)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.62)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.52)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.21)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 59
\-------------------------

Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.63)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.47)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.74)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.43)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.18)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.00)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.02)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.60)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.19)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.34)
67% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 60
\-------------------------

Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.43)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.63)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.37)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.16)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.97)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.48)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.44)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.47)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.11)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.97)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.14)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.23)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.69)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.97)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 61
\-------------------------

Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.57)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.88)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.56)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.89)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.86)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.97)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.86)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.72)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.97)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.60)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.11)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.02)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.71)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.06)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.22)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.80)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.09)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.86)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.40)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.70)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.57)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.67)
12% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 62
\-------------------------

Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.21)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.43)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.43)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.74)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.37)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.25)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.31)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.45)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.04)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 63
\-------------------------

Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.17)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.78)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.70)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.81)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.60)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.73)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.18)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.08)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.91)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.02)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.05)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.17)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.13)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.64)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.42)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.67)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.63)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.46)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.60)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.03)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.54)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.17)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.58)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.45)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.85)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 64
\-------------------------

Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.64)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.42)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.50)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.47)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.44)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.91)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.38)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.56)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.35)
55% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.35)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.04)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.74)
45% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.74)
45% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.74)
45% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.74)
45% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.74)
45% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.74)
45% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.74)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.05)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.72)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.11)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.24)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.26)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.91)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.73)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.60)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.21)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 65
\-------------------------

Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.77)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.59)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.49)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.70)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.31)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.62)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.06)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.82)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.28)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.65)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.82)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 66
\-------------------------

Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.58)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.44)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.78)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.15)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.49)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.29)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.17)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.41)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.12)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.19)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.68)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.24)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.86)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.03)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.20)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.18)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.89)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.00)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.01)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.26)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 67
\-------------------------

Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.31)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.22)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.70)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.77)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.77)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.23)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.69)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.08)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.04)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 68
\-------------------------

Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.80)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.74)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.15)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.33)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.80)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.67)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.00)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.68)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.57)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.78)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.89)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.49)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.13)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.11)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.15)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.90)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.70)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.12)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.35)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 69
\-------------------------

Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.94)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.98)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.88)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.87)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.01)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.66)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.93)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.19)
73% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 70
\-------------------------

Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.93)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.77)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.46)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.01)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.15)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.37)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.04)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.39)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.61)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.15)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.07)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.71)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.00)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.86)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.77)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.01)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.66)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.12)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.78)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.88)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.72)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.83)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.23)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.24)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.11)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.30)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.32)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.50)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.02)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.53)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 71
\-------------------------

Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.17)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.56)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.52)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.47)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.47)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.99)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.66)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.12)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.91)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.49)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 72
\-------------------------

Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.35)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.73)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.30)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.76)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 73
\-------------------------

Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.07)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.19)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.39)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.59)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.93)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.49)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.42)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.09)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.34)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.17)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.43)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.99)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.41)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.59)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.27)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.59)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.97)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.93)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.33)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.64)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.37)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.28)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.19)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.55)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.07)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.87)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.03)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.50)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.01)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.87)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 74
\-------------------------

Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.41)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.86)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.81)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.20)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.50)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.48)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.75)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.01)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.07)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.74)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.01)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.62)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.66)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.17)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.56)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.94)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.99)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.51)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.32)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.41)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.27)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.24)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.30)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.96)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.41)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.11)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.26)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.59)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.22)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 75
\-------------------------

Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.68)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.72)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.91)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.02)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.61)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.07)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.35)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.75)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.82)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.44)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.39)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.24)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.10)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.50)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.14)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.00)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.04)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.04)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.25)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.77)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.28)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.12)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.20)
23% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 76
\-------------------------

Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.61)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.85)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.95)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.85)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.65)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.71)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.22)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.69)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.24)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.97)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.74)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.27)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.68)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.13)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.49)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.47)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.42)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.09)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.13)
5% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 77
\-------------------------

Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.10)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.54)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.47)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.45)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 78
\-------------------------

Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.53)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.65)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.47)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.49)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.70)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.36)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.40)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.24)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.82)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.05)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.46)
63% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 79
\-------------------------

Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.65)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.49)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.94)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.59)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 80
\-------------------------

Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.51)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.25)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.24)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.32)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.72)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.33)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.32)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.09)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.99)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.07)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.27)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.06)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.01)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.03)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.72)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.20)
43% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 81
\-------------------------

Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.58)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.66)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.15)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.55)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.93)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.21)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.79)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.94)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.76)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.93)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.17)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.97)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.43)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.71)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.92)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 82
\-------------------------

Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.12)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.99)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.59)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.65)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.95)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.19)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.48)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.04)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.17)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.28)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.68)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.54)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 83
\-------------------------

Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.48)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.76)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.44)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.15)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.27)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 84
\-------------------------

Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.01)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.40)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.53)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.03)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.65)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.60)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.53)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.95)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.74)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.92)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.47)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.39)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.84)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.45)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 85
\-------------------------

Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.61)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.06)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.21)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.40)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.55)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.55)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.97)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.16)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.33)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.24)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.73)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 86
\-------------------------

Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.43)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.45)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.60)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.18)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.32)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.76)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.31)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.66)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.44)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.59)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.61)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.11)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.70)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.30)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.87)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.80)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.92)
24% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 87
\-------------------------

Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.70)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.23)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.13)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.17)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.71)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.23)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 88
\-------------------------

Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.07)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.05)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.60)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.51)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.18)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.03)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.74)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.99)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.64)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.89)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.83)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.93)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.12)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.14)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.03)
54% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 89
\-------------------------

Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.89)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.53)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.08)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.95)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.19)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.49)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.54)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.36)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.11)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.20)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.51)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.48)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.30)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.17)
44% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.17)
44% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.17)
44% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.17)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.29)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.20)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.32)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.78)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.11)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.60)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.53)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.98)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.32)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.38)
4% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 90
\-------------------------

Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.39)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.20)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.65)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.35)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.49)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.99)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.62)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.70)
68% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 91
\-------------------------

Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.90)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.15)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.91)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.38)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.91)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.61)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.68)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.48)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.55)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.42)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.96)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.91)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.01)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 92
\-------------------------

Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.33)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.68)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.99)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.33)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.69)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.58)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.87)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.41)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.74)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.17)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.34)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.08)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.44)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.62)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.07)
43% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.07)
43% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.07)
43% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.07)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.35)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.71)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.90)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.55)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.21)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.41)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.50)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 93
\-------------------------

Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.31)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.12)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.88)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.37)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 94
\-------------------------

Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.69)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.86)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.92)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.24)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.89)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 95
\-------------------------

Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.65)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.64)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.00)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.77)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.27)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.56)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.72)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.35)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 1
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.12)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.37)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.49)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.96)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.05)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.83)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.12)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.08)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.92)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.42)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.53)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.08)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.30)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded -0.09)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.73)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.36)
20% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 2
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.59)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.19)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.42)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 3
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.34)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.84)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.36)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.23)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.29)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.51)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.07)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.58)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.18)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.95)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 4
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.49)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.94)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.38)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.28)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.76)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.54)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.69)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.59)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.96)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.10)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.29)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.94)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 5
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.93)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.62)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.91)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.52)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.36)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.85)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.07)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.84)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.50)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.84)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.30)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.49)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 6
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.78)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.52)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.21)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.42)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.85)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.78)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.97)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.43)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.60)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.64)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.97)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.82)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 7
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.61)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.57)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.27)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.77)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.86)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.83)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.54)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.61)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.32)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.10)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.02)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.76)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.30)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.45)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 8
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.66)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.95)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.83)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.27)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.62)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.46)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.26)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.98)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.53)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.05)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.29)
63% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 9
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.85)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.33)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.24)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.25)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.58)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.58)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.38)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 10
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.58)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.96)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.65)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.97)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.03)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.36)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.57)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.36)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.71)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.99)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.42)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.74)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.27)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.29)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.35)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.78)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.21)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

Simulation ended. . . 
</pre>
</div>
</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[18]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># Load the &#39;sim_default-learning&#39; file from the default Q-Learning simulation</span>
<span class="n">vs</span><span class="o">.</span><span class="n">plot_trials</span><span class="p">(</span><span class="s1">&#39;sim_default-learning.csv&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-6">Question 6<a class="anchor-link" href="#Question-6">&#182;</a></h3><p>Using the visualization above that was produced from your default Q-Learning simulation, provide an analysis and make observations about the driving agent like in <strong>Question 3</strong>. Note that the simulation should have also produced the Q-table in a text file which can help you make observations about the agent's learning. Some additional things you could consider:</p>
<ul>
<li><em>Are there any observations that are similar between the basic driving agent and the default Q-Learning agent?</em></li>
<li><em>Approximately how many training trials did the driving agent require before testing? Does that number make sense given the epsilon-tolerance?</em></li>
<li><em>Is the decaying function you implemented for $\epsilon$ (the exploration factor) accurately represented in the parameters panel?</em></li>
<li><em>As the number of training trials increased, did the number of bad actions decrease? Did the average reward increase?</em></li>
<li><em>How does the safety and reliability rating compare to the initial driving agent?</em></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Answer:</strong></p>

<pre><code>- I don't see any similar observations in the plots
- I think it is 20 training trails. Yes
- Yes
- Yes, the number of bad actions decrease to 0.03 approximately and average reward increased to 1.5
- Both have improved</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h2 id="Improve-the-Q-Learning-Driving-Agent">Improve the Q-Learning Driving Agent<a class="anchor-link" href="#Improve-the-Q-Learning-Driving-Agent">&#182;</a></h2><p>The third step to creating an optimized Q-Learning agent is to perform the optimization! Now that the Q-Learning algorithm is implemented and the driving agent is successfully learning, it's necessary to tune settings and adjust learning paramaters so the driving agent learns both <strong>safety</strong> and <strong>efficiency</strong>. Typically this step will require a lot of trial and error, as some settings will invariably make the learning worse. One thing to keep in mind is the act of learning itself and the time that this takes: In theory, we could allow the agent to learn for an incredibly long amount of time; however, another goal of Q-Learning is to <em>transition from experimenting with unlearned behavior to acting on learned behavior</em>. For example, always allowing the agent to perform a random action during training (if $\epsilon = 1$ and never decays) will certainly make it <em>learn</em>, but never let it <em>act</em>. When improving on your Q-Learning implementation, consider the implications it creates and whether it is logistically sensible to make a particular adjustment.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Improved-Q-Learning-Simulation-Results">Improved Q-Learning Simulation Results<a class="anchor-link" href="#Improved-Q-Learning-Simulation-Results">&#182;</a></h3><p>To obtain results from the initial Q-Learning implementation, you will need to adjust the following flags and setup:</p>
<ul>
<li><code>'enforce_deadline'</code> - Set this to <code>True</code> to force the driving agent to capture whether it reaches the destination in time.</li>
<li><code>'update_delay'</code> - Set this to a small value (such as <code>0.01</code>) to reduce the time between steps in each trial.</li>
<li><code>'log_metrics'</code> - Set this to <code>True</code> to log the simluation results as a <code>.csv</code> file and the Q-table as a <code>.txt</code> file in <code>/logs/</code>.</li>
<li><code>'learning'</code> - Set this to <code>'True'</code> to tell the driving agent to use your Q-Learning implementation.</li>
<li><code>'optimized'</code> - Set this to <code>'True'</code> to tell the driving agent you are performing an optimized version of the Q-Learning implementation.</li>
</ul>
<p>Additional flags that can be adjusted as part of optimizing the Q-Learning agent:</p>
<ul>
<li><code>'n_test'</code> - Set this to some positive number (previously 10) to perform that many testing trials.</li>
<li><code>'alpha'</code> - Set this to a real number between 0 - 1 to adjust the learning rate of the Q-Learning algorithm.</li>
<li><code>'epsilon'</code> - Set this to a real number between 0 - 1 to adjust the starting exploration factor of the Q-Learning algorithm.</li>
<li><code>'tolerance'</code> - set this to some small value larger than 0 (default was 0.05) to set the epsilon threshold for testing.</li>
</ul>
<p>Furthermore, use a decaying function of your choice for $\epsilon$ (the exploration factor). Note that whichever function you use, it <strong>must decay to </strong><code>'tolerance'</code><strong> at a reasonable rate</strong>. The Q-Learning agent will not begin testing until this occurs. Some example decaying functions (for $t$, the number of trials):</p>
<p>$$ \epsilon = a^t, \textrm{for } 0 &lt; a &lt; 1 \hspace{50px}\epsilon = \frac{1}{t^2}\hspace{50px}\epsilon = e^{-at}, \textrm{for } 0 &lt; a &lt; 1 \hspace{50px} \epsilon = \cos(at), \textrm{for } 0 &lt; a &lt; 1$$
You may also use a decaying function for $\alpha$ (the learning rate) if you so choose, however this is typically less common. If you do so, be sure that it adheres to the inequality $0 \leq \alpha \leq 1$.</p>
<p>If you have difficulty getting your implementation to work, try setting the <code>'verbose'</code> flag to <code>True</code> to help debug. Flags that have been set here should be returned to their default setting when debugging. It is important that you understand what each flag does and how it affects the simulation!</p>
<p>Once you have successfully completed the improved Q-Learning simulation, run the code cell below to visualize the results. Note that log files are overwritten when identical simulations are run, so be careful with what log file is being loaded!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[21]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="o">%</span><span class="k">run</span> smartcab/agent.py
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>


<div class="output_subarea output_stream output_stdout output_text">
<pre>
/-------------------------
| Training trial 1
\-------------------------

Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.33)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.37)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.77)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.65)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.77)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.96)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.92)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.23)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.70)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.34)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.11)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.52)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.04)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.60)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.25)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.01)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.16)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.38)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.16)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.62)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 2
\-------------------------

Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.01)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.07)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.79)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.08)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.10)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.05)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.89)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.16)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.55)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.24)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.73)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.43)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.59)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.99)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.47)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.79)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.93)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.82)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.57)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.45)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.42)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.06)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.38)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.37)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.69)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.32)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.29)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.02)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.64)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.02)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 3
\-------------------------

Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.74)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.56)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.90)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.70)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.42)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded -0.02)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.57)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.44)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.10)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.66)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.55)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.66)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.11)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.04)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.33)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.59)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.90)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.39)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.45)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.25)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 4
\-------------------------

Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.90)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.02)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.71)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.42)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.85)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.56)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.18)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.37)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.17)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.33)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.19)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.17)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.08)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.77)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.35)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.90)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.14)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.57)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.34)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.87)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.18)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.33)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.00)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.49)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.33)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.16)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.40)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.70)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.91)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.28)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 5
\-------------------------

Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.10)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.60)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.40)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.07)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.80)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.13)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.80)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.63)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.56)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.64)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.36)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.26)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.76)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.09)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.49)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.19)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.23)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.45)
40% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.45)
40% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.45)
40% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.45)
40% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.45)
40% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.45)
40% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.45)
40% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.45)
40% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.45)
40% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.45)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.88)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.46)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.21)
30% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.21)
30% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.21)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.28)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.43)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.87)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.05)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.06)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.99)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.94)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.35)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded -0.51)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 6
\-------------------------

Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.35)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.13)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.75)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.24)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.95)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.86)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.44)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.08)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.89)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.74)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.90)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.09)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.64)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.86)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.35)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.86)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.43)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.92)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.46)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 7
\-------------------------

Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.20)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.42)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.94)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.94)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.65)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.62)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.91)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.56)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.62)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.33)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.10)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.15)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.80)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.47)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.20)
40% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.20)
40% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.20)
40% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.20)
40% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.20)
40% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.20)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.96)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.76)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.00)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.96)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.53)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.20)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.22)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.35)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.50)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.01)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 8
\-------------------------

Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.34)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.89)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.68)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.59)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.60)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.68)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.43)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.16)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.80)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.51)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.75)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.73)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.75)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.57)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.29)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.32)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.30)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.74)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.31)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.22)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.29)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.07)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.88)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.89)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 9
\-------------------------

Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.44)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.52)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.59)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.70)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.83)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.50)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.55)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.41)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.21)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.35)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.45)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.76)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.76)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.67)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.82)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.56)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.54)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.60)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.51)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.44)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.90)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.54)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.86)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.50)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.40)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.08)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.96)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded -0.77)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.65)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 10
\-------------------------

Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.9000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.95)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.57)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.29)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.24)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.17)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.67)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.35)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.43)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.50)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.53)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.67)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.48)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.10)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.73)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.23)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.29)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.59)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.05)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.29)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.02)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 11
\-------------------------

Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.31)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.19)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.96)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.48)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.24)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.93)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.26)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.18)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.74)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.45)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.11)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.05)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.28)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.03)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.98)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.40)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.54)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.23)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.26)
5% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 12
\-------------------------

Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.89)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.28)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.15)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.62)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.28)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.09)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.84)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.13)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.21)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.00)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.84)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.69)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.06)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.09)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.50)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.97)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.11)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.81)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.35)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.17)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.64)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.19)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.68)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.11)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.29)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.58)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.56)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.94)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.28)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.31)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 13
\-------------------------

Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.54)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.62)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.35)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.64)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.89)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.23)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.04)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.46)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.22)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.78)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.26)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.33)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.05)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.63)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.17)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.78)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.98)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.07)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.88)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.02)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 14
\-------------------------

Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.53)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.65)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.06)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.47)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.32)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.07)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.80)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.04)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.34)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.40)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.89)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.12)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.63)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.22)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.83)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.03)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.86)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.81)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.74)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 15
\-------------------------

Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.44)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.41)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.38)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.45)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.21)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.53)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.30)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.82)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.28)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.39)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.35)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.65)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.32)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.18)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.45)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.36)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.10)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.43)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.17)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.47)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 16
\-------------------------

Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.52)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.41)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.46)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.79)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.64)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.21)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.95)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.67)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.09)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.07)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.79)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.69)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.03)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.56)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.47)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.09)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.06)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.06)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.57)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.29)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 17
\-------------------------

Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.87)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.85)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.67)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.26)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.01)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.01)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.63)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.95)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.43)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 18
\-------------------------

Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.48)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.22)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.64)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.91)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.27)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.04)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.02)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 19
\-------------------------

Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.41)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.81)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.78)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.69)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.62)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.01)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.16)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.54)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.06)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.50)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.45)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.27)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.95)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.62)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.07)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.93)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.39)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.38)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.99)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.13)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.79)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.91)
37% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.91)
37% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.91)
37% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.91)
37% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.91)
37% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.91)
37% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.91)
37% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.91)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.32)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.02)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.51)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.99)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.38)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.51)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.81)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.95)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.54)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.42)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.31)
6% of time remaining to reach destination.

/-------------------
| Step 33 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.57)
3% of time remaining to reach destination.

/-------------------
| Step 34 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.92)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 20
\-------------------------

Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.8000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.61)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.30)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.51)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.68)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.14)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.11)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.67)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.41)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.80)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.70)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.99)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.03)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.23)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.81)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.29)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.10)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.61)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.44)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.56)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.67)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.26)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.07)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.27)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.97)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 21
\-------------------------

Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.31)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.45)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.88)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.85)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.15)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.75)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.40)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.58)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.77)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.64)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.58)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.43)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.02)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.38)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.34)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.17)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.61)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.87)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.94)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.71)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.51)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.30)
8% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.30)
8% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.30)
8% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.30)
8% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.30)
8% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.30)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.89)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.94)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 22
\-------------------------

Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.47)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.17)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.33)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.75)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.37)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.79)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.37)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.27)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.36)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.78)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.78)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.12)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.16)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.52)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.62)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.64)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.25)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.77)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.56)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.55)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.26)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.41)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.36)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.43)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.41)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 23
\-------------------------

Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.65)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.81)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.52)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.19)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.97)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.34)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.49)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.50)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.18)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.20)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.70)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.80)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.29)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.08)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.46)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.00)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.75)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.02)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.72)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.20)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.26)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.10)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.20)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.15)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.78)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 24
\-------------------------

Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.86)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.59)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.47)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.50)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.79)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.00)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.89)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.31)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.31)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.03)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.85)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.88)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.56)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.93)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.14)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.07)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.64)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.01)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.43)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded -0.23)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.82)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.15)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.21)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.19)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.57)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.20)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.48)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.19)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.40)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 25
\-------------------------

Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.02)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.41)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.76)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.45)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.92)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.76)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.31)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.48)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.50)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.96)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.43)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.19)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.54)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.76)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.30)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.41)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.78)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.08)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.04)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.70)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 26
\-------------------------

Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.27)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.32)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.73)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.23)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.00)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.59)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.94)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.48)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.42)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.08)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.90)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.75)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.96)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.35)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.48)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.31)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.09)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.98)
37% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 27
\-------------------------

Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.60)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.05)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.48)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.26)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.51)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.08)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.25)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.14)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.55)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.53)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.66)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.99)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.15)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.31)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.23)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.20)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.47)
15% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 28
\-------------------------

Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.45)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.24)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.10)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.86)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.53)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.62)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.32)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.22)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.15)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.91)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.85)
56% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 29
\-------------------------

Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.46)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.09)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.78)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.15)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.81)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.56)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.66)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.27)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.54)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.54)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.03)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.01)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.10)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.00)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.69)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.05)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.94)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.22)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.14)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.85)
12% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.85)
12% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.85)
12% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.85)
12% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.85)
12% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.85)
12% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.85)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.18)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.68)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.45)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 30
\-------------------------

Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.7000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.16)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.44)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.83)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.64)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.76)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.82)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.13)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.12)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.42)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.19)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.42)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.63)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.92)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.73)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.14)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.84)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.50)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.26)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.01)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.69)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 31
\-------------------------

Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.31)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.60)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.25)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.76)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.22)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.36)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.51)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.30)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.99)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.75)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.18)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.84)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.38)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.62)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.32)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.88)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 32
\-------------------------

Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.05)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.02)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.85)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.80)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.07)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.25)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.91)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.93)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.06)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.42)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.61)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.02)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.42)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.89)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.89)
50% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 33
\-------------------------

Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.41)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.64)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.24)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.86)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.01)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.30)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.76)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.24)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.85)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.51)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.72)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.76)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.62)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.88)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.46)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.43)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.46)
49% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 34
\-------------------------

Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.35)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.51)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.04)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.65)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.68)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.55)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.08)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.86)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded -0.15)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.27)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.23)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.40)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.77)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.31)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.52)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.88)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.42)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.01)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.88)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.12)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 35
\-------------------------

Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.84)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.07)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.71)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.02)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.07)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.45)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.06)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.97)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.58)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.65)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.70)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.13)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.14)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -10.82)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.61)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.46)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.03)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.53)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.16)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.52)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.42)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
12% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
12% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
12% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
12% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
12% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
12% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.14)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.11)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.03)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded -0.62)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 36
\-------------------------

Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.36)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.94)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.96)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.30)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.91)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.19)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.94)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.71)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.27)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.55)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.76)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.54)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.78)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.97)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.83)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.74)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.05)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.07)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.46)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.42)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.47)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.26)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.66)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.76)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.20)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.51)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.82)
3% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 37
\-------------------------

Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.83)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.42)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.74)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.73)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.61)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.96)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.36)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.80)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.81)
70% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.81)
70% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.81)
70% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.81)
70% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.81)
70% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.81)
70% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.81)
70% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.81)
70% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.81)
70% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.81)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.42)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.60)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.34)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.77)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.09)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.70)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.21)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.19)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.18)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.78)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.70)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.84)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.11)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.37)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.26)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.56)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.14)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.59)
7% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.23)
3% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.88)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 38
\-------------------------

Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.42)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.21)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.17)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.69)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.51)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.07)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.18)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.08)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.79)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.65)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.53)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.56)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.44)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.17)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.70)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.56)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.41)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.21)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.80)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 39
\-------------------------

Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.49)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.73)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.78)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.46)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.04)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.35)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.90)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.65)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.91)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.95)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.44)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.34)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.26)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.46)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.32)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.81)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.68)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.09)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.14)
5% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.14)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.53)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 40
\-------------------------

Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.6000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.55)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.72)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.45)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.16)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.83)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.54)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.54)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.50)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.72)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.87)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.66)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded -0.13)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.92)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.58)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.34)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.54)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.40)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.08)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.45)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded -0.34)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 41
\-------------------------

Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.46)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.33)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.92)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.79)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.09)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.92)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.84)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.41)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.18)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.90)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.29)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.67)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 42
\-------------------------

Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.12)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.13)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.20)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.52)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.20)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.43)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.11)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.38)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.38)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.91)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.13)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.10)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.22)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.33)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.52)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.14)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 43
\-------------------------

Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.76)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.33)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.74)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.85)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.70)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.16)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.92)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.58)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.21)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.19)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.41)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.75)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.54)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.97)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.14)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.78)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.58)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.94)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.78)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded -0.29)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.52)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.08)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.54)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.44)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.15)
23% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.61)
20% of time remaining to reach destination.

/-------------------
| Step 28 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.99)
17% of time remaining to reach destination.

/-------------------
| Step 29 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.70)
14% of time remaining to reach destination.

/-------------------
| Step 30 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.69)
11% of time remaining to reach destination.

/-------------------
| Step 31 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.32)
9% of time remaining to reach destination.

/-------------------
| Step 32 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
6% of time remaining to reach destination.

/-------------------
| Step 33 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.35)
3% of time remaining to reach destination.

/-------------------
| Step 34 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.88)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 44
\-------------------------

Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.36)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.35)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.12)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.09)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.28)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.16)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.45)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.22)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.43)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.30)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.83)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.51)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.01)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.68)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.91)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.56)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.02)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.91)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.12)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.45)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.88)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.03)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.90)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.55)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.50)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 45
\-------------------------

Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.27)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.04)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.14)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.35)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.31)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.88)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.76)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.18)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.26)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.90)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.48)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.76)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 46
\-------------------------

Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.66)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.03)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.42)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.07)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.33)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.55)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.32)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove left instead of right. (rewarded 0.61)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.62)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.62)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.09)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.51)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.85)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.25)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.44)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.08)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.03)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.48)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.93)
5% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.93)
5% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.93)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.75)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 47
\-------------------------

Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.39)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.63)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.54)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.34)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.75)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.94)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.45)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.41)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.53)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.15)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.65)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.68)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.49)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.16)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.28)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.86)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.21)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.53)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.86)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.63)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 48
\-------------------------

Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.01)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.43)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.61)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.67)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.29)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.85)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.31)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.12)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.85)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.89)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.20)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.30)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.03)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.15)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.69)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.66)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.77)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.25)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.58)
24% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 49
\-------------------------

Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.02)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.96)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.45)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.89)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.04)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 50
\-------------------------

Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.5000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.32)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.39)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.25)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.52)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.13)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.55)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.47)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.10)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.99)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.08)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.82)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.27)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.53)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.33)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.12)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.95)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.42)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.43)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.67)
27% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 51
\-------------------------

Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.18)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.46)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.33)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.04)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.57)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.18)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.79)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 52
\-------------------------

Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.68)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.50)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.98)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.04)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.57)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.84)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.17)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.56)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.57)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.93)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.10)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.91)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 53
\-------------------------

Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.18)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.06)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.93)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.80)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.22)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.69)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.04)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.97)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.85)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.39)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.51)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.51)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.60)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.09)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.42)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.06)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.55)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.61)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.11)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.21)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.89)
16% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.89)
16% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.89)
16% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.89)
16% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.89)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.94)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.72)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded -0.60)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.05)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 54
\-------------------------

Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.40)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.66)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.40)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.31)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.10)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.33)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.38)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.43)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 55
\-------------------------

Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.46)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.46)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.07)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.60)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.31)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.56)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.63)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.81)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.25)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.30)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.06)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.39)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.40)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.18)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.88)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.27)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.30)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.62)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.70)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.35)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.13)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.68)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.40)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.23)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 56
\-------------------------

Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.00)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.24)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.51)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.24)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.81)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.33)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.27)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.18)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.37)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.54)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.18)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.62)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.19)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.24)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -20.75)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.90)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.63)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.79)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.76)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.99)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 57
\-------------------------

Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.59)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.69)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.95)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.58)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.69)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.47)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.07)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded -0.07)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.46)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.44)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.16)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.43)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.16)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.07)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.37)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.09)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.46)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.00)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.24)
5% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 58
\-------------------------

Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.72)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.03)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.46)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.59)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.21)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.29)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.14)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.93)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.05)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.58)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.67)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.78)
48% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 59
\-------------------------

Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.04)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.58)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.25)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.51)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.87)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.46)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.43)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.15)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.49)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.46)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.89)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.16)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.35)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.21)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.19)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.56)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.89)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 60
\-------------------------

Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.4000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.75)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.95)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.79)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.63)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.59)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.79)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.73)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.54)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.66)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.93)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.80)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.75)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.92)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.45)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.18)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 61
\-------------------------

Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.52)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.39)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.63)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.55)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.46)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.59)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.91)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.01)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.43)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.58)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.49)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.30)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.02)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.72)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.95)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.57)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.79)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.58)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.61)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.62)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 62
\-------------------------

Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.74)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.98)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.09)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.04)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.72)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.89)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.16)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.11)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.93)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.85)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.89)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.12)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.71)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.65)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.54)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 63
\-------------------------

Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.73)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.15)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.64)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.33)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.69)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.10)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.94)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.93)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 64
\-------------------------

Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.96)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.88)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.80)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.43)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.54)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.88)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.71)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.80)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.77)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.09)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.37)
45% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 65
\-------------------------

Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.99)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.37)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.45)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.60)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.51)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.15)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.57)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 66
\-------------------------

Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.52)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.08)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.47)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.50)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.56)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.97)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.73)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.93)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.39)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.13)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.29)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.96)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.80)
53% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 67
\-------------------------

Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.77)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.50)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.03)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.22)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.11)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.58)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.32)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.16)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.01)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.68)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.89)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.56)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.07)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.61)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.71)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.50)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.86)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.40)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.44)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.53)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.62)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.06)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.15)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded -0.25)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.60)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;right&#39;, &#39;right&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -19.20)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving forward through a red light. (rewarded -9.43)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.28)
7% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 68
\-------------------------

Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.37)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.49)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.24)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.89)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.90)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.55)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.01)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.09)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.38)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.57)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.12)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.59)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.88)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.67)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 69
\-------------------------

Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -39.78)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.88)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.27)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.38)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.36)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.27)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.65)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.93)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.80)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.76)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.25)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.96)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.55)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.80)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.65)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.06)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.04)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.46)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.10)
24% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
20% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.67)
16% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.65)
12% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.15)
8% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent drove right instead of left. (rewarded -0.13)
4% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.14)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Training trial 70
\-------------------------

Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.3000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.98)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.76)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.46)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.59)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.02)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.25)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.27)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.97)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.83)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.09)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.23)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.78)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 71
\-------------------------

Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.15)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.64)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.13)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.13)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.08)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.16)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.81)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.75)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.87)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.81)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.69)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.31)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.35)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.71)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.49)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.32)
36% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 72
\-------------------------

Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.83)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.54)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.35)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.16)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.15)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.46)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.
Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.87)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.14)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.05)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.08)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.56)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.69)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.20)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.22)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.54)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.52)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.44)
15% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 73
\-------------------------

Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.63)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.23)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.17)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.48)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.54)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.31)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.13)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.56)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.29)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.39)
60% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.39)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light with traffic and cause a major accident. (rewarded -40.73)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.53)
52% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 74
\-------------------------

Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving right through traffic and cause a minor accident. (rewarded -19.72)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.07)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.97)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.25)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.34)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.60)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -10.30)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.39)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.07)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.17)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.67)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.87)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.95)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.75)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.57)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.97)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.32)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.63)
28% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 75
\-------------------------

Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.98)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.17)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.63)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.95)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.83)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.19)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.55)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.14)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.92)
64% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 76
\-------------------------

Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.67)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.98)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.66)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.94)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.30)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.94)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -10.98)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.27)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.83)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.48)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.63)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.81)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.72)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.58)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.57)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.36)
43% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 77
\-------------------------

Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.12)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.44)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.44)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.31)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.11)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.83)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 0.67)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.99)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.23)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.56)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.16)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.06)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.72)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.51)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 78
\-------------------------

Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.04)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.09)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.00)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.75)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.64)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.86)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.32)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -39.29)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.81)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.02)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.20)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.68)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.07)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -5.78)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.87)
47% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 79
\-------------------------

Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.66)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.33)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.88)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving forward through a red light. (rewarded -9.88)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.14)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.89)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.71)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.71)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.41)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 80
\-------------------------

Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.2000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.38)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent drove left instead of right. (rewarded 1.40)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.87)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.75)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.63)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.64)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.85)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
73% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 81
\-------------------------

Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.46)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.92)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.73)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.08)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.46)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.57)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.31)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 82
\-------------------------

Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.57)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.13)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.48)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.40)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.87)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.12)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.18)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through a red light with traffic and cause a major accident. (rewarded -40.74)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.56)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 0.15)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.41)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.37)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.23)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.51)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 83
\-------------------------

Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.14)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.09)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.26)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.00)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.74)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.21)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.33)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.26)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.95)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.93)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.49)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 84
\-------------------------

Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.46)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.35)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.06)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.40)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.29)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.90)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;left&#39;)
Agent drove right instead of left. (rewarded 1.04)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.10)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.54)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -9.41)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.60)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.17)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.80)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.09)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.86)
40% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 85
\-------------------------

Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.24)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.03)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.76)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.58)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.42)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.28)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.69)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.80)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.70)
55% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 86
\-------------------------

Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1400; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.96)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.52)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.30)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.38)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 87
\-------------------------

Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1300; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -9.38)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.22)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.96)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.80)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.77)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.92)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.13)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.08)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.67)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.39)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.21)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent idled at a green light with no oncoming traffic. (rewarded -4.75)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.57)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.31)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.19)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.87)
32% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.87)
32% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.87)
32% of time remaining to reach destination.
Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.87)
32% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.69)
28% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.90)
24% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 88
\-------------------------

Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1200; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.04)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.19)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.75)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.83)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.97)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.98)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.03)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent attempted driving forward through a red light. (rewarded -10.37)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.38)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.34)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.85)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.08)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.15)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.50)
30% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 89
\-------------------------

Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1100; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.16)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded 1.84)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.86)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.13)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -10.50)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.76)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent drove left instead of forward. (rewarded -0.04)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.48)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.92)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.27)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.76)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.18)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.70)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.44)
44% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.97)
40% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.67)
36% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.85)
32% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 90
\-------------------------

Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.1000; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.77)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.86)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.74)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.91)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.48)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.67)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.41)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.03)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 91
\-------------------------

Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0900; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.79)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent attempted driving left through a red light. (rewarded -9.85)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;right&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.36)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.29)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.65)
75% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 92
\-------------------------

Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0800; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.34)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent attempted driving left through a red light. (rewarded -9.36)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.93)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.84)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.51)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.91)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.05)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.65)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent attempted driving left through a red light. (rewarded -10.87)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.80)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.26)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.82)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.11)
35% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 93
\-------------------------

Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0700; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 1.35)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.78)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.24)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.86)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.49)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.06)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.04)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.77)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.43)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.21)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.07)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.97)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.87)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.02)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 94
\-------------------------

Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0600; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.57)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.62)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.67)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.68)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.00)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.05)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.55)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.13)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.12)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.98)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.84)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 0.85)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent drove forward instead of right. (rewarded 0.57)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.01)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.26)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.36)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.89)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.27)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.98)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.20)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.47)
37% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.48)
34% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.91)
31% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.99)
29% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.35)
26% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.22)
23% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Training trial 95
\-------------------------

Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000
Simulating trial. . . 
epsilon = 0.0500; alpha = 0.5000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.20)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.05)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.40)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.94)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.09)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.38)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.85)
65% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 0.20)
60% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.53)
55% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.46)
50% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.92)
45% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.22)
40% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.52)
35% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.93)
30% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.01)
25% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 1.33)
20% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.31)
15% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.39)
10% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.63)
5% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.54)
0% of time remaining to reach destination.

Trial Aborted!
Agent did not reach the destination.

/-------------------------
| Testing trial 1
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.74)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.22)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.47)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.76)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.59)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.73)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.37)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.45)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.49)
74% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.97)
71% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.73)
69% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.66)
66% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent attempted driving left through traffic and cause a minor accident. (rewarded -20.10)
63% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.70)
60% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.40)
57% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.82)
54% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.35)
51% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.95)
49% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.13)
46% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.99)
43% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.67)
40% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.05)
37% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 2
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.48)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.08)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.91)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.68)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.17)
80% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 3
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.39)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.20)
94% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.44)
91% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.46)
89% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.14)
86% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.97)
83% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.50)
80% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.63)
77% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.75)
74% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 4
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.72)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.79)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.57)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.65)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.99)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.37)
70% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 5
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.65)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.01)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.11)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.00)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.62)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.21)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.94)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.51)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.06)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.88)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.81)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.59)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.34)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded -0.00)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;left&#39;)
Agent drove forward instead of left. (rewarded 1.22)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.90)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.95)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.11)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.40)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 0.75)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.60)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.58)
27% of time remaining to reach destination.

/-------------------
| Step 22 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.17)
23% of time remaining to reach destination.

/-------------------
| Step 23 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.56)
20% of time remaining to reach destination.

/-------------------
| Step 24 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.31)
17% of time remaining to reach destination.

/-------------------
| Step 25 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.66)
13% of time remaining to reach destination.

/-------------------
| Step 26 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;right&#39;)
Agent properly idled at a red light. (rewarded 1.25)
10% of time remaining to reach destination.

/-------------------
| Step 27 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 0.57)
7% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 6
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.53)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.20)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.47)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.19)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.57)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.24)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.06)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.48)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.29)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.71)
60% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.79)
56% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.34)
52% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.15)
48% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.39)
44% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 7
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.78)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;right&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.05)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.71)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.26)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.61)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.19)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;forward&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.82)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 0.99)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.21)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.63)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.94)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.67)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.96)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.56)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.20)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.65)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;left&#39;, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.62)
43% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 8
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.95)
96% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.17)
92% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.43)
88% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.77)
84% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.32)
80% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.52)
76% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.58)
72% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.73)
68% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 0.99)
64% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.55)
60% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 9
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;forward&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.03)
95% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.94)
90% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;forward&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.39)
85% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.63)
80% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.36)
75% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.02)
70% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;right&#39;, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.07)
65% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

/-------------------------
| Testing trial 10
\-------------------------

Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000
Simulating trial. . . 
epsilon = 0.0000; alpha = 0.0000

/-------------------
| Step 0 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 1.63)
97% of time remaining to reach destination.

/-------------------
| Step 1 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;right&#39;)
Agent followed the waypoint right. (rewarded 2.56)
93% of time remaining to reach destination.

/-------------------
| Step 2 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.30)
90% of time remaining to reach destination.

/-------------------
| Step 3 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 1.67)
87% of time remaining to reach destination.

/-------------------
| Step 4 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.77)
83% of time remaining to reach destination.

/-------------------
| Step 5 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 2.00)
80% of time remaining to reach destination.

/-------------------
| Step 6 Results
\-------------------

Agent previous state: (&#39;green&#39;, &#39;right&#39;, &#39;forward&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.79)
77% of time remaining to reach destination.

/-------------------
| Step 7 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.02)
73% of time remaining to reach destination.

/-------------------
| Step 8 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.35)
70% of time remaining to reach destination.

/-------------------
| Step 9 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.73)
67% of time remaining to reach destination.

/-------------------
| Step 10 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;forward&#39;)
Agent followed the waypoint forward. (rewarded 1.62)
63% of time remaining to reach destination.

/-------------------
| Step 11 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.20)
60% of time remaining to reach destination.

/-------------------
| Step 12 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;forward&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
57% of time remaining to reach destination.

/-------------------
| Step 13 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 2.64)
53% of time remaining to reach destination.

/-------------------
| Step 14 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;forward&#39;)
Agent properly idled at a red light. (rewarded 2.22)
50% of time remaining to reach destination.

/-------------------
| Step 15 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;right&#39;, &#39;left&#39;, &#39;forward&#39;)
Agent drove right instead of forward. (rewarded 0.29)
47% of time remaining to reach destination.

/-------------------
| Step 16 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.27)
43% of time remaining to reach destination.

/-------------------
| Step 17 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, None, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.47)
40% of time remaining to reach destination.

/-------------------
| Step 18 Results
\-------------------

Agent previous state: (&#39;red&#39;, &#39;left&#39;, &#39;right&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 1.51)
37% of time remaining to reach destination.

/-------------------
| Step 19 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, None, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 1.86)
33% of time remaining to reach destination.

/-------------------
| Step 20 Results
\-------------------

Agent previous state: (&#39;red&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent properly idled at a red light. (rewarded 2.15)
30% of time remaining to reach destination.

/-------------------
| Step 21 Results
\-------------------

Agent previous state: (&#39;green&#39;, None, &#39;left&#39;, &#39;left&#39;)
Agent followed the waypoint left. (rewarded 0.97)
27% of time remaining to reach destination.

Trial Completed!
Agent reached the destination.

Simulation ended. . . 
</pre>
</div>
</div>

<div class="output_area">

<div class="prompt"></div>




<div class="output_text output_subarea ">
<pre>&lt;matplotlib.figure.Figure at 0x108a95b10&gt;</pre>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing code_cell rendered">
<div class="input">
<div class="prompt input_prompt">In&nbsp;[22]:</div>
<div class="inner_cell">
    <div class="input_area">
<div class=" highlight hl-ipython2"><pre><span></span><span class="c1"># Load the &#39;sim_improved-learning&#39; file from the improved Q-Learning simulation</span>
<span class="n">vs</span><span class="o">.</span><span class="n">plot_trials</span><span class="p">(</span><span class="s1">&#39;sim_improved-learning.csv&#39;</span><span class="p">)</span>
</pre></div>

</div>
</div>
</div>

<div class="output_wrapper">
<div class="output">


<div class="output_area">

<div class="prompt"></div>




<div class="output_png output_subarea ">
<img src="data:image/png;base64,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
"
>
</div>

</div>

</div>
</div>

</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-7">Question 7<a class="anchor-link" href="#Question-7">&#182;</a></h3><p>Using the visualization above that was produced from your improved Q-Learning simulation, provide a final analysis and make observations about the improved driving agent like in <strong>Question 6</strong>. Questions you should answer:</p>
<ul>
<li><em>What decaying function was used for epsilon (the exploration factor)?</em></li>
<li><em>Approximately how many training trials were needed for your agent before begining testing?</em></li>
<li><em>What epsilon-tolerance and alpha (learning rate) did you use? Why did you use them?</em></li>
<li><em>How much improvement was made with this Q-Learner when compared to the default Q-Learner from the previous section?</em></li>
<li><em>Would you say that the Q-Learner results show that your driving agent successfully learned an appropriate policy?</em></li>
<li><em>Are you satisfied with the safety and reliability ratings of the </em>Smartcab<em>?</em></li>
</ul>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Answer:</strong></p>

<pre><code>- self.epsilon decreasing by 0.01
- ~96
- default epsilon tolerance 0.05 and alpha 0.5
- There is some decrease in the average rewards to 0.5 and even the bad actions decreased to 0.09
- No, based on the safety rating
- No, safety rating is much lower. In real life, this would make a minor accident</code></pre>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Define-an-Optimal-Policy">Define an Optimal Policy<a class="anchor-link" href="#Define-an-Optimal-Policy">&#182;</a></h3><p>Sometimes, the answer to the important question <em>"what am I trying to get my agent to learn?"</em> only has a theoretical answer and cannot be concretely described. Here, however, you can concretely define what it is the agent is trying to learn, and that is the U.S. right-of-way traffic laws. Since these laws are known information, you can further define, for each state the <em>Smartcab</em> is occupying, the optimal action for the driving agent based on these laws. In that case, we call the set of optimal state-action pairs an <strong>optimal policy</strong>. Hence, unlike some theoretical answers, it is clear whether the agent is acting "incorrectly" not only by the reward (penalty) it receives, but also by pure observation. If the agent drives through a red light, we both see it receive a negative reward but also know that it is not the correct behavior. This can be used to your advantage for verifying whether the <strong>policy</strong> your driving agent has learned is the correct one, or if it is a <strong>suboptimal policy</strong>.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Question-8">Question 8<a class="anchor-link" href="#Question-8">&#182;</a></h3><ol>
<li><p>Please summarize what the optimal policy is for the smartcab in the given environment. What would be the best set of instructions possible given what we know about the environment? 
<em>You can explain with words or a table, but you should thoroughly discuss the optimal policy.</em></p>
</li>
<li><p>Next, investigate the <code>'sim_improved-learning.txt'</code> text file to see the results of your improved Q-Learning algorithm. <em>For each state that has been recorded from the simulation, is the <strong>policy</strong> (the action with the highest value) correct for the given state? Are there any states where the policy is different than what would be expected from an optimal policy?</em></p>
</li>
<li><p>Provide a few examples from your recorded Q-table which demonstrate that your smartcab learned the optimal policy. Explain why these entries demonstrate the optimal policy.</p>
</li>
<li><p>Try to find at least one entry where the smartcab did <em>not</em> learn the optimal policy.  Discuss why your cab may have not learned the correct policy for the given state.</p>
</li>
</ol>
<p>Be sure to document your <code>state</code> dictionary below, it should be easy for the reader to understand what each state represents.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Answer:</strong></p>
<p>An optimal policy in our environment:</p>

<pre><code>- Deciding the car's direction (left, forward, right) to reach destination
- Decide movement depends on light, stop at red and  free to move on green
- Look at light for left and right turns
- No rules to be ignored for safety reasons

</code></pre>
<p>There are few scenarios which made sense, I also noticed that actions that could have led to violations are heavily penalized. For instance, waypoint on a red light with a heavy penalized left action</p>
<p>State: light, left, oncoming, waypoint</p>

<pre><code>- Moving forward on green light: ('green', 'left', 'left', 'forward') -- forward : 2.04, -- None : -2.63, -- right : -0.01, -- left : 0.00
- Correct left turn on green light: ('green', 'left', 'forward', 'left') -- forward : 0.00, -- None : -2.84, -- right : 0.34, -- left : 0.00

</code></pre>
<p>('green', 'forward', 'forward', 'right')  -- forward : 0.00, -- None : 0.00, -- right : 1.68, -- left : -9.67. For red light, not picking right is the correct decision when there's oncoming traffic from left, but in this case we had the green light. The green light might have caused the smartcab to learn the incorrect policy.</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<hr>
<h3 id="Optional:-Future-Rewards---Discount-Factor,-'gamma'">Optional: Future Rewards - Discount Factor, <code>'gamma'</code><a class="anchor-link" href="#Optional:-Future-Rewards---Discount-Factor,-'gamma'">&#182;</a></h3><p>Curiously, as part of the Q-Learning algorithm, you were asked to <strong>not</strong> use the discount factor, <code>'gamma'</code> in the implementation. Including future rewards in the algorithm is used to aid in propagating positive rewards backwards from a future state to the current state. Essentially, if the driving agent is given the option to make several actions to arrive at different states, including future rewards will bias the agent towards states that could provide even more rewards. An example of this would be the driving agent moving towards a goal: With all actions and rewards equal, moving towards the goal would theoretically yield better rewards if there is an additional reward for reaching the goal. However, even though in this project, the driving agent is trying to reach a destination in the allotted time, including future rewards will not benefit the agent. In fact, if the agent were given many trials to learn, it could negatively affect Q-values!</p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<h3 id="Optional-Question-9">Optional Question 9<a class="anchor-link" href="#Optional-Question-9">&#182;</a></h3><p><em>There are two characteristics about the project that invalidate the use of future rewards in the Q-Learning algorithm. One characteristic has to do with the </em>Smartcab<em> itself, and the other has to do with the environment. Can you figure out what they are and why future rewards won't work for this project?</em></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<p><strong>Answer:</strong></p>

</div>
</div>
</div>
<div class="cell border-box-sizing text_cell rendered"><div class="prompt input_prompt">
</div>
<div class="inner_cell">
<div class="text_cell_render border-box-sizing rendered_html">
<blockquote><p><strong>Note</strong>: Once you have completed all of the code implementations and successfully answered each question above, you may finalize your work by exporting the iPython Notebook as an HTML document. You can do this by using the menu above and navigating to<br>
<strong>File -&gt; Download as -&gt; HTML (.html)</strong>. Include the finished document along with this notebook as your submission.</p>
</blockquote>

</div>
</div>
</div>
    </div>
  </div>
</body>

 


</html>
